{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1tOS7oWba4s"
   },
   "source": [
    "# Large language models (LLMs): Part II\n",
    "\n",
    "Author: Archit Vasan , including materials on LLMs by Varuni Sastri, and discussion/editorial work by Taylor Childers, Carlo Graziani, Bethany Lusch, and Venkat Vishwanath (Argonne)\n",
    "\n",
    "Inspiration from the blog posts \"The Illustrated Transformer\" and \"The Illustrated GPT2\" by Jay Alammar, highly recommended reading.\n",
    "\n",
    "Before you begin, make sure that you have your environment set up and your repo refreshed, as described in previous lessons, and reviewed in the accompanying 'Readme.md' file. Make sure that you select the kernel 'datascience/conda-2023-01-10' at the top-left of the Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Training and inference using Hugging Face\n",
    "2. Elements of an LLM\n",
    "3. Attention mechanisms\n",
    "4. Positional encoding\n",
    "5. Output layers\n",
    "6. Training loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HTTP_PROXY\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"HTTPS_PROXY\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"http_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"https_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\"\n",
    "os.environ[\"ftp_proxy\"]=\"http://proxy-01.pub.alcf.anl.gov:3128\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM training and inference using HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/hf-logo-with-title.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "HuggingFace is a platform and community that provides open-source library tools and resources like pre-trained models and datasets.\n",
    "Refer to the following links for more information :\n",
    "\n",
    "https://huggingface.co/docs/hub/index\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warning: _Large Language Models are only as good as their training data. They have no ethics, no judgement, or editing ability. We will be using some pretrained models from Hugging Face which used wide samples of internet hosted text. The datasets have not been strictly filtered to restrict all malign content so the generated text may be surprisingly dark or questionable. They do not reflect our core values and are only used for demonstration purposes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "\n",
    "We can use the Huggingface pipeline with a pretrained GPT2 model to generate text given a prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b57bc0aae3d4486a3eb0f659d535a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'My dog really wanted to get back home. After my dog disappeared, he came back.\\n\\n'},\n",
       " {'generated_text': \"My dog really wanted to spend the night but was too busy with work the next day. It's\"},\n",
       " {'generated_text': 'My dog really wanted to live with us,\" Schmitt says.\\n\\nThe couple found their shelter'},\n",
       " {'generated_text': 'My dog really wanted to bring him back to the circus,\" says Jeff. \"Before that, he'},\n",
       " {'generated_text': 'My dog really wanted to eat this and as they came on we went round in circles, one by'}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM, AutoConfig\n",
    "from transformers import pipeline\n",
    "input_text = \"My dog really wanted to\"\n",
    "generator = pipeline(\"text-generation\", model=\"openai-community/gpt2\")\n",
    "generator(input_text, max_length=20, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover  evaluation metrics,as well as safe and responsibilities practices when using LLMs in **Session 8**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also load in our own dataset and train a model with this data as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      " - defaults\n",
      "Platform: linux-64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "\n",
      "==> WARNING: A newer version of conda exists. <==\n",
      "    current version: 24.1.0\n",
      "    latest version: 24.3.0\n",
      "\n",
      "Please update conda by running\n",
      "\n",
      "    $ conda update -n base -c defaults conda\n",
      "\n",
      "\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TextDataset,DataCollatorForLanguageModeling\n",
    "\n",
    "def load_dataset(train_path,test_path,tokenizer):\n",
    "    train_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=train_path,\n",
    "          block_size=128) \n",
    "    test_dataset = TextDataset(\n",
    "          tokenizer=tokenizer,\n",
    "          file_path=test_path,\n",
    "          block_size=128)   \n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=False,\n",
    "    )\n",
    "    return train_dataset,test_dataset,data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "train_dataset,test_dataset,data_collator = load_dataset('dataset/train_input.txt','dataset/test_input.txt', tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForLanguageModeling(tokenizer=GPT2TokenizerFast(name_or_path='openai-community/gpt2', vocab_size=50257, model_max_length=1024, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
       "}, mlm=False, mlm_probability=0.15, pad_to_multiple_of=None, tf_experimental_compile=False, return_tensors='pt')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2\", #The output directory\n",
    "    overwrite_output_dir=True, #overwrite the content of the output directory\n",
    "    num_train_epochs=3, # number of training epochs\n",
    "    per_device_train_batch_size=32, # batch size for training\n",
    "    per_device_eval_batch_size=64,  # batch size for evaluation\n",
    "    eval_steps = 40, # Number of update steps between two evaluations.\n",
    "    save_steps=80, # after # steps model is saved \n",
    "    warmup_steps=50,# number of warmup steps for learning rate scheduler\n",
    "    prediction_loss_only=True,\n",
    "    )\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is going on below the hood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two components that are \"black-boxes\" here:\n",
    "1. The method for tokenization\n",
    "2. The model that generates novel text.\n",
    "\n",
    "Carlo Graziani already gave a great explanation of tokenization last week and how this affects embeddings (https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/Sequential_Data_Models.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we will take a closer look at how the model is designed to deal with language."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look inside GPT2! GPT2 incorporates the `GPT2LMHeadModel` architecture so let's inspect this more closely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "GPT2LMHeadModel                                    --\n",
       "├─GPT2Model: 1-1                                   --\n",
       "│    └─Embedding: 2-1                              38,597,376\n",
       "│    └─Embedding: 2-2                              786,432\n",
       "│    └─Dropout: 2-3                                --\n",
       "│    └─ModuleList: 2-4                             --\n",
       "│    │    └─GPT2Block: 3-1                         7,087,872\n",
       "│    │    └─GPT2Block: 3-2                         7,087,872\n",
       "│    │    └─GPT2Block: 3-3                         7,087,872\n",
       "│    │    └─GPT2Block: 3-4                         7,087,872\n",
       "│    │    └─GPT2Block: 3-5                         7,087,872\n",
       "│    │    └─GPT2Block: 3-6                         7,087,872\n",
       "│    │    └─GPT2Block: 3-7                         7,087,872\n",
       "│    │    └─GPT2Block: 3-8                         7,087,872\n",
       "│    │    └─GPT2Block: 3-9                         7,087,872\n",
       "│    │    └─GPT2Block: 3-10                        7,087,872\n",
       "│    │    └─GPT2Block: 3-11                        7,087,872\n",
       "│    │    └─GPT2Block: 3-12                        7,087,872\n",
       "│    └─LayerNorm: 2-5                              1,536\n",
       "├─Linear: 1-2                                      38,597,376\n",
       "===========================================================================\n",
       "Total params: 163,037,184\n",
       "Trainable params: 163,037,184\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "model = GPT2LMHeadModel.from_pretrained('openai-community/gpt2')\n",
    "print(model)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General elements of an LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPT-2 is an example of the popular Transformer architecture. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cig2mvfguetQ"
   },
   "source": [
    "<img src=\"images/decoder_only_block.png\" alt=\"Drawing\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "Image credit: https://arxiv.org/pdf/1706.03762.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gray section in this figure is the Transfomer Decoder and it is the main mechanism GPT2 uses to encode context of language into its predictions.\n",
    "\n",
    "<img src=\"images/transformer-decoder-intro.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer-Decoder is composed of Decoder blocks stacked ontop of each other where each contains two types of layers: \n",
    "1. Masked Self-Attention and \n",
    "2. Feed Forward Neural Networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already discussed Feed Forward Neural Networks in detail in the other lectures in this series. To review this, please look at https://github.com/argonne-lcf/ai-science-training-series/blob/main/02_intro_neural_networks/01_introduction_mnist.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture, we will \n",
    "* First, discuss attention mechanisms at length as this is arguably the greatest contribution by Transformers.\n",
    "* Second, extend the discussion from last week (https://github.com/argonne-lcf/ai-science-training-series/blob/main/04_intro_to_llms/Sequential_Data_Models.ipynb) on embedding input data while taking into account position.\n",
    "* Third, discuss outputting real text/sequences from the models.\n",
    "* Fourth, build a training loop for a mini-LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's set up all the imports we will need**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x74e3f40ec370>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## IMPORTS\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 100\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------\n",
    "\n",
    "torch.manual_seed(1337)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BowLYFlCrDrr"
   },
   "source": [
    "## Attention mechanisms\n",
    "\n",
    "Suppose the following sentence is an input sentence we want to translate using an LLM:\n",
    "\n",
    "`”The animal didn't cross the street because it was too tired”`\n",
    "\n",
    "Last week, Carlo mentioned that the Transformer learns an embedding of all words allowing interpretation of meanings of words.\n",
    "\n",
    "<img src=\"images/viz-bert-voc-verbs.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "So, if the model did a good job in token embedding, it will \"know\" what all the words in this sentence mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But to understand a full sentence, the model also need to understand what each word means in relation to other words.\n",
    "\n",
    "For example, when we read the sentence:\n",
    "`”The animal didn't cross the street because it was too tired”`\n",
    "we know intuitively that the word `\"it\"` refers to `\"animal\"`, the state for `\"it\"` is `\"tired\"`, and the associated action is `\"didn't cross\"`.\n",
    "\n",
    "However, the model needs a way to learn all of this information in a simple yet generalizable way.\n",
    "What makes Transformers particularly powerful compared to earlier sequential architectures is how it encodes context with the **self-attention mechanism**.\n",
    "\n",
    "As the model processes each word in the input sequence, attention looks at other positions in the input sequence for clues to a better understanding for this word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_self-attention_visualization.png\" alt=\"Drawing\" style=\"width: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UGbAi0cJ7x3a"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-attention mechanisms use 3 vectors to encode the context of a word in a sequence with another word:\n",
    "1. Query: the word representation we score other words against using the other word's keys\n",
    "2. Key: labels for the words in a sequence that we match against the query\n",
    "3. Value: actual word representation. We will use the queries and keys to score the word's relevance to the query, and multiply this by the value. \n",
    "\n",
    "An analogy provided by Jay Alammar is thinking about attention as choosing a file from a file cabinet according to information on a post-it note. You can use the post-it note (query) to identify the folder (key) that most matches the topic you are looking up. Then you access the contents of the file (value) according to its relevance to your query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/self-attention-example-folders-3.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Image credit: https://jalammar.github.io/illustrated-gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our models, we can encode queries, keys, and values using simple linear layers with the same size (`sequence length, head_size`). During the training process, these layers will be updated to best encode context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 32 # channels\n",
    "head_size = 16\n",
    "\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jzf9VE_AqWeR"
   },
   "source": [
    "The algorithm for self-attention is as follows:\n",
    "\n",
    "1. Generate query, key and value vectors for each word\n",
    "2. Calculate a score for each word in the input sentence against each other.\n",
    "3. Divide the scores by the square root of the dimension of the key vectors to stabilize the gradients. This is then passed through a softmax operation.\n",
    "4. Multiply each value vector by the softmax score.\n",
    "5. Sum up the weighted value vectors to produce the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/self-attention-output.png\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOwm-NkXA8U3"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how attention is performed in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 8, 32])\n",
      "Linear(in_features=32, out_features=16, bias=False)\n",
      "torch.Size([4, 8, 16]) torch.Size([4, 8, 16]) torch.Size([4, 8, 16])\n",
      "torch.Size([4, 8, 16])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "B,T,C = 4,8,32 # batch, time (sequence length), channels\n",
    "x = torch.randn(B,T,C)\n",
    "print(x.shape)\n",
    "#print(x)\n",
    "\n",
    "# Here we want the wei to be data dependent - ie gather info from the past but in a data dependant way\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "print(key)\n",
    "\n",
    "k = key(x)   # (B, T, 16) # each token here (totally B*T) produce a key and query in parallel and independently\n",
    "q = query(x) # (B, T, 16)\n",
    "v = value(x)\n",
    "print(k.shape, q.shape, v.shape)\n",
    "\n",
    "wei =  q @ k.transpose(-2, -1) * head_size**-0.5 # (B, T, 16) @ (B, 16, T) ---> (B, T, T). #\n",
    "wei = F.softmax(wei, dim=-1) # exponentiate and normalize giving a nice distibution that sums to 1 and\n",
    "                             # now it tells us that in a data dependent manner how much of info to aggregate from\n",
    "\n",
    "out = wei @ v # aggregate the attention scores and value vector.\n",
    "\n",
    "print(out.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0618, -0.0091, -0.3488,  0.3208,  0.2971, -0.1573, -0.0561,  0.1068,\n",
      "          0.0368,  0.0139, -0.0017,  0.3110,  0.1404, -0.0158,  0.1853,  0.4290],\n",
      "        [ 0.1578, -0.0971, -0.4256,  0.3538,  0.3621, -0.2392, -0.0536,  0.1759,\n",
      "          0.1115,  0.0282, -0.0649,  0.3641,  0.1928,  0.0261,  0.2162,  0.3758],\n",
      "        [ 0.1293,  0.0759, -0.2946,  0.2292,  0.2215, -0.0710, -0.0107,  0.1616,\n",
      "         -0.0930, -0.0877,  0.0567,  0.1899,  0.0311, -0.0894,  0.0309,  0.5471],\n",
      "        [ 0.1247,  0.1400, -0.2436,  0.1819,  0.1976,  0.0338, -0.0028,  0.1124,\n",
      "         -0.1477, -0.0748,  0.0650,  0.1392, -0.0314, -0.0989,  0.0613,  0.5433],\n",
      "        [ 0.0667,  0.1845, -0.2135,  0.2813,  0.2064,  0.0873,  0.0084,  0.2055,\n",
      "         -0.1130, -0.1466,  0.0459,  0.1923, -0.0275, -0.1107,  0.0065,  0.4674],\n",
      "        [ 0.1924,  0.1693, -0.1568,  0.2284,  0.1620,  0.0737,  0.0443,  0.2519,\n",
      "         -0.1912, -0.1979,  0.0832,  0.0713, -0.0826, -0.0848, -0.1047,  0.6089],\n",
      "        [ 0.1184,  0.0884, -0.2652,  0.2560,  0.1840,  0.0284, -0.0621,  0.1181,\n",
      "         -0.0880,  0.0104,  0.1123,  0.1850,  0.0369, -0.0730,  0.0663,  0.5242],\n",
      "        [ 0.1243,  0.0453, -0.3412,  0.2709,  0.2335, -0.0948, -0.0421,  0.2143,\n",
      "         -0.0330, -0.0313,  0.0520,  0.2378,  0.1084, -0.0959,  0.0300,  0.4707]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(out[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lwyFlxKW6oA"
   },
   "source": [
    "### Multi-head attention\n",
    "\n",
    "In practice, multiple attention heads are used which\n",
    "1. Expands the model’s ability to focus on different positions and prevent the attention to be dominated by the word itself.\n",
    "2. Have multiple “representation subspaces”. Have multiple sets of Query/Key/Value weight matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_multi-headed_self-attention-recap.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oHsezdVBIaf"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see attention mechanisms in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to use the powerful visualization tool bertviz, which allows an interactive experience of the attention mechanisms. Normally these mechanisms are abstracted away but this will allow us to inspect our model in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bertviz in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (1.4.0)\n",
      "Requirement already satisfied: transformers>=2.0 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from bertviz) (4.39.2)\n",
      "Requirement already satisfied: torch>=1.0 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from bertviz) (2.2.2)\n",
      "Requirement already satisfied: tqdm in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from bertviz) (4.66.2)\n",
      "Requirement already satisfied: boto3 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from bertviz) (1.34.77)\n",
      "Requirement already satisfied: requests in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from bertviz) (2.31.0)\n",
      "Requirement already satisfied: regex in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from bertviz) (2023.12.25)\n",
      "Requirement already satisfied: sentencepiece in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from bertviz) (0.2.0)\n",
      "Requirement already satisfied: filelock in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from torch>=1.0->bertviz) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from torch>=1.0->bertviz) (4.9.0)\n",
      "Requirement already satisfied: sympy in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from torch>=1.0->bertviz) (1.12)\n",
      "Requirement already satisfied: networkx in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from torch>=1.0->bertviz) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from torch>=1.0->bertviz) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from torch>=1.0->bertviz) (2024.2.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from transformers>=2.0->bertviz) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from transformers>=2.0->bertviz) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from transformers>=2.0->bertviz) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from transformers>=2.0->bertviz) (6.0.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from transformers>=2.0->bertviz) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from transformers>=2.0->bertviz) (0.4.2)\n",
      "Requirement already satisfied: botocore<1.35.0,>=1.34.77 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from boto3->bertviz) (1.34.77)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from boto3->bertviz) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from boto3->bertviz) (0.10.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from requests->bertviz) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from requests->bertviz) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from requests->bertviz) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from requests->bertviz) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from botocore<1.35.0,>=1.34.77->boto3->bertviz) (2.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from jinja2->torch>=1.0->bertviz) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from sympy->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/azzaea/miniconda3/envs/ai-science-training-series/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.77->boto3->bertviz) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install bertviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load in the model, GPT2 and look at the attention mechanisms. \n",
    "\n",
    "**Hint... click on the different blocks in the visualization to see the attention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\"></script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "      \n",
       "        <div id=\"bertviz-44469e5e4085440eb4a5d60e6255af5b\" style=\"font-family:'Helvetica Neue', Helvetica, Arial, sans-serif;\">\n",
       "            <span style=\"user-select:none\">\n",
       "                \n",
       "            </span>\n",
       "            <div id='vis'></div>\n",
       "        </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "/**\n * @fileoverview Transformer Visualization D3 javascript code.\n *\n * Based on: https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/visualization/attention.js\n *\n * Change log:\n *\n * 02/01/19  Jesse Vig   Initial implementation\n * 12/31/20  Jesse Vig   Support multiple visualizations in single notebook.\n * 01/19/21  Jesse Vig   Support light/dark modes\n * 02/06/21  Jesse Vig   Move require config from separate jupyter notebook step\n * 05/03/21  Jesse Vig   Adjust visualization height dynamically\n * 03/23/22  Daniel SC   Update requirement URLs for d3 and jQuery (source of bug not allowing end result to be displayed on browsers)\n **/\n\nrequire.config({\n  paths: {\n      d3: 'https://cdnjs.cloudflare.com/ajax/libs/d3/5.7.0/d3.min',\n    jquery: 'https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.0/jquery.min',\n  }\n});\n\nrequirejs(['jquery', 'd3'], function($, d3) {\n\n        const params = {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.03878015652298927, 0.0, 0.0, 0.0, 0.0], [0.7466979622840881, 0.11987316608428955, 0.13342881202697754, 0.0, 0.0, 0.0], [0.5885031223297119, 0.13792064785957336, 0.2121373564004898, 0.06143894046545029, 0.0, 0.0], [0.657085657119751, 0.08996300399303436, 0.12751281261444092, 0.08361566811800003, 0.041822824627161026, 0.0], [0.2728872299194336, 0.11203358322381973, 0.1663985550403595, 0.08467113971710205, 0.1695273518562317, 0.19448217749595642]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616553947329521, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677494075149298, 0.008447976782917976, 0.9890843033790588, 0.0, 0.0, 0.0], [0.00012328437878750265, 0.0018733113538473845, 0.013126927427947521, 0.9848765134811401, 0.0, 0.0], [0.001066954922862351, 0.0011366254184395075, 0.003034990979358554, 0.0015735066263005137, 0.9931879043579102, 0.0], [0.00019792039529420435, 0.001052812789566815, 0.001543740276247263, 0.0009642774821259081, 3.492446558084339e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4757843613624573, 0.524215579032898, 0.0, 0.0, 0.0, 0.0], [0.5906046032905579, 0.24866101145744324, 0.16073429584503174, 0.0, 0.0, 0.0], [0.5529290437698364, 0.18856696784496307, 0.14457567036151886, 0.11392836272716522, 0.0, 0.0], [0.45094093680381775, 0.16486790776252747, 0.1731802374124527, 0.11748012155294418, 0.0935308039188385, 0.0], [0.42572474479675293, 0.17328651249408722, 0.15651947259902954, 0.07022644579410553, 0.08087006211280823, 0.09337273985147476]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133620738983154, 0.38663801550865173, 0.0, 0.0, 0.0, 0.0], [0.06098512187600136, 0.032534655183553696, 0.9064802527427673, 0.0, 0.0, 0.0], [0.006717090494930744, 0.00040128923137672246, 0.7572956681251526, 0.23558591306209564, 0.0, 0.0], [0.037227679044008255, 0.002948855282738805, 0.10081084817647934, 0.04142273962497711, 0.8175898790359497, 0.0], [0.049897726625204086, 0.0003075831336900592, 0.0024198247119784355, 0.0034334915690124035, 0.0006823895964771509, 0.9432590007781982]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.051044415682554245, 0.0, 0.0, 0.0, 0.0], [0.6821408271789551, 0.13952413201332092, 0.1783350259065628, 0.0, 0.0, 0.0], [0.20366324484348297, 0.05641485005617142, 0.06399302184581757, 0.6759289503097534, 0.0, 0.0], [0.34195467829704285, 0.06725437194108963, 0.0792618989944458, 0.17836204171180725, 0.33316701650619507, 0.0], [0.09464015811681747, 0.007428203243762255, 0.006983973551541567, 0.007184373214840889, 0.018724260851740837, 0.865039050579071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3383461534976959, 0.6616538166999817, 0.0, 0.0, 0.0, 0.0], [0.0785597637295723, 0.006165447179228067, 0.9152747988700867, 0.0, 0.0, 0.0], [0.016775967553257942, 0.0004037704202346504, 0.0033404622226953506, 0.9794798493385315, 0.0, 0.0], [0.02760038524866104, 0.0004441516939550638, 0.0006541667389683425, 0.00022661815455649048, 0.971074640750885, 0.0], [0.01024815533310175, 3.70154702977743e-05, 0.00016064026567619294, 2.7341822715243325e-05, 1.018727652990492e-05, 0.9895166754722595]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.01749664917588234, 0.0, 0.0, 0.0, 0.0], [0.8874197602272034, 0.05467934533953667, 0.05790084972977638, 0.0, 0.0, 0.0], [0.6849910616874695, 0.1228068396449089, 0.04972025007009506, 0.14248186349868774, 0.0, 0.0], [0.6015853881835938, 0.09881889075040817, 0.07070115208625793, 0.1665254533290863, 0.06236908212304115, 0.0], [0.32325056195259094, 0.12567417323589325, 0.044321782886981964, 0.0707697719335556, 0.06606651842594147, 0.36991724371910095]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352679014206, 0.0, 0.0, 0.0, 0.0], [0.45986419916152954, 0.39703109860420227, 0.14310474693775177, 0.0, 0.0, 0.0], [0.30038732290267944, 0.2218172401189804, 0.38161519169807434, 0.09618023037910461, 0.0, 0.0], [0.18963932991027832, 0.13763730227947235, 0.2017350047826767, 0.2363216131925583, 0.23466676473617554, 0.0], [0.15410445630550385, 0.09489510953426361, 0.11902561783790588, 0.10277969390153885, 0.4317217767238617, 0.09747332334518433]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36500003933906555, 0.6349999904632568, 0.0, 0.0, 0.0, 0.0], [0.24595220386981964, 0.5519202351570129, 0.2021276205778122, 0.0, 0.0, 0.0], [0.2721358835697174, 0.40738624334335327, 0.25186216831207275, 0.06861573457717896, 0.0, 0.0], [0.10242553800344467, 0.16683615744113922, 0.5248048901557922, 0.054454632103443146, 0.15147878229618073, 0.0], [0.2502950131893158, 0.2219812572002411, 0.18899965286254883, 0.10677117109298706, 0.13032686710357666, 0.10162601619958878]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990506052970886, 0.3009493947029114, 0.0, 0.0, 0.0, 0.0], [0.5107942223548889, 0.2948642373085022, 0.1943415254354477, 0.0, 0.0, 0.0], [0.4604707360267639, 0.2805190682411194, 0.19174803793430328, 0.06726215034723282, 0.0, 0.0], [0.3764842450618744, 0.21120667457580566, 0.20214542746543884, 0.10207020491361618, 0.10809347778558731, 0.0], [0.30138447880744934, 0.20456182956695557, 0.18250326812267303, 0.11019383370876312, 0.16291271150112152, 0.03844381868839264]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131583094596863, 0.2868416905403137, 0.0, 0.0, 0.0, 0.0], [0.40588000416755676, 0.18063302338123322, 0.4134870171546936, 0.0, 0.0, 0.0], [0.265546053647995, 0.16985857486724854, 0.3358593285083771, 0.22873607277870178, 0.0, 0.0], [0.31385403871536255, 0.18316693603992462, 0.14928360283374786, 0.053776729851961136, 0.2999187111854553, 0.0], [0.20466555655002594, 0.18731114268302917, 0.15959151089191437, 0.06381774693727493, 0.036423034965991974, 0.3481909930706024]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242318153381, 0.34137579798698425, 0.0, 0.0, 0.0, 0.0], [0.5917776226997375, 0.31600359082221985, 0.09221873432397842, 0.0, 0.0, 0.0], [0.5477152466773987, 0.23586955666542053, 0.06145599111914635, 0.15495921671390533, 0.0, 0.0], [0.45870599150657654, 0.22440002858638763, 0.07887427508831024, 0.09920337796211243, 0.13881628215312958, 0.0], [0.3274371922016144, 0.1960083246231079, 0.06805712729692459, 0.0892510637640953, 0.11618072539567947, 0.20306558907032013]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448371924459934, 0.0, 0.0, 0.0, 0.0], [0.8594854474067688, 0.06906108558177948, 0.07145342975854874, 0.0, 0.0, 0.0], [0.38000524044036865, 0.041275594383478165, 0.5496614575386047, 0.029057739302515984, 0.0, 0.0], [0.21445223689079285, 0.0508873388171196, 0.4317440986633301, 0.25869300961494446, 0.044223327189683914, 0.0], [0.111752949655056, 0.01759309507906437, 0.027507491409778595, 0.04086776077747345, 0.7754665017127991, 0.02681216225028038]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285967946052551, 0.07140325009822845, 0.0, 0.0, 0.0, 0.0], [0.6077285408973694, 0.3121427297592163, 0.08012861758470535, 0.0, 0.0, 0.0], [0.49429091811180115, 0.28503692150115967, 0.11849319934844971, 0.10217898339033127, 0.0, 0.0], [0.41838788986206055, 0.23117896914482117, 0.08340620994567871, 0.11365960538387299, 0.15336722135543823, 0.0], [0.4221586287021637, 0.1291714310646057, 0.08740921318531036, 0.10163760185241699, 0.2123025506734848, 0.04732052609324455]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786477088928223, 0.02135234884917736, 0.0, 0.0, 0.0, 0.0], [0.7749120593070984, 0.06510375440120697, 0.15998421609401703, 0.0, 0.0, 0.0], [0.6484923362731934, 0.07483138144016266, 0.14751607179641724, 0.12916028499603271, 0.0, 0.0], [0.5224635601043701, 0.06921815872192383, 0.13823407888412476, 0.11106593906879425, 0.1590181291103363, 0.0], [0.39645180106163025, 0.0732581615447998, 0.12938156723976135, 0.10642433166503906, 0.14863993227481842, 0.1458442062139511]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525904893875122, 0.4474094808101654, 0.0, 0.0, 0.0, 0.0], [0.5585010051727295, 0.21762590110301971, 0.2238730788230896, 0.0, 0.0, 0.0], [0.5143129229545593, 0.15964671969413757, 0.15491962432861328, 0.17112071812152863, 0.0, 0.0], [0.5039961934089661, 0.11401884257793427, 0.11974027007818222, 0.12552587687969208, 0.13671886920928955, 0.0], [0.5061841607093811, 0.08567395061254501, 0.0890301913022995, 0.09759818762540817, 0.1027572751045227, 0.11875622719526291]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545962333679, 0.07574538886547089, 0.0, 0.0, 0.0, 0.0], [0.8257425427436829, 0.07932531088590622, 0.0949321761727333, 0.0, 0.0, 0.0], [0.7306380271911621, 0.0857183039188385, 0.08043929189443588, 0.1032043918967247, 0.0, 0.0], [0.6383241415023804, 0.07886382192373276, 0.07815025746822357, 0.08758099377155304, 0.11708082258701324, 0.0], [0.5552158951759338, 0.07409116625785828, 0.06834886968135834, 0.07778596878051758, 0.09999310225248337, 0.12456490099430084]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578917384147644, 0.14210829138755798, 0.0, 0.0, 0.0, 0.0], [0.6423039436340332, 0.16629017889499664, 0.19140596687793732, 0.0, 0.0, 0.0], [0.5530980229377747, 0.10609276592731476, 0.0782126635313034, 0.2625965178012848, 0.0, 0.0], [0.4012170732021332, 0.12223611772060394, 0.19347338378429413, 0.14164617657661438, 0.14142730832099915, 0.0], [0.40212586522102356, 0.1845075786113739, 0.07516815513372421, 0.058490391820669174, 0.14446333050727844, 0.13524462282657623]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791558980941772, 0.020844051614403725, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.06233251467347145, 0.0546833835542202, 0.0, 0.0, 0.0], [0.8105454444885254, 0.08617089688777924, 0.07321780920028687, 0.030065832659602165, 0.0, 0.0], [0.6819813847541809, 0.04990820214152336, 0.08296553045511246, 0.08369521796703339, 0.10144977271556854, 0.0], [0.40566882491111755, 0.07337658107280731, 0.08601409196853638, 0.061709288507699966, 0.1322644054889679, 0.24096685647964478]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670192003250122, 0.032980870455503464, 0.0, 0.0, 0.0, 0.0], [0.8449064493179321, 0.0851450115442276, 0.0699484571814537, 0.0, 0.0, 0.0], [0.7123573422431946, 0.07896048575639725, 0.05541076511144638, 0.15327146649360657, 0.0, 0.0], [0.6402613520622253, 0.07397552579641342, 0.04439307376742363, 0.14322122931480408, 0.09814891964197159, 0.0], [0.5073902010917664, 0.0752306878566742, 0.07754657417535782, 0.11362474411725998, 0.13947954773902893, 0.08672831207513809]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8487569689750671, 0.15124309062957764, 0.0, 0.0, 0.0, 0.0], [0.8415650725364685, 0.1210721880197525, 0.03736279159784317, 0.0, 0.0, 0.0], [0.7505519986152649, 0.11348932981491089, 0.061799585819244385, 0.07415914535522461, 0.0, 0.0], [0.6614717841148376, 0.1024264469742775, 0.05293431878089905, 0.07529717683792114, 0.10787024348974228, 0.0], [0.6014202833175659, 0.11340400576591492, 0.056319333612918854, 0.07096728682518005, 0.10906249284744263, 0.048826564103364944]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.944548487663269, 0.05545147880911827, 0.0, 0.0, 0.0, 0.0], [0.8874568939208984, 0.054742131382226944, 0.05780097469687462, 0.0, 0.0, 0.0], [0.8281886577606201, 0.06895007938146591, 0.05903475359082222, 0.04382640868425369, 0.0, 0.0], [0.6429896950721741, 0.06747541576623917, 0.11629696935415268, 0.05417948216199875, 0.11905848979949951, 0.0], [0.7367826104164124, 0.05611899122595787, 0.06857284158468246, 0.03421953693032265, 0.07875366508960724, 0.02555239573121071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913392090704292, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981191156432033, 0.5288337469100952, 0.4703681170940399, 0.0, 0.0, 0.0], [0.0007648480823263526, 0.34519821405410767, 0.30852678418159485, 0.3455101549625397, 0.0, 0.0], [0.0010283153969794512, 0.24135911464691162, 0.23320147395133972, 0.2555714547634125, 0.26883962750434875, 0.0], [0.0009746804134920239, 0.1778969019651413, 0.16743157804012299, 0.18587613105773926, 0.18734432756900787, 0.2804763615131378]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8244929909706116, 0.17550702393054962, 0.0, 0.0, 0.0, 0.0], [0.12386861443519592, 0.044499851763248444, 0.831631600856781, 0.0, 0.0, 0.0], [0.07924358546733856, 0.012965889647603035, 0.0015277165221050382, 0.9062628149986267, 0.0, 0.0], [0.08806417882442474, 0.021341092884540558, 0.0028886243235319853, 0.0028453839477151632, 0.8848607540130615, 0.0], [0.09983230382204056, 0.033633969724178314, 0.005500008352100849, 0.0024330539163202047, 0.0015082466416060925, 0.8570924401283264]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646892547607422, 0.0353107713162899, 0.0, 0.0, 0.0, 0.0], [0.7529153823852539, 0.08733484148979187, 0.1597498059272766, 0.0, 0.0, 0.0], [0.42022794485092163, 0.09195111691951752, 0.23549869656562805, 0.252322256565094, 0.0, 0.0], [0.30848926305770874, 0.05908147618174553, 0.3839130699634552, 0.1565914750099182, 0.09192472696304321, 0.0], [0.44790419936180115, 0.04329308494925499, 0.07969187945127487, 0.11081928014755249, 0.22124579548835754, 0.09704567492008209]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.008904017508029938, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084503158926964, 0.004147922620177269, 0.0, 0.0, 0.0], [0.9082901477813721, 0.033206067979335785, 0.00942116230726242, 0.049082688987255096, 0.0, 0.0], [0.8949135541915894, 0.055445462465286255, 0.005577605217695236, 0.031506799161434174, 0.01255655474960804, 0.0], [0.8497735857963562, 0.028890162706375122, 0.0036647969391196966, 0.0375199131667614, 0.03842795267701149, 0.04172346368432045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474557876586914, 0.0, 0.0, 0.0, 0.0], [0.4894774556159973, 0.4812203645706177, 0.029302187263965607, 0.0, 0.0, 0.0], [0.11772160977125168, 0.13121220469474792, 0.6702309250831604, 0.08083526045084, 0.0, 0.0], [0.1304369419813156, 0.04068683832883835, 0.26520389318466187, 0.41143542528152466, 0.15223684906959534, 0.0], [0.1266191005706787, 0.032751258462667465, 0.03567875921726227, 0.06039170175790787, 0.6021819114685059, 0.14237721264362335]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.980517566204071, 0.01948240026831627, 0.0, 0.0, 0.0, 0.0], [0.7948852777481079, 0.12061893939971924, 0.08449575304985046, 0.0, 0.0, 0.0], [0.5612363815307617, 0.1574312299489975, 0.20339664816856384, 0.07793566584587097, 0.0, 0.0], [0.4258384108543396, 0.10742022097110748, 0.15123607218265533, 0.08755000680685043, 0.22795531153678894, 0.0], [0.24752643704414368, 0.024188265204429626, 0.03039526753127575, 0.08586962521076202, 0.5714335441589355, 0.04058679938316345]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.01122323703020811, 0.0, 0.0, 0.0, 0.0], [0.7572692036628723, 0.2231736183166504, 0.01955709606409073, 0.0, 0.0, 0.0], [0.5341880917549133, 0.2210758924484253, 0.17621803283691406, 0.0685180276632309, 0.0, 0.0], [0.17095300555229187, 0.08229414373636246, 0.5760217905044556, 0.11097608506679535, 0.05975497514009476, 0.0], [0.24871177971363068, 0.08880802243947983, 0.08980196714401245, 0.0972934365272522, 0.4413082003593445, 0.03407653048634529]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422131538391113, 0.15778684616088867, 0.0, 0.0, 0.0, 0.0], [0.4684130549430847, 0.46105337142944336, 0.0705336406826973, 0.0, 0.0, 0.0], [0.25881415605545044, 0.46358874440193176, 0.18503506481647491, 0.0925619900226593, 0.0, 0.0], [0.18399569392204285, 0.2915416359901428, 0.17031100392341614, 0.271729975938797, 0.0824216827750206, 0.0], [0.16469906270503998, 0.2472696751356125, 0.08770576864480972, 0.22574983537197113, 0.17745333909988403, 0.09712236374616623]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005395531654358, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.04406513273715973, 0.04906249791383743, 0.0, 0.0, 0.0], [0.8582223653793335, 0.055348243564367294, 0.04041939601302147, 0.04601001366972923, 0.0, 0.0], [0.7855250835418701, 0.04124249890446663, 0.08369286358356476, 0.04887617379426956, 0.04066333919763565, 0.0], [0.7856320142745972, 0.0501464307308197, 0.04751263186335564, 0.02736588381230831, 0.056147389113903046, 0.03319568186998367]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041037559509277, 0.09589625895023346, 0.0, 0.0, 0.0, 0.0], [0.5862313508987427, 0.07199815660715103, 0.3417705297470093, 0.0, 0.0, 0.0], [0.3878959119319916, 0.046608008444309235, 0.20279017090797424, 0.3627059757709503, 0.0, 0.0], [0.26652413606643677, 0.024533025920391083, 0.12211950123310089, 0.20041197538375854, 0.38641130924224854, 0.0], [0.2335742563009262, 0.020537324249744415, 0.09610339999198914, 0.13062246143817902, 0.22990480065345764, 0.28925779461860657]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.963991105556488, 0.03600893169641495, 0.0, 0.0, 0.0, 0.0], [0.7075551152229309, 0.25427767634391785, 0.038167133927345276, 0.0, 0.0, 0.0], [0.25665274262428284, 0.20589302480220795, 0.01665661111474037, 0.520797610282898, 0.0, 0.0], [0.10379385948181152, 0.04639101028442383, 0.008698644116520882, 0.7866848707199097, 0.05443154275417328, 0.0], [0.22143402695655823, 0.0337974950671196, 0.029023971408605576, 0.5412918925285339, 0.15286105871200562, 0.021591659635305405]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.989170253276825, 0.010829685255885124, 0.0, 0.0, 0.0, 0.0], [0.7913153767585754, 0.12309646606445312, 0.08558811247348785, 0.0, 0.0, 0.0], [0.2954603433609009, 0.15808320045471191, 0.4217235743999481, 0.12473281472921371, 0.0, 0.0], [0.23441055417060852, 0.0988653227686882, 0.3316012918949127, 0.19713936746120453, 0.13798347115516663, 0.0], [0.1972840130329132, 0.05741839483380318, 0.06909026205539703, 0.16469816863536835, 0.27972766757011414, 0.23178143799304962]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127879142761, 0.06408723443746567, 0.0, 0.0, 0.0, 0.0], [0.7888628244400024, 0.08673480153083801, 0.12440239638090134, 0.0, 0.0, 0.0], [0.6535121202468872, 0.07573549449443817, 0.09732560813426971, 0.17342683672904968, 0.0, 0.0], [0.5222765207290649, 0.058278851211071014, 0.09920472651720047, 0.1702084243297577, 0.1500314474105835, 0.0], [0.4108835756778717, 0.04730607569217682, 0.07265683263540268, 0.10560745000839233, 0.10550019145011902, 0.25804585218429565]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161671757698059, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.03887055814266205, 0.06458978354930878, 0.0, 0.0, 0.0], [0.8264951705932617, 0.03213461861014366, 0.05196719244122505, 0.08940298110246658, 0.0, 0.0], [0.7718172073364258, 0.030402861535549164, 0.0458274744451046, 0.07118476927280426, 0.08076761662960052, 0.0], [0.7292329668998718, 0.021699856966733932, 0.03307478129863739, 0.047200966626405716, 0.06474560499191284, 0.10404585301876068]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432916935533285, 0.0, 0.0, 0.0, 0.0], [0.9552939534187317, 0.008025328628718853, 0.036680713295936584, 0.0, 0.0, 0.0], [0.9254708290100098, 0.002755587687715888, 0.0020629996433854103, 0.06971065700054169, 0.0, 0.0], [0.8660573363304138, 0.0038883851375430822, 0.000678603071719408, 0.0006981496699154377, 0.12867748737335205, 0.0], [0.8455931544303894, 0.003780404571443796, 0.00025342340813949704, 6.0270424000918865e-05, 0.00011820694635389373, 0.1501944661140442]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926245391368866, 0.07375462353229523, 0.0, 0.0, 0.0, 0.0], [0.7717148065567017, 0.16242054104804993, 0.06586459279060364, 0.0, 0.0, 0.0], [0.8167630434036255, 0.07807210832834244, 0.0632404014468193, 0.041924409568309784, 0.0, 0.0], [0.6867170333862305, 0.0775521919131279, 0.10056910663843155, 0.05955108255147934, 0.07561051100492477, 0.0], [0.6421169638633728, 0.11014891415834427, 0.07688148319721222, 0.0540333017706871, 0.10333617776632309, 0.013483177870512009]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395950436592102, 0.06040492281317711, 0.0, 0.0, 0.0, 0.0], [0.23004497587680817, 0.6617403626441956, 0.10821466147899628, 0.0, 0.0, 0.0], [0.26702186465263367, 0.3607969880104065, 0.32496148347854614, 0.04721970111131668, 0.0, 0.0], [0.5952020287513733, 0.12269298732280731, 0.06302020698785782, 0.08916758000850677, 0.1299172192811966, 0.0], [0.10284583270549774, 0.02938019670546055, 0.013739015907049179, 0.04586033150553703, 0.7698504328727722, 0.038324225693941116]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9040982723236084, 0.09590171277523041, 0.0, 0.0, 0.0, 0.0], [0.3572365343570709, 0.6274626851081848, 0.015300804749131203, 0.0, 0.0, 0.0], [0.5917993783950806, 0.27640512585639954, 0.1047598347067833, 0.027035698294639587, 0.0, 0.0], [0.725443422794342, 0.04983116313815117, 0.014982681721448898, 0.17781180143356323, 0.03193084895610809, 0.0], [0.7612781524658203, 0.06158874183893204, 0.005942091345787048, 0.01642647571861744, 0.12677863240242004, 0.027985820546746254]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.0052412403747439384, 0.0, 0.0, 0.0, 0.0], [0.9632413387298584, 0.017816439270973206, 0.018942102789878845, 0.0, 0.0, 0.0], [0.9671077132225037, 0.00850959774106741, 0.008562250062823296, 0.015820471569895744, 0.0, 0.0], [0.9340996146202087, 0.011952398344874382, 0.020180240273475647, 0.026750784367322922, 0.007016909774392843, 0.0], [0.9587239027023315, 0.0046570878475904465, 0.003326777368783951, 0.006545277312397957, 0.010182438418269157, 0.0165645033121109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000895977020264, 0.0, 0.0, 0.0, 0.0], [0.7917605638504028, 0.17533239722251892, 0.032907020300626755, 0.0, 0.0, 0.0], [0.7949193120002747, 0.10531842708587646, 0.04021839052438736, 0.05954379960894585, 0.0, 0.0], [0.7097724676132202, 0.10552524775266647, 0.06597544252872467, 0.05765583738684654, 0.06107091158628464, 0.0], [0.7506600022315979, 0.02651444636285305, 0.021576009690761566, 0.034296732395887375, 0.08494473248720169, 0.08200826495885849]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9837515950202942, 0.0162484273314476, 0.0, 0.0, 0.0, 0.0], [0.5615504384040833, 0.0895683541893959, 0.34888115525245667, 0.0, 0.0, 0.0], [0.32929134368896484, 0.024114875122904778, 0.5428050756454468, 0.10378869622945786, 0.0, 0.0], [0.3433033525943756, 0.013086446560919285, 0.5121970772743225, 0.1114620715379715, 0.019951023161411285, 0.0], [0.47928258776664734, 0.01733352243900299, 0.11805258691310883, 0.061302680522203445, 0.20071890950202942, 0.12330960482358932]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115303866565228, 0.0, 0.0, 0.0, 0.0], [0.5282692313194275, 0.3292275369167328, 0.14250318706035614, 0.0, 0.0, 0.0], [0.48788610100746155, 0.23368680477142334, 0.17578008770942688, 0.10264701396226883, 0.0, 0.0], [0.3144499659538269, 0.18065093457698822, 0.1687125414609909, 0.0950651690363884, 0.24112141132354736, 0.0], [0.5168768763542175, 0.03589698299765587, 0.026187948882579803, 0.040397047996520996, 0.18791775405406952, 0.1927233636379242]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750301003456116, 0.12496990710496902, 0.0, 0.0, 0.0, 0.0], [0.45505934953689575, 0.4900447130203247, 0.05489590764045715, 0.0, 0.0, 0.0], [0.29337161779403687, 0.5449915528297424, 0.0944426879286766, 0.06719415634870529, 0.0, 0.0], [0.4897075593471527, 0.2721004784107208, 0.06861964613199234, 0.14694835245609283, 0.022623902186751366, 0.0], [0.4729074239730835, 0.08103105425834656, 0.016052115708589554, 0.30672237277030945, 0.10120674967765808, 0.02208031341433525]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.963021993637085, 0.03697802126407623, 0.0, 0.0, 0.0, 0.0], [0.7557188272476196, 0.1643645018339157, 0.07991666346788406, 0.0, 0.0, 0.0], [0.694770872592926, 0.08409887552261353, 0.06382588297128677, 0.15730437636375427, 0.0, 0.0], [0.5821155905723572, 0.03297805413603783, 0.0793655514717102, 0.19441358745098114, 0.11112719774246216, 0.0], [0.5974549651145935, 0.04261094331741333, 0.06919678300619125, 0.14563405513763428, 0.12481731921434402, 0.0202859565615654]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217814654111862, 0.0, 0.0, 0.0, 0.0], [0.9312834143638611, 0.010560267604887486, 0.058156415820121765, 0.0, 0.0, 0.0], [0.8435331583023071, 0.015694988891482353, 0.04575095698237419, 0.09502096474170685, 0.0, 0.0], [0.7724104523658752, 0.011981240473687649, 0.03504590317606926, 0.038767579942941666, 0.1417948305606842, 0.0], [0.7642903923988342, 0.009868821129202843, 0.008122745901346207, 0.013314379379153252, 0.04824402555823326, 0.15615971386432648]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701176285743713, 0.029882336035370827, 0.0, 0.0, 0.0, 0.0], [0.6563995480537415, 0.2250628024339676, 0.11853764951229095, 0.0, 0.0, 0.0], [0.6958045363426208, 0.1470196545124054, 0.07146001607179642, 0.08571576327085495, 0.0, 0.0], [0.6353277564048767, 0.13460667431354523, 0.030994096770882607, 0.05691614747047424, 0.14215528964996338, 0.0], [0.6779407262802124, 0.053654126822948456, 0.01800624467432499, 0.06284503638744354, 0.11038195341825485, 0.0771719217300415]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.01776665635406971, 0.0, 0.0, 0.0, 0.0], [0.9037663340568542, 0.06541521847248077, 0.030818503350019455, 0.0, 0.0, 0.0], [0.8119180798530579, 0.0367904007434845, 0.060561105608940125, 0.09073032438755035, 0.0, 0.0], [0.4054645299911499, 0.10383853316307068, 0.10211227834224701, 0.35434234142303467, 0.034242235124111176, 0.0], [0.22824543714523315, 0.01727859117090702, 0.05055435001850128, 0.60157310962677, 0.09411708265542984, 0.00823147315531969]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685172259807587, 0.0, 0.0, 0.0, 0.0], [0.3544574975967407, 0.5317599177360535, 0.1137826219201088, 0.0, 0.0, 0.0], [0.07823366671800613, 0.7221353054046631, 0.10936662554740906, 0.0902644619345665, 0.0, 0.0], [0.2196805477142334, 0.4048425555229187, 0.12358102947473526, 0.2001877725124359, 0.051708076149225235, 0.0], [0.3608982563018799, 0.10459048300981522, 0.06983845680952072, 0.2976471781730652, 0.13869889080524445, 0.02832678146660328]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.026783788576722145, 0.0, 0.0, 0.0, 0.0], [0.9167553782463074, 0.06145269423723221, 0.021791934967041016, 0.0, 0.0, 0.0], [0.8543075919151306, 0.08049620687961578, 0.030335109680891037, 0.034860990941524506, 0.0, 0.0], [0.8919222354888916, 0.04280753806233406, 0.022044949233531952, 0.02347046323120594, 0.019754746928811073, 0.0], [0.8116774559020996, 0.03413516283035278, 0.0356765054166317, 0.047485511749982834, 0.025396987795829773, 0.04562840983271599]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761363983154, 0.04972385615110397, 0.0, 0.0, 0.0, 0.0], [0.7637456059455872, 0.20073604583740234, 0.03551841527223587, 0.0, 0.0, 0.0], [0.6279088854789734, 0.03768148273229599, 0.1994541436433792, 0.13495543599128723, 0.0, 0.0], [0.6397086381912231, 0.02700713276863098, 0.09081915766000748, 0.20653723180294037, 0.035927820950746536, 0.0], [0.4559429883956909, 0.021641205996274948, 0.1293955296278, 0.2180088311433792, 0.10379859805107117, 0.07121279090642929]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498406648635864, 0.05015937238931656, 0.0, 0.0, 0.0, 0.0], [0.8688726425170898, 0.08722181618213654, 0.04390554130077362, 0.0, 0.0, 0.0], [0.6937952637672424, 0.0635920837521553, 0.09179038554430008, 0.15082231163978577, 0.0, 0.0], [0.7266590595245361, 0.04389895126223564, 0.04683984816074371, 0.09851840138435364, 0.08408375084400177, 0.0], [0.7848993539810181, 0.03714781999588013, 0.012907813303172588, 0.010539380833506584, 0.12079199403524399, 0.03371361270546913]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.010894606821238995, 0.0, 0.0, 0.0, 0.0], [0.892951250076294, 0.08700107038021088, 0.020047664642333984, 0.0, 0.0, 0.0], [0.7891112565994263, 0.09797292202711105, 0.08633271604776382, 0.02658306621015072, 0.0, 0.0], [0.885063886642456, 0.036449968814849854, 0.05395442247390747, 0.012377167120575905, 0.012154590338468552, 0.0], [0.6861320734024048, 0.05720379576086998, 0.011636333540081978, 0.021660450845956802, 0.17488113045692444, 0.04848625510931015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396193027496338, 0.060380712151527405, 0.0, 0.0, 0.0, 0.0], [0.7851804494857788, 0.197513610124588, 0.017305942252278328, 0.0, 0.0, 0.0], [0.7660509347915649, 0.15444689989089966, 0.0318828746676445, 0.04761933162808418, 0.0, 0.0], [0.7035239934921265, 0.05171409249305725, 0.077609583735466, 0.15338999032974243, 0.013762434013187885, 0.0], [0.7121893763542175, 0.04994234815239906, 0.03772536665201187, 0.08649078756570816, 0.06541399657726288, 0.04823809117078781]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.974072277545929, 0.025927752256393433, 0.0, 0.0, 0.0, 0.0], [0.7925394177436829, 0.011715620756149292, 0.19574491679668427, 0.0, 0.0, 0.0], [0.5106772184371948, 0.007296795025467873, 0.03962019085884094, 0.4424057602882385, 0.0, 0.0], [0.5862478017807007, 0.012099707499146461, 0.024585247039794922, 0.06737809628248215, 0.3096891939640045, 0.0], [0.30196306109428406, 0.007724035996943712, 0.01151816826313734, 0.046947166323661804, 0.2214674949645996, 0.4103800356388092]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554162025452, 0.025544587522745132, 0.0, 0.0, 0.0, 0.0], [0.9769196510314941, 0.015048512257635593, 0.008031918667256832, 0.0, 0.0, 0.0], [0.9060618877410889, 0.025875478982925415, 0.025954892858862877, 0.04210778325796127, 0.0, 0.0], [0.9400084614753723, 0.005556638352572918, 0.0058282846584916115, 0.03175770491361618, 0.016849035397171974, 0.0], [0.9105738401412964, 0.0019752189982682467, 0.008646726608276367, 0.013360840268433094, 0.03543964400887489, 0.030003678053617477]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.979166567325592, 0.020833449438214302, 0.0, 0.0, 0.0, 0.0], [0.8444863557815552, 0.13507838547229767, 0.02043527364730835, 0.0, 0.0, 0.0], [0.7903082966804504, 0.14559200406074524, 0.037530045956373215, 0.026569729670882225, 0.0, 0.0], [0.7298941016197205, 0.05649605393409729, 0.032735347747802734, 0.10400402545928955, 0.07687048614025116, 0.0], [0.568419873714447, 0.04388812184333801, 0.02629331313073635, 0.08117102831602097, 0.24314767122268677, 0.03707994148135185]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499867558479309, 0.05001330375671387, 0.0, 0.0, 0.0, 0.0], [0.9336171746253967, 0.05848868191242218, 0.007894241251051426, 0.0, 0.0, 0.0], [0.7897834181785583, 0.11071828007698059, 0.05360167473554611, 0.045896559953689575, 0.0, 0.0], [0.8859302997589111, 0.057529717683792114, 0.013743197545409203, 0.0033877412788569927, 0.03940894827246666, 0.0], [0.9337607622146606, 0.026470690965652466, 0.004523388110101223, 0.0061904797330498695, 0.01413296815007925, 0.014921694993972778]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521789657555132e-09, 0.0, 0.0, 0.0, 0.0], [6.675865279248683e-06, 0.9999804496765137, 1.2841366697102785e-05, 0.0, 0.0, 0.0], [2.219356076693657e-08, 2.6684428000578464e-09, 0.9999971389770508, 2.8136157652625116e-06, 0.0, 0.0], [1.01456237189268e-06, 4.464133951387339e-08, 0.0003535677387844771, 0.9993677735328674, 0.0002776373294182122, 0.0], [9.436675885154955e-10, 1.3820520249063772e-11, 5.017758986092247e-10, 2.965133027998945e-09, 0.9999971389770508, 2.8643603400269058e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.00513677392154932, 0.0, 0.0, 0.0, 0.0], [0.9274218082427979, 0.018323849886655807, 0.05425437539815903, 0.0, 0.0, 0.0], [0.9678993821144104, 0.004143417812883854, 0.004314432851970196, 0.02364278770983219, 0.0, 0.0], [0.8999071717262268, 0.001467167865484953, 0.00029133641510270536, 0.002585020614787936, 0.09574926644563675, 0.0], [0.9386117458343506, 0.0002224828494945541, 0.000614664051681757, 0.001549560227431357, 0.030689271166920662, 0.028312314301729202]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042755335831316e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.261408210164518e-06, 0.0017206644406542182, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328855934294552e-08, 6.376191095114336e-07, 0.00011847116547869518, 0.0, 0.0], [0.9996154308319092, 3.473202525583474e-07, 3.892067468314053e-08, 4.468416250347218e-07, 0.00038369710091501474, 0.0], [0.9994840621948242, 1.655026871105747e-08, 2.8715449573724072e-08, 1.0638223102432676e-06, 0.00021266414842102677, 0.0003021186566911638]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135718345642, 0.048586405813694, 0.0, 0.0, 0.0, 0.0], [0.5749947428703308, 0.39028114080429077, 0.0347241647541523, 0.0, 0.0, 0.0], [0.7442325353622437, 0.17524097859859467, 0.0756472796201706, 0.004879264626652002, 0.0, 0.0], [0.523206889629364, 0.09429334104061127, 0.11381854116916656, 0.19979327917099, 0.06888788938522339, 0.0], [0.47472435235977173, 0.0563659593462944, 0.04530364274978638, 0.06967295706272125, 0.30980348587036133, 0.044129520654678345]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734644055366516, 0.126535564661026, 0.0, 0.0, 0.0, 0.0], [0.6097905039787292, 0.35417330265045166, 0.0360361710190773, 0.0, 0.0, 0.0], [0.45984211564064026, 0.38697874546051025, 0.09960118681192398, 0.05357794463634491, 0.0, 0.0], [0.5722204446792603, 0.2363625466823578, 0.08344576507806778, 0.06921909749507904, 0.03875207155942917, 0.0], [0.5143575072288513, 0.16723042726516724, 0.09019391983747482, 0.07654452323913574, 0.10578084737062454, 0.04589279368519783]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9812288880348206, 0.018771151080727577, 0.0, 0.0, 0.0, 0.0], [0.6142947673797607, 0.35039713978767395, 0.0353081040084362, 0.0, 0.0, 0.0], [0.5770688652992249, 0.328584223985672, 0.055082451552152634, 0.039264384657144547, 0.0, 0.0], [0.17188116908073425, 0.011042451485991478, 0.05457846447825432, 0.7326594591140747, 0.02983839437365532, 0.0], [0.37830105423927307, 0.01707008294761181, 0.021754104644060135, 0.44096946716308594, 0.06093835458159447, 0.08096688240766525]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112392425537, 0.007688738871365786, 0.0, 0.0, 0.0, 0.0], [0.9498788118362427, 0.016709720715880394, 0.033411379903554916, 0.0, 0.0, 0.0], [0.9961295127868652, 0.0008787269471213222, 0.0006868162308819592, 0.0023048450239002705, 0.0, 0.0], [0.9935757517814636, 0.0032634951639920473, 0.0009993825806304812, 0.00027932191733270884, 0.0018820537952706218, 0.0], [0.9907532930374146, 0.00021344218112062663, 0.0004595224454533309, 0.0007905578822828829, 0.004424717277288437, 0.003358344314619899]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647740125656128, 0.035225991159677505, 0.0, 0.0, 0.0, 0.0], [0.8194127678871155, 0.13654406368732452, 0.04404320567846298, 0.0, 0.0, 0.0], [0.7584251761436462, 0.0068789515644311905, 0.2065333127975464, 0.02816261723637581, 0.0, 0.0], [0.5298108458518982, 0.002678817370906472, 0.07858007401227951, 0.35983920097351074, 0.029091067612171173, 0.0], [0.7544422149658203, 0.00036782497772946954, 0.001971350284293294, 0.0032400612253695726, 0.19423353672027588, 0.04574500769376755]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749131202697754, 0.02508683130145073, 0.0, 0.0, 0.0, 0.0], [0.930647075176239, 0.05705663561820984, 0.01229619700461626, 0.0, 0.0, 0.0], [0.9305250644683838, 0.05277109891176224, 0.01111944206058979, 0.005584415048360825, 0.0, 0.0], [0.8863320350646973, 0.01292418036609888, 0.01772470958530903, 0.06150195747613907, 0.021517055109143257, 0.0], [0.7916849255561829, 0.015036109834909439, 0.03174788877367973, 0.0339219830930233, 0.0370798222720623, 0.09052923321723938]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608501195907593, 0.039149872958660126, 0.0, 0.0, 0.0, 0.0], [0.9121276140213013, 0.02257644012570381, 0.06529594957828522, 0.0, 0.0, 0.0], [0.9364106059074402, 0.015584413893520832, 0.02454490214586258, 0.02346000261604786, 0.0, 0.0], [0.9454618096351624, 0.006762299686670303, 0.02202620916068554, 0.009137802757322788, 0.016611848026514053, 0.0], [0.8346170783042908, 0.001881695119664073, 0.005609031766653061, 0.018873531371355057, 0.12449152022600174, 0.01452707126736641]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.003577270545065403, 0.0, 0.0, 0.0, 0.0], [0.9713929891586304, 0.024452976882457733, 0.004154053051024675, 0.0, 0.0, 0.0], [0.9735793471336365, 0.019003355875611305, 0.0036644041538238525, 0.0037529151886701584, 0.0, 0.0], [0.9586310982704163, 0.007116172928363085, 0.009218409657478333, 0.022725693881511688, 0.0023084846325218678, 0.0], [0.973607063293457, 0.008490558713674545, 0.003251240821555257, 0.003606433281674981, 0.004877456929534674, 0.006167223677039146]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97598797082901, 0.0240120030939579, 0.0, 0.0, 0.0, 0.0], [0.9460636377334595, 0.04211386665701866, 0.011822479777038097, 0.0, 0.0, 0.0], [0.8446812629699707, 0.04293115809559822, 0.05218207836151123, 0.06020554527640343, 0.0, 0.0], [0.9378372430801392, 0.033548641949892044, 0.008826455101370811, 0.002879226813092828, 0.01690846122801304, 0.0], [0.8124933838844299, 0.026967588812112808, 0.05999188870191574, 0.034457236528396606, 0.011011847294867039, 0.05507794767618179]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001205563545227, 0.0998794212937355, 0.0, 0.0, 0.0, 0.0], [0.6271934509277344, 0.07988736778497696, 0.29291924834251404, 0.0, 0.0, 0.0], [0.7624076008796692, 0.027344295755028725, 0.03867937996983528, 0.17156872153282166, 0.0, 0.0], [0.79959636926651, 0.01433627400547266, 0.014375641942024231, 0.025438494980335236, 0.1462530791759491, 0.0], [0.7851975560188293, 0.042040448635816574, 0.025253605097532272, 0.029083941131830215, 0.029306193813681602, 0.08911836892366409]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.004553201142698526, 0.0, 0.0, 0.0, 0.0], [0.9356000423431396, 0.044767580926418304, 0.019632400944828987, 0.0, 0.0, 0.0], [0.5605553984642029, 0.09861965477466583, 0.2998325824737549, 0.04099232703447342, 0.0, 0.0], [0.5893705487251282, 0.11000985652208328, 0.08033650368452072, 0.16754065454006195, 0.05274243280291557, 0.0], [0.2230590581893921, 0.0568079873919487, 0.05467963591217995, 0.24734023213386536, 0.3111236095428467, 0.10698942840099335]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301449656486511, 0.06985504180192947, 0.0, 0.0, 0.0, 0.0], [0.8936479687690735, 0.08535707741975784, 0.020994914695620537, 0.0, 0.0, 0.0], [0.8404536247253418, 0.10619223117828369, 0.02363678440451622, 0.029717370867729187, 0.0, 0.0], [0.8927385807037354, 0.02478473260998726, 0.00831903237849474, 0.05165451392531395, 0.022503186017274857, 0.0], [0.8646611571311951, 0.009503157809376717, 0.002432981040328741, 0.04796745628118515, 0.04273199662566185, 0.03270323947072029]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037422835826874, 0.0, 0.0, 0.0, 0.0], [0.9702037572860718, 0.016807053238153458, 0.012989138253033161, 0.0, 0.0, 0.0], [0.9524770379066467, 0.016064444556832314, 0.013456214219331741, 0.0180023442953825, 0.0, 0.0], [0.9332928657531738, 0.018971983343362808, 0.020146803930401802, 0.01702374592423439, 0.010564545169472694, 0.0], [0.9113592505455017, 0.012528626248240471, 0.022096220403909683, 0.01751856319606304, 0.018517863005399704, 0.017979340627789497]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681769013404846, 0.03182307630777359, 0.0, 0.0, 0.0, 0.0], [0.9096417427062988, 0.07916691154241562, 0.011191281490027905, 0.0, 0.0, 0.0], [0.8379933834075928, 0.13078251481056213, 0.012141004204750061, 0.01908305659890175, 0.0, 0.0], [0.9116524457931519, 0.05451960116624832, 0.00949935894459486, 0.0074658701196312904, 0.01686273142695427, 0.0], [0.8510289192199707, 0.07338210195302963, 0.008022499270737171, 0.009083169512450695, 0.042610082775354385, 0.015873299911618233]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.02009764313697815, 0.0, 0.0, 0.0, 0.0], [0.9558742642402649, 0.029063284397125244, 0.015062497928738594, 0.0, 0.0, 0.0], [0.7943135499954224, 0.06074092164635658, 0.06907661259174347, 0.07586897909641266, 0.0, 0.0], [0.5494326949119568, 0.031547144055366516, 0.05482024699449539, 0.057880766689777374, 0.3063191771507263, 0.0], [0.6453983187675476, 0.01077093742787838, 0.017528066411614418, 0.021579790860414505, 0.2495826631784439, 0.05514022707939148]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506808519363403, 0.04931914433836937, 0.0, 0.0, 0.0, 0.0], [0.8553212285041809, 0.09256298840045929, 0.052115827798843384, 0.0, 0.0, 0.0], [0.8508526086807251, 0.04734603688120842, 0.044177182018756866, 0.05762412026524544, 0.0, 0.0], [0.7697132229804993, 0.027885833755135536, 0.031017232686281204, 0.06842483580112457, 0.10295882076025009, 0.0], [0.7931901812553406, 0.04052194580435753, 0.029241986572742462, 0.04478125646710396, 0.048947159200906754, 0.04331747069954872]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.022968942299485207, 0.0, 0.0, 0.0, 0.0], [0.9429817199707031, 0.017321482300758362, 0.0396968275308609, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583560585975647, 0.013035798445343971, 0.06394600868225098, 0.0, 0.0], [0.9222431778907776, 0.003644014708697796, 0.003740262007340789, 0.010410335846245289, 0.059962138533592224, 0.0], [0.9198878407478333, 0.0030822637490928173, 0.003482747357338667, 0.004206812474876642, 0.02125430852174759, 0.04808592423796654]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977458119392395, 0.02254188433289528, 0.0, 0.0, 0.0, 0.0], [0.8929324150085449, 0.07475482672452927, 0.0323127843439579, 0.0, 0.0, 0.0], [0.8423509001731873, 0.05980299785733223, 0.03740081936120987, 0.06044524163007736, 0.0, 0.0], [0.767462968826294, 0.035363469272851944, 0.04215509817004204, 0.06658641993999481, 0.08843207359313965, 0.0], [0.6182615160942078, 0.01611061953008175, 0.020167585462331772, 0.03868892788887024, 0.23146988451480865, 0.07530143857002258]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.963485836982727, 0.03651418536901474, 0.0, 0.0, 0.0, 0.0], [0.43639373779296875, 0.5226370692253113, 0.04096921905875206, 0.0, 0.0, 0.0], [0.36086130142211914, 0.3512967824935913, 0.26551079750061035, 0.02233118936419487, 0.0, 0.0], [0.3942915201187134, 0.021704623475670815, 0.07794354856014252, 0.37168946862220764, 0.1343708485364914, 0.0], [0.6310727000236511, 0.01698393188416958, 0.025941966101527214, 0.08615903556346893, 0.21831940114498138, 0.021522900089621544]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750265257433057, 0.0, 0.0, 0.0, 0.0], [0.9944872260093689, 0.00048264089855365455, 0.00503021664917469, 0.0, 0.0, 0.0], [0.9981209635734558, 2.7051630240748636e-05, 0.00011307386739645153, 0.0017389451386407018, 0.0, 0.0], [0.9982239603996277, 6.836584361735731e-05, 0.00010199925600318238, 6.0283757193246856e-05, 0.0015453121159225702, 0.0], [0.9982888102531433, 1.0552238336458686e-06, 3.2780964829726145e-05, 0.00013039026816841215, 0.0006605856469832361, 0.0008863684488460422]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328921765089035, 0.0, 0.0, 0.0, 0.0], [0.9727687239646912, 0.0018561383476480842, 0.02537512220442295, 0.0, 0.0, 0.0], [0.9724301099777222, 0.0019586123526096344, 0.011192457750439644, 0.01441886555403471, 0.0, 0.0], [0.9782042503356934, 0.0009589158580638468, 0.0018706476548686624, 0.00632655480876565, 0.012639649212360382, 0.0], [0.9592596888542175, 0.0024555146228522062, 0.0016124180983752012, 0.005019665230065584, 0.006687097251415253, 0.024965699762105942]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9628998041152954, 0.03710014000535011, 0.0, 0.0, 0.0, 0.0], [0.3680197596549988, 0.6152254343032837, 0.016754796728491783, 0.0, 0.0, 0.0], [0.31735214591026306, 0.6140002608299255, 0.05375148355960846, 0.01489606499671936, 0.0, 0.0], [0.48987358808517456, 0.21071450412273407, 0.04693010076880455, 0.2070041298866272, 0.045477770268917084, 0.0], [0.48774272203445435, 0.17695209383964539, 0.06915208697319031, 0.09849290549755096, 0.12091457843780518, 0.046745575964450836]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794418811798096, 0.020558098331093788, 0.0, 0.0, 0.0, 0.0], [0.6677899956703186, 0.31032389402389526, 0.0218860674649477, 0.0, 0.0, 0.0], [0.7118753790855408, 0.11108547449111938, 0.14187391102313995, 0.035165149718523026, 0.0, 0.0], [0.45014700293540955, 0.040360286831855774, 0.04045799747109413, 0.38856977224349976, 0.08046494424343109, 0.0], [0.49346214532852173, 0.013696905225515366, 0.008126788772642612, 0.13074517250061035, 0.308614045381546, 0.045354992151260376]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846053123474121, 0.015394614078104496, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713806349784136, 0.01161233615130186, 0.0, 0.0, 0.0], [0.9326632022857666, 0.01957837864756584, 0.024103516712784767, 0.02365492470562458, 0.0, 0.0], [0.9422017931938171, 0.000953896960709244, 0.0010897991014644504, 0.0031933619175106287, 0.052561093121767044, 0.0], [0.9352930784225464, 0.0010279371635988355, 0.004444435704499483, 0.0016371363308280706, 0.010590983554720879, 0.04700643941760063]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216777635738254, 0.0, 0.0, 0.0, 0.0], [0.9893350005149841, 0.0011178924469277263, 0.009547130204737186, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997050124686211e-05, 0.00013218850654084235, 0.0017900384264066815, 0.0, 0.0], [0.9986976385116577, 4.104415711481124e-05, 3.868347448587883e-06, 2.367625893384684e-05, 0.0012337109073996544, 0.0], [0.9971562623977661, 1.8522179743740708e-05, 1.8826630139301415e-06, 2.7900254281121306e-05, 0.0006533460109494627, 0.0021419869735836983]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768234491348267, 0.02317662164568901, 0.0, 0.0, 0.0, 0.0], [0.9194676876068115, 0.05088196322321892, 0.02965034544467926, 0.0, 0.0, 0.0], [0.8474553823471069, 0.061001770198345184, 0.04372374340891838, 0.04781917855143547, 0.0, 0.0], [0.8011624217033386, 0.041867032647132874, 0.043758004903793335, 0.04189474135637283, 0.07131776213645935, 0.0], [0.8031865954399109, 0.024504970759153366, 0.01732354238629341, 0.047443993389606476, 0.06109940633177757, 0.04644133895635605]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829428195953369, 0.017057137563824654, 0.0, 0.0, 0.0, 0.0], [0.8863733410835266, 0.09492677450180054, 0.018699826672673225, 0.0, 0.0, 0.0], [0.9231084585189819, 0.03696348890662193, 0.032198354601860046, 0.007729677949100733, 0.0, 0.0], [0.9068527221679688, 0.016046587377786636, 0.014310481958091259, 0.04543795436620712, 0.017352258786559105, 0.0], [0.6555962562561035, 0.05091032758355141, 0.02838490903377533, 0.1256556659936905, 0.10546886920928955, 0.03398396074771881]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502319693565369, 0.04976801201701164, 0.0, 0.0, 0.0, 0.0], [0.8829866647720337, 0.10009617358446121, 0.016917172819375992, 0.0, 0.0, 0.0], [0.8057460188865662, 0.1446353793144226, 0.030189279466867447, 0.01942942664027214, 0.0, 0.0], [0.8706232905387878, 0.03244056552648544, 0.02695164829492569, 0.04410288855433464, 0.025881504639983177, 0.0], [0.6883639693260193, 0.009681489318609238, 0.01644940674304962, 0.0987112894654274, 0.08971197158098221, 0.09708179533481598]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683124542236, 0.020731676369905472, 0.0, 0.0, 0.0, 0.0], [0.9523283243179321, 0.02593386545777321, 0.02173778973519802, 0.0, 0.0, 0.0], [0.9144353866577148, 0.017671233043074608, 0.022358505055308342, 0.04553479701280594, 0.0, 0.0], [0.9448293447494507, 0.006467582657933235, 0.006386060733348131, 0.032630886882543564, 0.00968615710735321, 0.0], [0.9347907304763794, 0.007862498052418232, 0.007788183633238077, 0.02143281139433384, 0.008491137996315956, 0.019634803757071495]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629686579108238, 0.0, 0.0, 0.0, 0.0], [0.9631112813949585, 0.009229938499629498, 0.02765873447060585, 0.0, 0.0, 0.0], [0.9706627130508423, 0.004149396438151598, 0.006813123356550932, 0.01837467961013317, 0.0, 0.0], [0.987951934337616, 0.002165883546695113, 0.00034901159233413637, 0.0015838207909837365, 0.00794942770153284, 0.0], [0.9457950592041016, 0.014583553187549114, 0.00036529707722365856, 0.000956955598667264, 0.013621604070067406, 0.024677496403455734]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878059029579163, 0.012194046750664711, 0.0, 0.0, 0.0, 0.0], [0.8710371851921082, 0.09448141604661942, 0.034481462091207504, 0.0, 0.0, 0.0], [0.6309770345687866, 0.1109040230512619, 0.19230300188064575, 0.06581595540046692, 0.0, 0.0], [0.5360509157180786, 0.04618922993540764, 0.13605229556560516, 0.2645542621612549, 0.01715322583913803, 0.0], [0.8287516236305237, 0.02373279444873333, 0.020080383867025375, 0.07245291024446487, 0.03043138049542904, 0.02455095946788788]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995686173439026, 0.10043134540319443, 0.0, 0.0, 0.0, 0.0], [0.27034321427345276, 0.6504333019256592, 0.07922343164682388, 0.0, 0.0, 0.0], [0.20541763305664062, 0.5892506837844849, 0.18085815012454987, 0.024473493918776512, 0.0, 0.0], [0.5573878288269043, 0.17741289734840393, 0.08806769549846649, 0.09881812334060669, 0.07831349968910217, 0.0], [0.5922902226448059, 0.08700639754533768, 0.05643284320831299, 0.056858986616134644, 0.12181543558835983, 0.08559618890285492]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9316377639770508, 0.0683622807264328, 0.0, 0.0, 0.0, 0.0], [0.9572945833206177, 0.02624361403286457, 0.01646178774535656, 0.0, 0.0, 0.0], [0.9880543351173401, 0.004273326601833105, 0.0029545894358307123, 0.00471765361726284, 0.0, 0.0], [0.99403977394104, 0.0009413412190042436, 0.0004739806754514575, 0.00011646930943243206, 0.004428436513990164, 0.0], [0.9806035161018372, 2.546903124311939e-05, 0.00016239489195868373, 0.00014764198567718267, 0.0013442462077364326, 0.017716852948069572]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821890361607075, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.01318411435931921, 0.011163423769176006, 0.0, 0.0, 0.0], [0.9418966770172119, 0.004721746779978275, 0.0023818088229745626, 0.050999779254198074, 0.0, 0.0], [0.9905040860176086, 0.0022848115768283606, 6.198474875418469e-05, 0.0005984468152746558, 0.006550653837621212, 0.0], [0.9697661399841309, 0.0008878829539753497, 0.00023466694983653724, 0.0017040737438946962, 0.004128328990191221, 0.023278919979929924]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716230630874634, 0.028376862406730652, 0.0, 0.0, 0.0, 0.0], [0.9223619699478149, 0.02890726923942566, 0.04873079061508179, 0.0, 0.0, 0.0], [0.8426315188407898, 0.02387217804789543, 0.047481440007686615, 0.08601480722427368, 0.0, 0.0], [0.8521119952201843, 0.02074424922466278, 0.04494624584913254, 0.05765005946159363, 0.024547399953007698, 0.0], [0.8800725936889648, 0.022448506206274033, 0.01823572628200054, 0.019254814833402634, 0.01585424318909645, 0.04413411393761635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412722587585449, 0.0587276890873909, 0.0, 0.0, 0.0, 0.0], [0.9163140058517456, 0.057591959834098816, 0.02609400823712349, 0.0, 0.0, 0.0], [0.839242160320282, 0.05769055336713791, 0.013829052448272705, 0.0892382562160492, 0.0, 0.0], [0.8987162709236145, 0.013477863743901253, 0.0003456453559920192, 0.003298757830634713, 0.08416145294904709, 0.0], [0.8701689839363098, 0.002700872253626585, 0.0014350019628182054, 0.00566619448363781, 0.088743194937706, 0.03128569945693016]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656724333763123, 0.034327585250139236, 0.0, 0.0, 0.0, 0.0], [0.9178614020347595, 0.062258072197437286, 0.01988055184483528, 0.0, 0.0, 0.0], [0.8233145475387573, 0.06282392889261246, 0.036704398691654205, 0.07715703547000885, 0.0, 0.0], [0.850174605846405, 0.03816927224397659, 0.0319649763405323, 0.05160144716501236, 0.028089681640267372, 0.0], [0.6572401523590088, 0.058774061501026154, 0.04336016625165939, 0.0901322066783905, 0.08146588504314423, 0.06902756541967392]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162063002586365, 0.08379373699426651, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.04099282994866371, 0.013829832896590233, 0.0, 0.0, 0.0], [0.8928353786468506, 0.05368685722351074, 0.01759699359536171, 0.035880763083696365, 0.0, 0.0], [0.8337051868438721, 0.0479961521923542, 0.03351322561502457, 0.04680856689810753, 0.03797685354948044, 0.0], [0.8167192339897156, 0.06337135285139084, 0.013286283239722252, 0.020469767972826958, 0.025292349979281425, 0.06086102873086929]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525134563446045, 0.047486573457717896, 0.0, 0.0, 0.0, 0.0], [0.30198583006858826, 0.6520950794219971, 0.04591907188296318, 0.0, 0.0, 0.0], [0.28558245301246643, 0.5569531917572021, 0.1444738656282425, 0.012990481220185757, 0.0, 0.0], [0.8438042998313904, 0.032251279801130295, 0.03954283893108368, 0.06848151236772537, 0.01592002436518669, 0.0], [0.6664933562278748, 0.060959313064813614, 0.040643513202667236, 0.06804481148719788, 0.09186359494924545, 0.07199548929929733]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.03173445910215378, 0.0, 0.0, 0.0, 0.0], [0.7385216355323792, 0.22856871783733368, 0.032909613102674484, 0.0, 0.0, 0.0], [0.5946674942970276, 0.23033156991004944, 0.14867623150348663, 0.026324672624468803, 0.0, 0.0], [0.6339268684387207, 0.05813005566596985, 0.09654270857572556, 0.14291921257972717, 0.06848114728927612, 0.0], [0.40375715494155884, 0.08945377916097641, 0.07635065168142319, 0.255871057510376, 0.1433035284280777, 0.03126388415694237]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020661659538746, 0.0, 0.0, 0.0, 0.0], [0.8631383180618286, 0.11056677997112274, 0.026294905692338943, 0.0, 0.0, 0.0], [0.9488076567649841, 0.028615081682801247, 0.006535593420267105, 0.016041621565818787, 0.0, 0.0], [0.9672168493270874, 0.006605003960430622, 0.00045171717647463083, 0.004844441078603268, 0.020881807431578636, 0.0], [0.9354621171951294, 0.020478062331676483, 0.0011700298637151718, 0.00705695990473032, 0.01631814055144787, 0.0195146594196558]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846673011779785, 0.015332723967730999, 0.0, 0.0, 0.0, 0.0], [0.9052741527557373, 0.08373655378818512, 0.010989287868142128, 0.0, 0.0, 0.0], [0.8145945072174072, 0.042837370187044144, 0.10568250715732574, 0.036885637789964676, 0.0, 0.0], [0.23519638180732727, 0.01201847568154335, 0.05280109494924545, 0.6516201496124268, 0.048363905400037766, 0.0], [0.31818586587905884, 0.018632402643561363, 0.03948185592889786, 0.3755546510219574, 0.2078723907470703, 0.0402727872133255]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.018826717510819435, 0.0, 0.0, 0.0, 0.0], [0.8618940711021423, 0.06479166448116302, 0.07331428676843643, 0.0, 0.0, 0.0], [0.766453742980957, 0.07330445200204849, 0.1035352423787117, 0.056706640869379044, 0.0, 0.0], [0.8128494620323181, 0.03215496614575386, 0.05900568142533302, 0.05416534096002579, 0.041824568063020706, 0.0], [0.8687859773635864, 0.026987846940755844, 0.02046993002295494, 0.0162973552942276, 0.032183725386857986, 0.035275135189294815]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926457941532135, 0.073542021214962, 0.0, 0.0, 0.0, 0.0], [0.8403539657592773, 0.06373762339353561, 0.09590839594602585, 0.0, 0.0, 0.0], [0.7330992221832275, 0.06451141089200974, 0.10380076617002487, 0.09858863800764084, 0.0, 0.0], [0.9143611788749695, 0.008257799781858921, 0.007320398464798927, 0.017966315150260925, 0.05209440737962723, 0.0], [0.8971914052963257, 0.008555522188544273, 0.007019452750682831, 0.014860584400594234, 0.03399764746427536, 0.03837532922625542]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180347919464111, 0.08196521550416946, 0.0, 0.0, 0.0, 0.0], [0.8328663110733032, 0.12199043482542038, 0.04514329135417938, 0.0, 0.0, 0.0], [0.7994154095649719, 0.08744152635335922, 0.03605787828564644, 0.0770852342247963, 0.0, 0.0], [0.8809852004051208, 0.020749634131789207, 0.020554551854729652, 0.017120789736509323, 0.06058980152010918, 0.0], [0.7453038692474365, 0.04433402419090271, 0.022549238055944443, 0.03315272554755211, 0.03357045724987984, 0.12108980864286423]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293893076479435, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414124794304371, 0.005408101249486208, 0.0, 0.0, 0.0], [0.9630486369132996, 0.015290760435163975, 0.010345693677663803, 0.011314942501485348, 0.0, 0.0], [0.9213568568229675, 0.014132469892501831, 0.017639169469475746, 0.01656768098473549, 0.03030369058251381, 0.0], [0.9373326301574707, 0.00906432792544365, 0.007548368535935879, 0.006576458923518658, 0.011827644892036915, 0.027650468051433563]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951003193855286, 0.004899636376649141, 0.0, 0.0, 0.0, 0.0], [0.9476009607315063, 0.04140780493617058, 0.01099125761538744, 0.0, 0.0, 0.0], [0.9142176508903503, 0.02352375164628029, 0.03914502263069153, 0.02311357483267784, 0.0, 0.0], [0.9534735679626465, 0.008932963944971561, 0.015272833406925201, 0.007908266969025135, 0.014412309974431992, 0.0], [0.9427104592323303, 0.00823306292295456, 0.004650986287742853, 0.004178097005933523, 0.00546351308003068, 0.034764066338539124]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543375372886658, 0.04566241055727005, 0.0, 0.0, 0.0, 0.0], [0.9696040749549866, 0.019547631964087486, 0.010848279111087322, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425404042005539, 0.008068868890404701, 0.008460727520287037, 0.0, 0.0], [0.9726192951202393, 0.0026976584922522306, 0.0004483137163333595, 0.0013814778067171574, 0.022853175178170204, 0.0], [0.9675467610359192, 0.00961342453956604, 0.0032030234578996897, 0.004248819779604673, 0.007442229427397251, 0.007945857010781765]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299127712845802, 0.0, 0.0, 0.0, 0.0], [0.9382631182670593, 0.042042478919029236, 0.019694417715072632, 0.0, 0.0, 0.0], [0.8351993560791016, 0.03487858176231384, 0.051344819366931915, 0.0785772055387497, 0.0, 0.0], [0.9042678475379944, 0.010541572235524654, 0.016426734626293182, 0.025921881198883057, 0.04284193366765976, 0.0], [0.8913140296936035, 0.00891267228871584, 0.005010711494833231, 0.008175608702003956, 0.013514735735952854, 0.07307220995426178]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693908452987671, 0.13060913980007172, 0.0, 0.0, 0.0, 0.0], [0.3507988452911377, 0.6063520908355713, 0.042849019169807434, 0.0, 0.0, 0.0], [0.35475632548332214, 0.3502025008201599, 0.24722392857074738, 0.04781724512577057, 0.0, 0.0], [0.3537069857120514, 0.03527746722102165, 0.09567110240459442, 0.4497954249382019, 0.06554897129535675, 0.0], [0.4132595956325531, 0.0905555933713913, 0.05286560207605362, 0.1746789813041687, 0.1738486886024475, 0.09479160606861115]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.0370243638753891, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.019658580422401428, 0.004698721691966057, 0.0, 0.0, 0.0], [0.9775736927986145, 0.013286304660141468, 0.002559035550802946, 0.006581062916666269, 0.0, 0.0], [0.9870141744613647, 0.007388271391391754, 0.0009579194593243301, 0.001831823494285345, 0.0028077620081603527, 0.0], [0.9409247040748596, 0.016633691266179085, 0.002297905972227454, 0.005890644155442715, 0.005512925796210766, 0.028740182518959045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9628270864486694, 0.037172894924879074, 0.0, 0.0, 0.0, 0.0], [0.9582236409187317, 0.024641895666718483, 0.017134448513388634, 0.0, 0.0, 0.0], [0.9351298809051514, 0.015331615693867207, 0.014811009168624878, 0.03472743555903435, 0.0, 0.0], [0.9225173592567444, 0.010528774000704288, 0.01101017277687788, 0.019440023228526115, 0.03650379180908203, 0.0], [0.8420169353485107, 0.043571919202804565, 0.007488253526389599, 0.014961487613618374, 0.023852787911891937, 0.06810857355594635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.007361318916082382, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.0033469724003225565, 0.0009136894368566573, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974780112504959, 0.001524552470073104, 0.009510664269328117, 0.0, 0.0], [0.9933527708053589, 0.001020328258164227, 0.00034337403485551476, 0.0010291122598573565, 0.004254385828971863, 0.0], [0.9749016761779785, 0.0004348014772403985, 0.00043065653881058097, 0.0012364378198981285, 0.0015347728040069342, 0.021461615338921547]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252472013235092, 0.0, 0.0, 0.0, 0.0], [0.9790638089179993, 0.016509108245372772, 0.004427104722708464, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432388022542, 0.008943166583776474, 0.00948089174926281, 0.0, 0.0], [0.939594030380249, 0.02151094563305378, 0.01027856208384037, 0.004555220250040293, 0.02406122162938118, 0.0], [0.9205074906349182, 0.016153674572706223, 0.010818609967827797, 0.016644401475787163, 0.014566432684659958, 0.02130942977964878]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898502826690674, 0.01014974806457758, 0.0, 0.0, 0.0, 0.0], [0.9820908904075623, 0.006907513365149498, 0.011001549661159515, 0.0, 0.0, 0.0], [0.9684996008872986, 0.008987593464553356, 0.015342592261731625, 0.00717011047527194, 0.0, 0.0], [0.9274123311042786, 0.00948523823171854, 0.022066060453653336, 0.03222879767417908, 0.008807609789073467, 0.0], [0.9006660580635071, 0.021623695269227028, 0.013808242976665497, 0.009843834675848484, 0.008521351031959057, 0.04553688317537308]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555598832666874, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.0024602413177490234, 0.002285487949848175, 0.0, 0.0, 0.0], [0.9862446188926697, 0.0015168129466474056, 0.004072288051247597, 0.008166329935193062, 0.0, 0.0], [0.9889963865280151, 0.0012260442599654198, 0.0007996379281394184, 0.0006774240755476058, 0.008300574496388435, 0.0], [0.9865202903747559, 0.0003942710463888943, 0.0009571776608936489, 0.0004954351461492479, 0.0009604924125596881, 0.010672245174646378]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.0178704671561718, 0.0, 0.0, 0.0, 0.0], [0.7489438056945801, 0.22002701461315155, 0.031029196456074715, 0.0, 0.0, 0.0], [0.28547796607017517, 0.21125660836696625, 0.4787161350250244, 0.024549288675189018, 0.0, 0.0], [0.8056640625, 0.026974599808454514, 0.043028153479099274, 0.0699373409152031, 0.0543958880007267, 0.0], [0.330722838640213, 0.022326549515128136, 0.01662709005177021, 0.08019424229860306, 0.4157470166683197, 0.13438232243061066]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697746634483337, 0.03022538498044014, 0.0, 0.0, 0.0, 0.0], [0.9800565838813782, 0.015018886886537075, 0.004924529232084751, 0.0, 0.0, 0.0], [0.9237861037254333, 0.052764758467674255, 0.006302410736680031, 0.017146745696663857, 0.0, 0.0], [0.9451841711997986, 0.036180540919303894, 0.00198921631090343, 0.003958734683692455, 0.012687314301729202, 0.0], [0.9633325934410095, 0.01866302639245987, 0.003041850635781884, 0.0070709227584302425, 0.005009420681744814, 0.002882068045437336]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.987324595451355, 0.012675454840064049, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.005541946738958359, 0.00400111498311162, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653464537113905, 0.003725278889760375, 0.010124054737389088, 0.0, 0.0], [0.9744364619255066, 0.004632262047380209, 0.002379997167736292, 0.006518092937767506, 0.01203305646777153, 0.0], [0.962449848651886, 0.0033743546810001135, 0.0013198552187532187, 0.0017274972051382065, 0.0029446722473949194, 0.028183748945593834]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.019232535734772682, 0.0, 0.0, 0.0, 0.0], [0.9664247035980225, 0.015413912013173103, 0.018161388114094734, 0.0, 0.0, 0.0], [0.9632683396339417, 0.0045381225645542145, 0.002925397362560034, 0.02926814928650856, 0.0, 0.0], [0.9562349319458008, 0.0012223637895658612, 0.0005304092774167657, 0.008671483024954796, 0.033340904861688614, 0.0], [0.9657101035118103, 0.0009808284230530262, 0.001668625045567751, 0.002634829841554165, 0.005866350140422583, 0.023139292374253273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639716148376465, 0.036028459668159485, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.033733103424310684, 0.009986814111471176, 0.0, 0.0, 0.0], [0.8539999127388, 0.08073018491268158, 0.03334438428282738, 0.031925465911626816, 0.0, 0.0], [0.9547491073608398, 0.009605025872588158, 0.004146162886172533, 0.002013320801779628, 0.02948642708361149, 0.0], [0.9331134557723999, 0.028699735179543495, 0.005477481987327337, 0.006368075963109732, 0.012613091617822647, 0.013728121295571327]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070063263177872, 0.0, 0.0, 0.0, 0.0], [0.929839015007019, 0.06189544126391411, 0.008265510201454163, 0.0, 0.0, 0.0], [0.8471821546554565, 0.09035056084394455, 0.017636140808463097, 0.04483111575245857, 0.0, 0.0], [0.8857702016830444, 0.03918176516890526, 0.007867725566029549, 0.0227658748626709, 0.04441441595554352, 0.0], [0.856328010559082, 0.10088998824357986, 0.006531470455229282, 0.008485930040478706, 0.007368440739810467, 0.02039625681936741]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353263735771179, 0.16467364132404327, 0.0, 0.0, 0.0, 0.0], [0.6160863041877747, 0.3137645721435547, 0.07014910131692886, 0.0, 0.0, 0.0], [0.3431629240512848, 0.2758498191833496, 0.11966046690940857, 0.26132678985595703, 0.0, 0.0], [0.5908164381980896, 0.050290923565626144, 0.04166606813669205, 0.21994958817958832, 0.09727700054645538, 0.0], [0.8481416702270508, 0.06318087130784988, 0.014733650721609592, 0.05526726692914963, 0.009014973416924477, 0.00966164655983448]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627320766448975, 0.037267982959747314, 0.0, 0.0, 0.0, 0.0], [0.7757522463798523, 0.17996273934841156, 0.04428492859005928, 0.0, 0.0, 0.0], [0.6317059397697449, 0.24380749464035034, 0.10925635695457458, 0.015230262652039528, 0.0, 0.0], [0.9539911150932312, 0.018182285130023956, 0.011601786129176617, 0.012299060821533203, 0.0039257691241800785, 0.0], [0.4035700857639313, 0.1423756629228592, 0.05661202594637871, 0.19757331907749176, 0.09299200773239136, 0.1068769320845604]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802619218826294, 0.0197380930185318, 0.0, 0.0, 0.0, 0.0], [0.98739093542099, 0.0078004347160458565, 0.004808679223060608, 0.0, 0.0, 0.0], [0.9283918738365173, 0.008301235735416412, 0.01330567803233862, 0.05000121891498566, 0.0, 0.0], [0.8981053829193115, 0.015591333620250225, 0.01017761416733265, 0.03998704254627228, 0.036138709634542465, 0.0], [0.975350022315979, 0.0003543298807926476, 0.0005866029532626271, 0.0011877480428665876, 0.0010750886285677552, 0.021446382626891136]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295329451560974, 0.0704670399427414, 0.0, 0.0, 0.0, 0.0], [0.9361504316329956, 0.04116694629192352, 0.0226825550198555, 0.0, 0.0, 0.0], [0.8486821055412292, 0.05802805721759796, 0.024856194853782654, 0.06843377649784088, 0.0, 0.0], [0.8661178946495056, 0.02232474647462368, 0.01036915834993124, 0.02600196562707424, 0.07518626749515533, 0.0], [0.8074419498443604, 0.04438265413045883, 0.018497120589017868, 0.033577870577573776, 0.01856123097240925, 0.07753915339708328]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.03194649890065193, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.025684893131256104, 0.004946068394929171, 0.0, 0.0, 0.0], [0.9620568156242371, 0.02255241759121418, 0.005471334792673588, 0.009919446893036366, 0.0, 0.0], [0.9727528095245361, 0.010137109085917473, 0.000757327419705689, 0.0028828924987465143, 0.013469826430082321, 0.0], [0.9624636769294739, 0.0031108984258025885, 0.0010007594246417284, 0.0019475906156003475, 0.008266216143965721, 0.023210890591144562]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542505502700806, 0.14574941992759705, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116342179477215, 0.01328686811029911, 0.0, 0.0, 0.0], [0.9257621169090271, 0.03257268667221069, 0.014612147584557533, 0.027053041383624077, 0.0, 0.0], [0.7923436760902405, 0.02730504609644413, 0.01880672201514244, 0.1385406255722046, 0.02300397865474224, 0.0], [0.6152049899101257, 0.026655390858650208, 0.02935311757028103, 0.05590883269906044, 0.1161130741238594, 0.1567646563053131]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534552469849586, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509459275752306, 0.004245327785611153, 0.0, 0.0, 0.0], [0.9584206342697144, 0.010963553562760353, 0.010456085205078125, 0.02015974558889866, 0.0, 0.0], [0.9604811668395996, 0.007182624191045761, 0.0030723451636731625, 0.00689891679212451, 0.022365113720297813, 0.0], [0.966888964176178, 0.003281292272731662, 0.005500538740307093, 0.004234083462506533, 0.005038043484091759, 0.015057160519063473]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018052086234093, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.01643000729382038, 0.005433553829789162, 0.0, 0.0, 0.0], [0.8618696928024292, 0.03609352558851242, 0.0755554735660553, 0.02648121677339077, 0.0, 0.0], [0.544983983039856, 0.015411156229674816, 0.023516563698649406, 0.2574361264705658, 0.1586521416902542, 0.0], [0.9571872353553772, 0.003080392023548484, 0.0014446907443925738, 0.006861583329737186, 0.014818795025348663, 0.01660730689764023]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156561374664307, 0.3843438923358917, 0.0, 0.0, 0.0, 0.0], [0.367606520652771, 0.42816346883773804, 0.20423001050949097, 0.0, 0.0, 0.0], [0.1647154688835144, 0.4136791527271271, 0.25092384219169617, 0.17068159580230713, 0.0, 0.0], [0.41844630241394043, 0.1524759829044342, 0.10305386781692505, 0.11071502417325974, 0.21530881524085999, 0.0], [0.19686943292617798, 0.2014620453119278, 0.12827253341674805, 0.09203249216079712, 0.09167563915252686, 0.289687842130661]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027368426322937, 0.09726322442293167, 0.0, 0.0, 0.0, 0.0], [0.9736634492874146, 0.014004282653331757, 0.01233229786157608, 0.0, 0.0, 0.0], [0.8504459261894226, 0.05690564960241318, 0.032060883939266205, 0.06058758124709129, 0.0, 0.0], [0.7661212086677551, 0.03530392050743103, 0.03433046117424965, 0.09675193578004837, 0.06749238818883896, 0.0], [0.8650376796722412, 0.020085245370864868, 0.011498053558170795, 0.018558334559202194, 0.018430272117257118, 0.06639042496681213]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469167277216911, 0.0, 0.0, 0.0, 0.0], [0.9816324710845947, 0.014176053926348686, 0.004191512707620859, 0.0, 0.0, 0.0], [0.9275255799293518, 0.04737215116620064, 0.011528274044394493, 0.013573979958891869, 0.0, 0.0], [0.9293117523193359, 0.025833306834101677, 0.0072271269746124744, 0.014300605282187462, 0.02332727052271366, 0.0], [0.8895059823989868, 0.046896327286958694, 0.00471712090075016, 0.0062866006046533585, 0.006090158596634865, 0.04650387167930603]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938026428222656, 0.10619727522134781, 0.0, 0.0, 0.0, 0.0], [0.8221704959869385, 0.06304488331079483, 0.1147845983505249, 0.0, 0.0, 0.0], [0.5047373175621033, 0.1537574827671051, 0.2277042418718338, 0.11380095779895782, 0.0, 0.0], [0.4082077145576477, 0.09066354483366013, 0.11696869134902954, 0.2455315738916397, 0.1386285126209259, 0.0], [0.7291042804718018, 0.06638873368501663, 0.023112762719392776, 0.031102973967790604, 0.057143229991197586, 0.09314799308776855]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247532486915588, 0.07524671405553818, 0.0, 0.0, 0.0, 0.0], [0.8957377076148987, 0.06989550590515137, 0.03436676412820816, 0.0, 0.0, 0.0], [0.7924939393997192, 0.0960114374756813, 0.05509119853377342, 0.0564035065472126, 0.0, 0.0], [0.7891507148742676, 0.0788029208779335, 0.038401566445827484, 0.05396970361471176, 0.039674971252679825, 0.0], [0.7807860374450684, 0.07993526756763458, 0.04253169521689415, 0.032342053949832916, 0.017816925421357155, 0.046588074415922165]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480887651443481, 0.051911257207393646, 0.0, 0.0, 0.0, 0.0], [0.8636947274208069, 0.047562047839164734, 0.08874327689409256, 0.0, 0.0, 0.0], [0.9341371059417725, 0.022224098443984985, 0.02262449823319912, 0.021014342084527016, 0.0, 0.0], [0.9588144421577454, 0.008020879700779915, 0.004490068182349205, 0.005862277466803789, 0.022812334820628166, 0.0], [0.938591718673706, 0.021227749064564705, 0.004872478079050779, 0.010940182954072952, 0.009524590335786343, 0.014843449927866459]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626558482646942, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.0051893810741603374, 0.0063303555361926556, 0.0, 0.0, 0.0], [0.9477092027664185, 0.017985135316848755, 0.0101566007360816, 0.024149004369974136, 0.0, 0.0], [0.9671926498413086, 0.006552808452397585, 0.0033227831590920687, 0.005563320126384497, 0.017368406057357788, 0.0], [0.9584562182426453, 0.007502940017729998, 0.005136321298778057, 0.008071621879935265, 0.005997124593704939, 0.014835815876722336]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8840709328651428, 0.11592899262905121, 0.0, 0.0, 0.0, 0.0], [0.9931254386901855, 0.005070803686976433, 0.0018038037233054638, 0.0, 0.0, 0.0], [0.9534159302711487, 0.023829057812690735, 0.007748991250991821, 0.015006075613200665, 0.0, 0.0], [0.9151287078857422, 0.010873107239603996, 0.013190997764468193, 0.011050429195165634, 0.04975675791501999, 0.0], [0.8769673109054565, 0.03385208174586296, 0.00848648976534605, 0.009969139471650124, 0.034685853868722916, 0.03603913635015488]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00037095055449754, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.524990021716803e-05, 0.37378302216529846, 0.6261518001556396, 0.0, 0.0, 0.0], [4.6060202294029295e-05, 0.21050900220870972, 0.41159701347351074, 0.37784793972969055, 0.0, 0.0], [4.753091707243584e-05, 0.11616972088813782, 0.23264293372631073, 0.39853307604789734, 0.25260666012763977, 0.0], [1.2476431265895371e-06, 0.14819703996181488, 0.15813156962394714, 0.30074343085289, 0.11939020454883575, 0.2735365033149719]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.02844420075416565, 0.0, 0.0, 0.0, 0.0], [0.9529065489768982, 0.0323307067155838, 0.01476274710148573, 0.0, 0.0, 0.0], [0.9343127012252808, 0.023512955754995346, 0.020498046651482582, 0.021676257252693176, 0.0, 0.0], [0.9529678225517273, 0.008551419712603092, 0.0043593295849859715, 0.008064556866884232, 0.026056945323944092, 0.0], [0.9653593897819519, 0.008487647399306297, 0.00349928205832839, 0.002721576252952218, 0.0032828852999955416, 0.016649367287755013]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630781769752502, 0.13692183792591095, 0.0, 0.0, 0.0, 0.0], [0.7696154713630676, 0.08513331413269043, 0.14525116980075836, 0.0, 0.0, 0.0], [0.713333249092102, 0.10170916467905045, 0.11931303888559341, 0.06564458459615707, 0.0, 0.0], [0.7186220288276672, 0.05444295331835747, 0.013868178240954876, 0.07808024436235428, 0.1349865198135376, 0.0], [0.7990155220031738, 0.0580558180809021, 0.009447004646062851, 0.01777038723230362, 0.021138489246368408, 0.09457285702228546]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.04810105636715889, 0.0, 0.0, 0.0, 0.0], [0.8580652475357056, 0.029445769265294075, 0.11248904466629028, 0.0, 0.0, 0.0], [0.6577733755111694, 0.08513451367616653, 0.12613117694854736, 0.13096091151237488, 0.0, 0.0], [0.8087366819381714, 0.032301660627126694, 0.01841818355023861, 0.06856147199869156, 0.07198206335306168, 0.0], [0.6683299541473389, 0.13281367719173431, 0.021880606189370155, 0.02787739969789982, 0.04923413321375847, 0.09986421465873718]]]], \"left_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"], \"right_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-44469e5e4085440eb4a5d60e6255af5b\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12}; // HACK: {\"attention\": [{\"name\": null, \"attn\": [[[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.961219847202301, 0.03878015652298927, 0.0, 0.0, 0.0, 0.0], [0.7466979622840881, 0.11987316608428955, 0.13342881202697754, 0.0, 0.0, 0.0], [0.5885031223297119, 0.13792064785957336, 0.2121373564004898, 0.06143894046545029, 0.0, 0.0], [0.657085657119751, 0.08996300399303436, 0.12751281261444092, 0.08361566811800003, 0.041822824627161026, 0.0], [0.2728872299194336, 0.11203358322381973, 0.1663985550403595, 0.08467113971710205, 0.1695273518562317, 0.19448217749595642]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010616553947329521, 0.9893833994865417, 0.0, 0.0, 0.0, 0.0], [0.0024677494075149298, 0.008447976782917976, 0.9890843033790588, 0.0, 0.0, 0.0], [0.00012328437878750265, 0.0018733113538473845, 0.013126927427947521, 0.9848765134811401, 0.0, 0.0], [0.001066954922862351, 0.0011366254184395075, 0.003034990979358554, 0.0015735066263005137, 0.9931879043579102, 0.0], [0.00019792039529420435, 0.001052812789566815, 0.001543740276247263, 0.0009642774821259081, 3.492446558084339e-05, 0.9962062835693359]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4757843613624573, 0.524215579032898, 0.0, 0.0, 0.0, 0.0], [0.5906046032905579, 0.24866101145744324, 0.16073429584503174, 0.0, 0.0, 0.0], [0.5529290437698364, 0.18856696784496307, 0.14457567036151886, 0.11392836272716522, 0.0, 0.0], [0.45094093680381775, 0.16486790776252747, 0.1731802374124527, 0.11748012155294418, 0.0935308039188385, 0.0], [0.42572474479675293, 0.17328651249408722, 0.15651947259902954, 0.07022644579410553, 0.08087006211280823, 0.09337273985147476]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6133620738983154, 0.38663801550865173, 0.0, 0.0, 0.0, 0.0], [0.06098512187600136, 0.032534655183553696, 0.9064802527427673, 0.0, 0.0, 0.0], [0.006717090494930744, 0.00040128923137672246, 0.7572956681251526, 0.23558591306209564, 0.0, 0.0], [0.037227679044008255, 0.002948855282738805, 0.10081084817647934, 0.04142273962497711, 0.8175898790359497, 0.0], [0.049897726625204086, 0.0003075831336900592, 0.0024198247119784355, 0.0034334915690124035, 0.0006823895964771509, 0.9432590007781982]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9489555954933167, 0.051044415682554245, 0.0, 0.0, 0.0, 0.0], [0.6821408271789551, 0.13952413201332092, 0.1783350259065628, 0.0, 0.0, 0.0], [0.20366324484348297, 0.05641485005617142, 0.06399302184581757, 0.6759289503097534, 0.0, 0.0], [0.34195467829704285, 0.06725437194108963, 0.0792618989944458, 0.17836204171180725, 0.33316701650619507, 0.0], [0.09464015811681747, 0.007428203243762255, 0.006983973551541567, 0.007184373214840889, 0.018724260851740837, 0.865039050579071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3383461534976959, 0.6616538166999817, 0.0, 0.0, 0.0, 0.0], [0.0785597637295723, 0.006165447179228067, 0.9152747988700867, 0.0, 0.0, 0.0], [0.016775967553257942, 0.0004037704202346504, 0.0033404622226953506, 0.9794798493385315, 0.0, 0.0], [0.02760038524866104, 0.0004441516939550638, 0.0006541667389683425, 0.00022661815455649048, 0.971074640750885, 0.0], [0.01024815533310175, 3.70154702977743e-05, 0.00016064026567619294, 2.7341822715243325e-05, 1.018727652990492e-05, 0.9895166754722595]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.982503354549408, 0.01749664917588234, 0.0, 0.0, 0.0, 0.0], [0.8874197602272034, 0.05467934533953667, 0.05790084972977638, 0.0, 0.0, 0.0], [0.6849910616874695, 0.1228068396449089, 0.04972025007009506, 0.14248186349868774, 0.0, 0.0], [0.6015853881835938, 0.09881889075040817, 0.07070115208625793, 0.1665254533290863, 0.06236908212304115, 0.0], [0.32325056195259094, 0.12567417323589325, 0.044321782886981964, 0.0707697719335556, 0.06606651842594147, 0.36991724371910095]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9191647171974182, 0.0808352679014206, 0.0, 0.0, 0.0, 0.0], [0.45986419916152954, 0.39703109860420227, 0.14310474693775177, 0.0, 0.0, 0.0], [0.30038732290267944, 0.2218172401189804, 0.38161519169807434, 0.09618023037910461, 0.0, 0.0], [0.18963932991027832, 0.13763730227947235, 0.2017350047826767, 0.2363216131925583, 0.23466676473617554, 0.0], [0.15410445630550385, 0.09489510953426361, 0.11902561783790588, 0.10277969390153885, 0.4317217767238617, 0.09747332334518433]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36500003933906555, 0.6349999904632568, 0.0, 0.0, 0.0, 0.0], [0.24595220386981964, 0.5519202351570129, 0.2021276205778122, 0.0, 0.0, 0.0], [0.2721358835697174, 0.40738624334335327, 0.25186216831207275, 0.06861573457717896, 0.0, 0.0], [0.10242553800344467, 0.16683615744113922, 0.5248048901557922, 0.054454632103443146, 0.15147878229618073, 0.0], [0.2502950131893158, 0.2219812572002411, 0.18899965286254883, 0.10677117109298706, 0.13032686710357666, 0.10162601619958878]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6990506052970886, 0.3009493947029114, 0.0, 0.0, 0.0, 0.0], [0.5107942223548889, 0.2948642373085022, 0.1943415254354477, 0.0, 0.0, 0.0], [0.4604707360267639, 0.2805190682411194, 0.19174803793430328, 0.06726215034723282, 0.0, 0.0], [0.3764842450618744, 0.21120667457580566, 0.20214542746543884, 0.10207020491361618, 0.10809347778558731, 0.0], [0.30138447880744934, 0.20456182956695557, 0.18250326812267303, 0.11019383370876312, 0.16291271150112152, 0.03844381868839264]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7131583094596863, 0.2868416905403137, 0.0, 0.0, 0.0, 0.0], [0.40588000416755676, 0.18063302338123322, 0.4134870171546936, 0.0, 0.0, 0.0], [0.265546053647995, 0.16985857486724854, 0.3358593285083771, 0.22873607277870178, 0.0, 0.0], [0.31385403871536255, 0.18316693603992462, 0.14928360283374786, 0.053776729851961136, 0.2999187111854553, 0.0], [0.20466555655002594, 0.18731114268302917, 0.15959151089191437, 0.06381774693727493, 0.036423034965991974, 0.3481909930706024]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6586242318153381, 0.34137579798698425, 0.0, 0.0, 0.0, 0.0], [0.5917776226997375, 0.31600359082221985, 0.09221873432397842, 0.0, 0.0, 0.0], [0.5477152466773987, 0.23586955666542053, 0.06145599111914635, 0.15495921671390533, 0.0, 0.0], [0.45870599150657654, 0.22440002858638763, 0.07887427508831024, 0.09920337796211243, 0.13881628215312958, 0.0], [0.3274371922016144, 0.1960083246231079, 0.06805712729692459, 0.0892510637640953, 0.11618072539567947, 0.20306558907032013]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9961552023887634, 0.0038448371924459934, 0.0, 0.0, 0.0, 0.0], [0.8594854474067688, 0.06906108558177948, 0.07145342975854874, 0.0, 0.0, 0.0], [0.38000524044036865, 0.041275594383478165, 0.5496614575386047, 0.029057739302515984, 0.0, 0.0], [0.21445223689079285, 0.0508873388171196, 0.4317440986633301, 0.25869300961494446, 0.044223327189683914, 0.0], [0.111752949655056, 0.01759309507906437, 0.027507491409778595, 0.04086776077747345, 0.7754665017127991, 0.02681216225028038]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9285967946052551, 0.07140325009822845, 0.0, 0.0, 0.0, 0.0], [0.6077285408973694, 0.3121427297592163, 0.08012861758470535, 0.0, 0.0, 0.0], [0.49429091811180115, 0.28503692150115967, 0.11849319934844971, 0.10217898339033127, 0.0, 0.0], [0.41838788986206055, 0.23117896914482117, 0.08340620994567871, 0.11365960538387299, 0.15336722135543823, 0.0], [0.4221586287021637, 0.1291714310646057, 0.08740921318531036, 0.10163760185241699, 0.2123025506734848, 0.04732052609324455]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9786477088928223, 0.02135234884917736, 0.0, 0.0, 0.0, 0.0], [0.7749120593070984, 0.06510375440120697, 0.15998421609401703, 0.0, 0.0, 0.0], [0.6484923362731934, 0.07483138144016266, 0.14751607179641724, 0.12916028499603271, 0.0, 0.0], [0.5224635601043701, 0.06921815872192383, 0.13823407888412476, 0.11106593906879425, 0.1590181291103363, 0.0], [0.39645180106163025, 0.0732581615447998, 0.12938156723976135, 0.10642433166503906, 0.14863993227481842, 0.1458442062139511]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5525904893875122, 0.4474094808101654, 0.0, 0.0, 0.0, 0.0], [0.5585010051727295, 0.21762590110301971, 0.2238730788230896, 0.0, 0.0, 0.0], [0.5143129229545593, 0.15964671969413757, 0.15491962432861328, 0.17112071812152863, 0.0, 0.0], [0.5039961934089661, 0.11401884257793427, 0.11974027007818222, 0.12552587687969208, 0.13671886920928955, 0.0], [0.5061841607093811, 0.08567395061254501, 0.0890301913022995, 0.09759818762540817, 0.1027572751045227, 0.11875622719526291]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9242545962333679, 0.07574538886547089, 0.0, 0.0, 0.0, 0.0], [0.8257425427436829, 0.07932531088590622, 0.0949321761727333, 0.0, 0.0, 0.0], [0.7306380271911621, 0.0857183039188385, 0.08043929189443588, 0.1032043918967247, 0.0, 0.0], [0.6383241415023804, 0.07886382192373276, 0.07815025746822357, 0.08758099377155304, 0.11708082258701324, 0.0], [0.5552158951759338, 0.07409116625785828, 0.06834886968135834, 0.07778596878051758, 0.09999310225248337, 0.12456490099430084]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8578917384147644, 0.14210829138755798, 0.0, 0.0, 0.0, 0.0], [0.6423039436340332, 0.16629017889499664, 0.19140596687793732, 0.0, 0.0, 0.0], [0.5530980229377747, 0.10609276592731476, 0.0782126635313034, 0.2625965178012848, 0.0, 0.0], [0.4012170732021332, 0.12223611772060394, 0.19347338378429413, 0.14164617657661438, 0.14142730832099915, 0.0], [0.40212586522102356, 0.1845075786113739, 0.07516815513372421, 0.058490391820669174, 0.14446333050727844, 0.13524462282657623]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9791558980941772, 0.020844051614403725, 0.0, 0.0, 0.0, 0.0], [0.8829841613769531, 0.06233251467347145, 0.0546833835542202, 0.0, 0.0, 0.0], [0.8105454444885254, 0.08617089688777924, 0.07321780920028687, 0.030065832659602165, 0.0, 0.0], [0.6819813847541809, 0.04990820214152336, 0.08296553045511246, 0.08369521796703339, 0.10144977271556854, 0.0], [0.40566882491111755, 0.07337658107280731, 0.08601409196853638, 0.061709288507699966, 0.1322644054889679, 0.24096685647964478]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9670192003250122, 0.032980870455503464, 0.0, 0.0, 0.0, 0.0], [0.8449064493179321, 0.0851450115442276, 0.0699484571814537, 0.0, 0.0, 0.0], [0.7123573422431946, 0.07896048575639725, 0.05541076511144638, 0.15327146649360657, 0.0, 0.0], [0.6402613520622253, 0.07397552579641342, 0.04439307376742363, 0.14322122931480408, 0.09814891964197159, 0.0], [0.5073902010917664, 0.0752306878566742, 0.07754657417535782, 0.11362474411725998, 0.13947954773902893, 0.08672831207513809]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8487569689750671, 0.15124309062957764, 0.0, 0.0, 0.0, 0.0], [0.8415650725364685, 0.1210721880197525, 0.03736279159784317, 0.0, 0.0, 0.0], [0.7505519986152649, 0.11348932981491089, 0.061799585819244385, 0.07415914535522461, 0.0, 0.0], [0.6614717841148376, 0.1024264469742775, 0.05293431878089905, 0.07529717683792114, 0.10787024348974228, 0.0], [0.6014202833175659, 0.11340400576591492, 0.056319333612918854, 0.07096728682518005, 0.10906249284744263, 0.048826564103364944]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.944548487663269, 0.05545147880911827, 0.0, 0.0, 0.0, 0.0], [0.8874568939208984, 0.054742131382226944, 0.05780097469687462, 0.0, 0.0, 0.0], [0.8281886577606201, 0.06895007938146591, 0.05903475359082222, 0.04382640868425369, 0.0, 0.0], [0.6429896950721741, 0.06747541576623917, 0.11629696935415268, 0.05417948216199875, 0.11905848979949951, 0.0], [0.7367826104164124, 0.05611899122595787, 0.06857284158468246, 0.03421953693032265, 0.07875366508960724, 0.02555239573121071]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0002913392090704292, 0.9997085928916931, 0.0, 0.0, 0.0, 0.0], [0.0007981191156432033, 0.5288337469100952, 0.4703681170940399, 0.0, 0.0, 0.0], [0.0007648480823263526, 0.34519821405410767, 0.30852678418159485, 0.3455101549625397, 0.0, 0.0], [0.0010283153969794512, 0.24135911464691162, 0.23320147395133972, 0.2555714547634125, 0.26883962750434875, 0.0], [0.0009746804134920239, 0.1778969019651413, 0.16743157804012299, 0.18587613105773926, 0.18734432756900787, 0.2804763615131378]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8244929909706116, 0.17550702393054962, 0.0, 0.0, 0.0, 0.0], [0.12386861443519592, 0.044499851763248444, 0.831631600856781, 0.0, 0.0, 0.0], [0.07924358546733856, 0.012965889647603035, 0.0015277165221050382, 0.9062628149986267, 0.0, 0.0], [0.08806417882442474, 0.021341092884540558, 0.0028886243235319853, 0.0028453839477151632, 0.8848607540130615, 0.0], [0.09983230382204056, 0.033633969724178314, 0.005500008352100849, 0.0024330539163202047, 0.0015082466416060925, 0.8570924401283264]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9646892547607422, 0.0353107713162899, 0.0, 0.0, 0.0, 0.0], [0.7529153823852539, 0.08733484148979187, 0.1597498059272766, 0.0, 0.0, 0.0], [0.42022794485092163, 0.09195111691951752, 0.23549869656562805, 0.252322256565094, 0.0, 0.0], [0.30848926305770874, 0.05908147618174553, 0.3839130699634552, 0.1565914750099182, 0.09192472696304321, 0.0], [0.44790419936180115, 0.04329308494925499, 0.07969187945127487, 0.11081928014755249, 0.22124579548835754, 0.09704567492008209]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.991096019744873, 0.008904017508029938, 0.0, 0.0, 0.0, 0.0], [0.9697675704956055, 0.026084503158926964, 0.004147922620177269, 0.0, 0.0, 0.0], [0.9082901477813721, 0.033206067979335785, 0.00942116230726242, 0.049082688987255096, 0.0, 0.0], [0.8949135541915894, 0.055445462465286255, 0.005577605217695236, 0.031506799161434174, 0.01255655474960804, 0.0], [0.8497735857963562, 0.028890162706375122, 0.0036647969391196966, 0.0375199131667614, 0.03842795267701149, 0.04172346368432045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984525442123413, 0.0015474557876586914, 0.0, 0.0, 0.0, 0.0], [0.4894774556159973, 0.4812203645706177, 0.029302187263965607, 0.0, 0.0, 0.0], [0.11772160977125168, 0.13121220469474792, 0.6702309250831604, 0.08083526045084, 0.0, 0.0], [0.1304369419813156, 0.04068683832883835, 0.26520389318466187, 0.41143542528152466, 0.15223684906959534, 0.0], [0.1266191005706787, 0.032751258462667465, 0.03567875921726227, 0.06039170175790787, 0.6021819114685059, 0.14237721264362335]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.980517566204071, 0.01948240026831627, 0.0, 0.0, 0.0, 0.0], [0.7948852777481079, 0.12061893939971924, 0.08449575304985046, 0.0, 0.0, 0.0], [0.5612363815307617, 0.1574312299489975, 0.20339664816856384, 0.07793566584587097, 0.0, 0.0], [0.4258384108543396, 0.10742022097110748, 0.15123607218265533, 0.08755000680685043, 0.22795531153678894, 0.0], [0.24752643704414368, 0.024188265204429626, 0.03039526753127575, 0.08586962521076202, 0.5714335441589355, 0.04058679938316345]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887767434120178, 0.01122323703020811, 0.0, 0.0, 0.0, 0.0], [0.7572692036628723, 0.2231736183166504, 0.01955709606409073, 0.0, 0.0, 0.0], [0.5341880917549133, 0.2210758924484253, 0.17621803283691406, 0.0685180276632309, 0.0, 0.0], [0.17095300555229187, 0.08229414373636246, 0.5760217905044556, 0.11097608506679535, 0.05975497514009476, 0.0], [0.24871177971363068, 0.08880802243947983, 0.08980196714401245, 0.0972934365272522, 0.4413082003593445, 0.03407653048634529]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8422131538391113, 0.15778684616088867, 0.0, 0.0, 0.0, 0.0], [0.4684130549430847, 0.46105337142944336, 0.0705336406826973, 0.0, 0.0, 0.0], [0.25881415605545044, 0.46358874440193176, 0.18503506481647491, 0.0925619900226593, 0.0, 0.0], [0.18399569392204285, 0.2915416359901428, 0.17031100392341614, 0.271729975938797, 0.0824216827750206, 0.0], [0.16469906270503998, 0.2472696751356125, 0.08770576864480972, 0.22574983537197113, 0.17745333909988403, 0.09712236374616623]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9919946193695068, 0.008005395531654358, 0.0, 0.0, 0.0, 0.0], [0.9068724513053894, 0.04406513273715973, 0.04906249791383743, 0.0, 0.0, 0.0], [0.8582223653793335, 0.055348243564367294, 0.04041939601302147, 0.04601001366972923, 0.0, 0.0], [0.7855250835418701, 0.04124249890446663, 0.08369286358356476, 0.04887617379426956, 0.04066333919763565, 0.0], [0.7856320142745972, 0.0501464307308197, 0.04751263186335564, 0.02736588381230831, 0.056147389113903046, 0.03319568186998367]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9041037559509277, 0.09589625895023346, 0.0, 0.0, 0.0, 0.0], [0.5862313508987427, 0.07199815660715103, 0.3417705297470093, 0.0, 0.0, 0.0], [0.3878959119319916, 0.046608008444309235, 0.20279017090797424, 0.3627059757709503, 0.0, 0.0], [0.26652413606643677, 0.024533025920391083, 0.12211950123310089, 0.20041197538375854, 0.38641130924224854, 0.0], [0.2335742563009262, 0.020537324249744415, 0.09610339999198914, 0.13062246143817902, 0.22990480065345764, 0.28925779461860657]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.963991105556488, 0.03600893169641495, 0.0, 0.0, 0.0, 0.0], [0.7075551152229309, 0.25427767634391785, 0.038167133927345276, 0.0, 0.0, 0.0], [0.25665274262428284, 0.20589302480220795, 0.01665661111474037, 0.520797610282898, 0.0, 0.0], [0.10379385948181152, 0.04639101028442383, 0.008698644116520882, 0.7866848707199097, 0.05443154275417328, 0.0], [0.22143402695655823, 0.0337974950671196, 0.029023971408605576, 0.5412918925285339, 0.15286105871200562, 0.021591659635305405]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.989170253276825, 0.010829685255885124, 0.0, 0.0, 0.0, 0.0], [0.7913153767585754, 0.12309646606445312, 0.08558811247348785, 0.0, 0.0, 0.0], [0.2954603433609009, 0.15808320045471191, 0.4217235743999481, 0.12473281472921371, 0.0, 0.0], [0.23441055417060852, 0.0988653227686882, 0.3316012918949127, 0.19713936746120453, 0.13798347115516663, 0.0], [0.1972840130329132, 0.05741839483380318, 0.06909026205539703, 0.16469816863536835, 0.27972766757011414, 0.23178143799304962]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9359127879142761, 0.06408723443746567, 0.0, 0.0, 0.0, 0.0], [0.7888628244400024, 0.08673480153083801, 0.12440239638090134, 0.0, 0.0, 0.0], [0.6535121202468872, 0.07573549449443817, 0.09732560813426971, 0.17342683672904968, 0.0, 0.0], [0.5222765207290649, 0.058278851211071014, 0.09920472651720047, 0.1702084243297577, 0.1500314474105835, 0.0], [0.4108835756778717, 0.04730607569217682, 0.07265683263540268, 0.10560745000839233, 0.10550019145011902, 0.25804585218429565]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9683833122253418, 0.03161671757698059, 0.0, 0.0, 0.0, 0.0], [0.8965396881103516, 0.03887055814266205, 0.06458978354930878, 0.0, 0.0, 0.0], [0.8264951705932617, 0.03213461861014366, 0.05196719244122505, 0.08940298110246658, 0.0, 0.0], [0.7718172073364258, 0.030402861535549164, 0.0458274744451046, 0.07118476927280426, 0.08076761662960052, 0.0], [0.7292329668998718, 0.021699856966733932, 0.03307478129863739, 0.047200966626405716, 0.06474560499191284, 0.10404585301876068]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9979567527770996, 0.0020432916935533285, 0.0, 0.0, 0.0, 0.0], [0.9552939534187317, 0.008025328628718853, 0.036680713295936584, 0.0, 0.0, 0.0], [0.9254708290100098, 0.002755587687715888, 0.0020629996433854103, 0.06971065700054169, 0.0, 0.0], [0.8660573363304138, 0.0038883851375430822, 0.000678603071719408, 0.0006981496699154377, 0.12867748737335205, 0.0], [0.8455931544303894, 0.003780404571443796, 0.00025342340813949704, 6.0270424000918865e-05, 0.00011820694635389373, 0.1501944661140442]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926245391368866, 0.07375462353229523, 0.0, 0.0, 0.0, 0.0], [0.7717148065567017, 0.16242054104804993, 0.06586459279060364, 0.0, 0.0, 0.0], [0.8167630434036255, 0.07807210832834244, 0.0632404014468193, 0.041924409568309784, 0.0, 0.0], [0.6867170333862305, 0.0775521919131279, 0.10056910663843155, 0.05955108255147934, 0.07561051100492477, 0.0], [0.6421169638633728, 0.11014891415834427, 0.07688148319721222, 0.0540333017706871, 0.10333617776632309, 0.013483177870512009]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9395950436592102, 0.06040492281317711, 0.0, 0.0, 0.0, 0.0], [0.23004497587680817, 0.6617403626441956, 0.10821466147899628, 0.0, 0.0, 0.0], [0.26702186465263367, 0.3607969880104065, 0.32496148347854614, 0.04721970111131668, 0.0, 0.0], [0.5952020287513733, 0.12269298732280731, 0.06302020698785782, 0.08916758000850677, 0.1299172192811966, 0.0], [0.10284583270549774, 0.02938019670546055, 0.013739015907049179, 0.04586033150553703, 0.7698504328727722, 0.038324225693941116]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9040982723236084, 0.09590171277523041, 0.0, 0.0, 0.0, 0.0], [0.3572365343570709, 0.6274626851081848, 0.015300804749131203, 0.0, 0.0, 0.0], [0.5917993783950806, 0.27640512585639954, 0.1047598347067833, 0.027035698294639587, 0.0, 0.0], [0.725443422794342, 0.04983116313815117, 0.014982681721448898, 0.17781180143356323, 0.03193084895610809, 0.0], [0.7612781524658203, 0.06158874183893204, 0.005942091345787048, 0.01642647571861744, 0.12677863240242004, 0.027985820546746254]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9947587847709656, 0.0052412403747439384, 0.0, 0.0, 0.0, 0.0], [0.9632413387298584, 0.017816439270973206, 0.018942102789878845, 0.0, 0.0, 0.0], [0.9671077132225037, 0.00850959774106741, 0.008562250062823296, 0.015820471569895744, 0.0, 0.0], [0.9340996146202087, 0.011952398344874382, 0.020180240273475647, 0.026750784367322922, 0.007016909774392843, 0.0], [0.9587239027023315, 0.0046570878475904465, 0.003326777368783951, 0.006545277312397957, 0.010182438418269157, 0.0165645033121109]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9769991040229797, 0.023000895977020264, 0.0, 0.0, 0.0, 0.0], [0.7917605638504028, 0.17533239722251892, 0.032907020300626755, 0.0, 0.0, 0.0], [0.7949193120002747, 0.10531842708587646, 0.04021839052438736, 0.05954379960894585, 0.0, 0.0], [0.7097724676132202, 0.10552524775266647, 0.06597544252872467, 0.05765583738684654, 0.06107091158628464, 0.0], [0.7506600022315979, 0.02651444636285305, 0.021576009690761566, 0.034296732395887375, 0.08494473248720169, 0.08200826495885849]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9837515950202942, 0.0162484273314476, 0.0, 0.0, 0.0, 0.0], [0.5615504384040833, 0.0895683541893959, 0.34888115525245667, 0.0, 0.0, 0.0], [0.32929134368896484, 0.024114875122904778, 0.5428050756454468, 0.10378869622945786, 0.0, 0.0], [0.3433033525943756, 0.013086446560919285, 0.5121970772743225, 0.1114620715379715, 0.019951023161411285, 0.0], [0.47928258776664734, 0.01733352243900299, 0.11805258691310883, 0.061302680522203445, 0.20071890950202942, 0.12330960482358932]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9908847212791443, 0.009115303866565228, 0.0, 0.0, 0.0, 0.0], [0.5282692313194275, 0.3292275369167328, 0.14250318706035614, 0.0, 0.0, 0.0], [0.48788610100746155, 0.23368680477142334, 0.17578008770942688, 0.10264701396226883, 0.0, 0.0], [0.3144499659538269, 0.18065093457698822, 0.1687125414609909, 0.0950651690363884, 0.24112141132354736, 0.0], [0.5168768763542175, 0.03589698299765587, 0.026187948882579803, 0.040397047996520996, 0.18791775405406952, 0.1927233636379242]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8750301003456116, 0.12496990710496902, 0.0, 0.0, 0.0, 0.0], [0.45505934953689575, 0.4900447130203247, 0.05489590764045715, 0.0, 0.0, 0.0], [0.29337161779403687, 0.5449915528297424, 0.0944426879286766, 0.06719415634870529, 0.0, 0.0], [0.4897075593471527, 0.2721004784107208, 0.06861964613199234, 0.14694835245609283, 0.022623902186751366, 0.0], [0.4729074239730835, 0.08103105425834656, 0.016052115708589554, 0.30672237277030945, 0.10120674967765808, 0.02208031341433525]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.963021993637085, 0.03697802126407623, 0.0, 0.0, 0.0, 0.0], [0.7557188272476196, 0.1643645018339157, 0.07991666346788406, 0.0, 0.0, 0.0], [0.694770872592926, 0.08409887552261353, 0.06382588297128677, 0.15730437636375427, 0.0, 0.0], [0.5821155905723572, 0.03297805413603783, 0.0793655514717102, 0.19441358745098114, 0.11112719774246216, 0.0], [0.5974549651145935, 0.04261094331741333, 0.06919678300619125, 0.14563405513763428, 0.12481731921434402, 0.0202859565615654]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9957822561264038, 0.004217814654111862, 0.0, 0.0, 0.0, 0.0], [0.9312834143638611, 0.010560267604887486, 0.058156415820121765, 0.0, 0.0, 0.0], [0.8435331583023071, 0.015694988891482353, 0.04575095698237419, 0.09502096474170685, 0.0, 0.0], [0.7724104523658752, 0.011981240473687649, 0.03504590317606926, 0.038767579942941666, 0.1417948305606842, 0.0], [0.7642903923988342, 0.009868821129202843, 0.008122745901346207, 0.013314379379153252, 0.04824402555823326, 0.15615971386432648]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9701176285743713, 0.029882336035370827, 0.0, 0.0, 0.0, 0.0], [0.6563995480537415, 0.2250628024339676, 0.11853764951229095, 0.0, 0.0, 0.0], [0.6958045363426208, 0.1470196545124054, 0.07146001607179642, 0.08571576327085495, 0.0, 0.0], [0.6353277564048767, 0.13460667431354523, 0.030994096770882607, 0.05691614747047424, 0.14215528964996338, 0.0], [0.6779407262802124, 0.053654126822948456, 0.01800624467432499, 0.06284503638744354, 0.11038195341825485, 0.0771719217300415]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9822334051132202, 0.01776665635406971, 0.0, 0.0, 0.0, 0.0], [0.9037663340568542, 0.06541521847248077, 0.030818503350019455, 0.0, 0.0, 0.0], [0.8119180798530579, 0.0367904007434845, 0.060561105608940125, 0.09073032438755035, 0.0, 0.0], [0.4054645299911499, 0.10383853316307068, 0.10211227834224701, 0.35434234142303467, 0.034242235124111176, 0.0], [0.22824543714523315, 0.01727859117090702, 0.05055435001850128, 0.60157310962677, 0.09411708265542984, 0.00823147315531969]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9873148202896118, 0.012685172259807587, 0.0, 0.0, 0.0, 0.0], [0.3544574975967407, 0.5317599177360535, 0.1137826219201088, 0.0, 0.0, 0.0], [0.07823366671800613, 0.7221353054046631, 0.10936662554740906, 0.0902644619345665, 0.0, 0.0], [0.2196805477142334, 0.4048425555229187, 0.12358102947473526, 0.2001877725124359, 0.051708076149225235, 0.0], [0.3608982563018799, 0.10459048300981522, 0.06983845680952072, 0.2976471781730652, 0.13869889080524445, 0.02832678146660328]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9732162356376648, 0.026783788576722145, 0.0, 0.0, 0.0, 0.0], [0.9167553782463074, 0.06145269423723221, 0.021791934967041016, 0.0, 0.0, 0.0], [0.8543075919151306, 0.08049620687961578, 0.030335109680891037, 0.034860990941524506, 0.0, 0.0], [0.8919222354888916, 0.04280753806233406, 0.022044949233531952, 0.02347046323120594, 0.019754746928811073, 0.0], [0.8116774559020996, 0.03413516283035278, 0.0356765054166317, 0.047485511749982834, 0.025396987795829773, 0.04562840983271599]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502761363983154, 0.04972385615110397, 0.0, 0.0, 0.0, 0.0], [0.7637456059455872, 0.20073604583740234, 0.03551841527223587, 0.0, 0.0, 0.0], [0.6279088854789734, 0.03768148273229599, 0.1994541436433792, 0.13495543599128723, 0.0, 0.0], [0.6397086381912231, 0.02700713276863098, 0.09081915766000748, 0.20653723180294037, 0.035927820950746536, 0.0], [0.4559429883956909, 0.021641205996274948, 0.1293955296278, 0.2180088311433792, 0.10379859805107117, 0.07121279090642929]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498406648635864, 0.05015937238931656, 0.0, 0.0, 0.0, 0.0], [0.8688726425170898, 0.08722181618213654, 0.04390554130077362, 0.0, 0.0, 0.0], [0.6937952637672424, 0.0635920837521553, 0.09179038554430008, 0.15082231163978577, 0.0, 0.0], [0.7266590595245361, 0.04389895126223564, 0.04683984816074371, 0.09851840138435364, 0.08408375084400177, 0.0], [0.7848993539810181, 0.03714781999588013, 0.012907813303172588, 0.010539380833506584, 0.12079199403524399, 0.03371361270546913]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9891054034233093, 0.010894606821238995, 0.0, 0.0, 0.0, 0.0], [0.892951250076294, 0.08700107038021088, 0.020047664642333984, 0.0, 0.0, 0.0], [0.7891112565994263, 0.09797292202711105, 0.08633271604776382, 0.02658306621015072, 0.0, 0.0], [0.885063886642456, 0.036449968814849854, 0.05395442247390747, 0.012377167120575905, 0.012154590338468552, 0.0], [0.6861320734024048, 0.05720379576086998, 0.011636333540081978, 0.021660450845956802, 0.17488113045692444, 0.04848625510931015]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9396193027496338, 0.060380712151527405, 0.0, 0.0, 0.0, 0.0], [0.7851804494857788, 0.197513610124588, 0.017305942252278328, 0.0, 0.0, 0.0], [0.7660509347915649, 0.15444689989089966, 0.0318828746676445, 0.04761933162808418, 0.0, 0.0], [0.7035239934921265, 0.05171409249305725, 0.077609583735466, 0.15338999032974243, 0.013762434013187885, 0.0], [0.7121893763542175, 0.04994234815239906, 0.03772536665201187, 0.08649078756570816, 0.06541399657726288, 0.04823809117078781]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.974072277545929, 0.025927752256393433, 0.0, 0.0, 0.0, 0.0], [0.7925394177436829, 0.011715620756149292, 0.19574491679668427, 0.0, 0.0, 0.0], [0.5106772184371948, 0.007296795025467873, 0.03962019085884094, 0.4424057602882385, 0.0, 0.0], [0.5862478017807007, 0.012099707499146461, 0.024585247039794922, 0.06737809628248215, 0.3096891939640045, 0.0], [0.30196306109428406, 0.007724035996943712, 0.01151816826313734, 0.046947166323661804, 0.2214674949645996, 0.4103800356388092]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9744554162025452, 0.025544587522745132, 0.0, 0.0, 0.0, 0.0], [0.9769196510314941, 0.015048512257635593, 0.008031918667256832, 0.0, 0.0, 0.0], [0.9060618877410889, 0.025875478982925415, 0.025954892858862877, 0.04210778325796127, 0.0, 0.0], [0.9400084614753723, 0.005556638352572918, 0.0058282846584916115, 0.03175770491361618, 0.016849035397171974, 0.0], [0.9105738401412964, 0.0019752189982682467, 0.008646726608276367, 0.013360840268433094, 0.03543964400887489, 0.030003678053617477]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.979166567325592, 0.020833449438214302, 0.0, 0.0, 0.0, 0.0], [0.8444863557815552, 0.13507838547229767, 0.02043527364730835, 0.0, 0.0, 0.0], [0.7903082966804504, 0.14559200406074524, 0.037530045956373215, 0.026569729670882225, 0.0, 0.0], [0.7298941016197205, 0.05649605393409729, 0.032735347747802734, 0.10400402545928955, 0.07687048614025116, 0.0], [0.568419873714447, 0.04388812184333801, 0.02629331313073635, 0.08117102831602097, 0.24314767122268677, 0.03707994148135185]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9499867558479309, 0.05001330375671387, 0.0, 0.0, 0.0, 0.0], [0.9336171746253967, 0.05848868191242218, 0.007894241251051426, 0.0, 0.0, 0.0], [0.7897834181785583, 0.11071828007698059, 0.05360167473554611, 0.045896559953689575, 0.0, 0.0], [0.8859302997589111, 0.057529717683792114, 0.013743197545409203, 0.0033877412788569927, 0.03940894827246666, 0.0], [0.9337607622146606, 0.026470690965652466, 0.004523388110101223, 0.0061904797330498695, 0.01413296815007925, 0.014921694993972778]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.8521789657555132e-09, 0.0, 0.0, 0.0, 0.0], [6.675865279248683e-06, 0.9999804496765137, 1.2841366697102785e-05, 0.0, 0.0, 0.0], [2.219356076693657e-08, 2.6684428000578464e-09, 0.9999971389770508, 2.8136157652625116e-06, 0.0, 0.0], [1.01456237189268e-06, 4.464133951387339e-08, 0.0003535677387844771, 0.9993677735328674, 0.0002776373294182122, 0.0], [9.436675885154955e-10, 1.3820520249063772e-11, 5.017758986092247e-10, 2.965133027998945e-09, 0.9999971389770508, 2.8643603400269058e-06]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9948632121086121, 0.00513677392154932, 0.0, 0.0, 0.0, 0.0], [0.9274218082427979, 0.018323849886655807, 0.05425437539815903, 0.0, 0.0, 0.0], [0.9678993821144104, 0.004143417812883854, 0.004314432851970196, 0.02364278770983219, 0.0, 0.0], [0.8999071717262268, 0.001467167865484953, 0.00029133641510270536, 0.002585020614787936, 0.09574926644563675, 0.0], [0.9386117458343506, 0.0002224828494945541, 0.000614664051681757, 0.001549560227431357, 0.030689271166920662, 0.028312314301729202]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9999959468841553, 4.042755335831316e-06, 0.0, 0.0, 0.0, 0.0], [0.9982761144638062, 3.261408210164518e-06, 0.0017206644406542182, 0.0, 0.0, 0.0], [0.9998809099197388, 5.328855934294552e-08, 6.376191095114336e-07, 0.00011847116547869518, 0.0, 0.0], [0.9996154308319092, 3.473202525583474e-07, 3.892067468314053e-08, 4.468416250347218e-07, 0.00038369710091501474, 0.0], [0.9994840621948242, 1.655026871105747e-08, 2.8715449573724072e-08, 1.0638223102432676e-06, 0.00021266414842102677, 0.0003021186566911638]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514135718345642, 0.048586405813694, 0.0, 0.0, 0.0, 0.0], [0.5749947428703308, 0.39028114080429077, 0.0347241647541523, 0.0, 0.0, 0.0], [0.7442325353622437, 0.17524097859859467, 0.0756472796201706, 0.004879264626652002, 0.0, 0.0], [0.523206889629364, 0.09429334104061127, 0.11381854116916656, 0.19979327917099, 0.06888788938522339, 0.0], [0.47472435235977173, 0.0563659593462944, 0.04530364274978638, 0.06967295706272125, 0.30980348587036133, 0.044129520654678345]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8734644055366516, 0.126535564661026, 0.0, 0.0, 0.0, 0.0], [0.6097905039787292, 0.35417330265045166, 0.0360361710190773, 0.0, 0.0, 0.0], [0.45984211564064026, 0.38697874546051025, 0.09960118681192398, 0.05357794463634491, 0.0, 0.0], [0.5722204446792603, 0.2363625466823578, 0.08344576507806778, 0.06921909749507904, 0.03875207155942917, 0.0], [0.5143575072288513, 0.16723042726516724, 0.09019391983747482, 0.07654452323913574, 0.10578084737062454, 0.04589279368519783]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9812288880348206, 0.018771151080727577, 0.0, 0.0, 0.0, 0.0], [0.6142947673797607, 0.35039713978767395, 0.0353081040084362, 0.0, 0.0, 0.0], [0.5770688652992249, 0.328584223985672, 0.055082451552152634, 0.039264384657144547, 0.0, 0.0], [0.17188116908073425, 0.011042451485991478, 0.05457846447825432, 0.7326594591140747, 0.02983839437365532, 0.0], [0.37830105423927307, 0.01707008294761181, 0.021754104644060135, 0.44096946716308594, 0.06093835458159447, 0.08096688240766525]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9923112392425537, 0.007688738871365786, 0.0, 0.0, 0.0, 0.0], [0.9498788118362427, 0.016709720715880394, 0.033411379903554916, 0.0, 0.0, 0.0], [0.9961295127868652, 0.0008787269471213222, 0.0006868162308819592, 0.0023048450239002705, 0.0, 0.0], [0.9935757517814636, 0.0032634951639920473, 0.0009993825806304812, 0.00027932191733270884, 0.0018820537952706218, 0.0], [0.9907532930374146, 0.00021344218112062663, 0.0004595224454533309, 0.0007905578822828829, 0.004424717277288437, 0.003358344314619899]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9647740125656128, 0.035225991159677505, 0.0, 0.0, 0.0, 0.0], [0.8194127678871155, 0.13654406368732452, 0.04404320567846298, 0.0, 0.0, 0.0], [0.7584251761436462, 0.0068789515644311905, 0.2065333127975464, 0.02816261723637581, 0.0, 0.0], [0.5298108458518982, 0.002678817370906472, 0.07858007401227951, 0.35983920097351074, 0.029091067612171173, 0.0], [0.7544422149658203, 0.00036782497772946954, 0.001971350284293294, 0.0032400612253695726, 0.19423353672027588, 0.04574500769376755]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9749131202697754, 0.02508683130145073, 0.0, 0.0, 0.0, 0.0], [0.930647075176239, 0.05705663561820984, 0.01229619700461626, 0.0, 0.0, 0.0], [0.9305250644683838, 0.05277109891176224, 0.01111944206058979, 0.005584415048360825, 0.0, 0.0], [0.8863320350646973, 0.01292418036609888, 0.01772470958530903, 0.06150195747613907, 0.021517055109143257, 0.0], [0.7916849255561829, 0.015036109834909439, 0.03174788877367973, 0.0339219830930233, 0.0370798222720623, 0.09052923321723938]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9608501195907593, 0.039149872958660126, 0.0, 0.0, 0.0, 0.0], [0.9121276140213013, 0.02257644012570381, 0.06529594957828522, 0.0, 0.0, 0.0], [0.9364106059074402, 0.015584413893520832, 0.02454490214586258, 0.02346000261604786, 0.0, 0.0], [0.9454618096351624, 0.006762299686670303, 0.02202620916068554, 0.009137802757322788, 0.016611848026514053, 0.0], [0.8346170783042908, 0.001881695119664073, 0.005609031766653061, 0.018873531371355057, 0.12449152022600174, 0.01452707126736641]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9964227080345154, 0.003577270545065403, 0.0, 0.0, 0.0, 0.0], [0.9713929891586304, 0.024452976882457733, 0.004154053051024675, 0.0, 0.0, 0.0], [0.9735793471336365, 0.019003355875611305, 0.0036644041538238525, 0.0037529151886701584, 0.0, 0.0], [0.9586310982704163, 0.007116172928363085, 0.009218409657478333, 0.022725693881511688, 0.0023084846325218678, 0.0], [0.973607063293457, 0.008490558713674545, 0.003251240821555257, 0.003606433281674981, 0.004877456929534674, 0.006167223677039146]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.97598797082901, 0.0240120030939579, 0.0, 0.0, 0.0, 0.0], [0.9460636377334595, 0.04211386665701866, 0.011822479777038097, 0.0, 0.0, 0.0], [0.8446812629699707, 0.04293115809559822, 0.05218207836151123, 0.06020554527640343, 0.0, 0.0], [0.9378372430801392, 0.033548641949892044, 0.008826455101370811, 0.002879226813092828, 0.01690846122801304, 0.0], [0.8124933838844299, 0.026967588812112808, 0.05999188870191574, 0.034457236528396606, 0.011011847294867039, 0.05507794767618179]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9001205563545227, 0.0998794212937355, 0.0, 0.0, 0.0, 0.0], [0.6271934509277344, 0.07988736778497696, 0.29291924834251404, 0.0, 0.0, 0.0], [0.7624076008796692, 0.027344295755028725, 0.03867937996983528, 0.17156872153282166, 0.0, 0.0], [0.79959636926651, 0.01433627400547266, 0.014375641942024231, 0.025438494980335236, 0.1462530791759491, 0.0], [0.7851975560188293, 0.042040448635816574, 0.025253605097532272, 0.029083941131830215, 0.029306193813681602, 0.08911836892366409]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954467415809631, 0.004553201142698526, 0.0, 0.0, 0.0, 0.0], [0.9356000423431396, 0.044767580926418304, 0.019632400944828987, 0.0, 0.0, 0.0], [0.5605553984642029, 0.09861965477466583, 0.2998325824737549, 0.04099232703447342, 0.0, 0.0], [0.5893705487251282, 0.11000985652208328, 0.08033650368452072, 0.16754065454006195, 0.05274243280291557, 0.0], [0.2230590581893921, 0.0568079873919487, 0.05467963591217995, 0.24734023213386536, 0.3111236095428467, 0.10698942840099335]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9301449656486511, 0.06985504180192947, 0.0, 0.0, 0.0, 0.0], [0.8936479687690735, 0.08535707741975784, 0.020994914695620537, 0.0, 0.0, 0.0], [0.8404536247253418, 0.10619223117828369, 0.02363678440451622, 0.029717370867729187, 0.0, 0.0], [0.8927385807037354, 0.02478473260998726, 0.00831903237849474, 0.05165451392531395, 0.022503186017274857, 0.0], [0.8646611571311951, 0.009503157809376717, 0.002432981040328741, 0.04796745628118515, 0.04273199662566185, 0.03270323947072029]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9859625697135925, 0.014037422835826874, 0.0, 0.0, 0.0, 0.0], [0.9702037572860718, 0.016807053238153458, 0.012989138253033161, 0.0, 0.0, 0.0], [0.9524770379066467, 0.016064444556832314, 0.013456214219331741, 0.0180023442953825, 0.0, 0.0], [0.9332928657531738, 0.018971983343362808, 0.020146803930401802, 0.01702374592423439, 0.010564545169472694, 0.0], [0.9113592505455017, 0.012528626248240471, 0.022096220403909683, 0.01751856319606304, 0.018517863005399704, 0.017979340627789497]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9681769013404846, 0.03182307630777359, 0.0, 0.0, 0.0, 0.0], [0.9096417427062988, 0.07916691154241562, 0.011191281490027905, 0.0, 0.0, 0.0], [0.8379933834075928, 0.13078251481056213, 0.012141004204750061, 0.01908305659890175, 0.0, 0.0], [0.9116524457931519, 0.05451960116624832, 0.00949935894459486, 0.0074658701196312904, 0.01686273142695427, 0.0], [0.8510289192199707, 0.07338210195302963, 0.008022499270737171, 0.009083169512450695, 0.042610082775354385, 0.015873299911618233]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9799023866653442, 0.02009764313697815, 0.0, 0.0, 0.0, 0.0], [0.9558742642402649, 0.029063284397125244, 0.015062497928738594, 0.0, 0.0, 0.0], [0.7943135499954224, 0.06074092164635658, 0.06907661259174347, 0.07586897909641266, 0.0, 0.0], [0.5494326949119568, 0.031547144055366516, 0.05482024699449539, 0.057880766689777374, 0.3063191771507263, 0.0], [0.6453983187675476, 0.01077093742787838, 0.017528066411614418, 0.021579790860414505, 0.2495826631784439, 0.05514022707939148]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9506808519363403, 0.04931914433836937, 0.0, 0.0, 0.0, 0.0], [0.8553212285041809, 0.09256298840045929, 0.052115827798843384, 0.0, 0.0, 0.0], [0.8508526086807251, 0.04734603688120842, 0.044177182018756866, 0.05762412026524544, 0.0, 0.0], [0.7697132229804993, 0.027885833755135536, 0.031017232686281204, 0.06842483580112457, 0.10295882076025009, 0.0], [0.7931901812553406, 0.04052194580435753, 0.029241986572742462, 0.04478125646710396, 0.048947159200906754, 0.04331747069954872]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9770310521125793, 0.022968942299485207, 0.0, 0.0, 0.0, 0.0], [0.9429817199707031, 0.017321482300758362, 0.0396968275308609, 0.0, 0.0, 0.0], [0.9144344925880432, 0.008583560585975647, 0.013035798445343971, 0.06394600868225098, 0.0, 0.0], [0.9222431778907776, 0.003644014708697796, 0.003740262007340789, 0.010410335846245289, 0.059962138533592224, 0.0], [0.9198878407478333, 0.0030822637490928173, 0.003482747357338667, 0.004206812474876642, 0.02125430852174759, 0.04808592423796654]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.977458119392395, 0.02254188433289528, 0.0, 0.0, 0.0, 0.0], [0.8929324150085449, 0.07475482672452927, 0.0323127843439579, 0.0, 0.0, 0.0], [0.8423509001731873, 0.05980299785733223, 0.03740081936120987, 0.06044524163007736, 0.0, 0.0], [0.767462968826294, 0.035363469272851944, 0.04215509817004204, 0.06658641993999481, 0.08843207359313965, 0.0], [0.6182615160942078, 0.01611061953008175, 0.020167585462331772, 0.03868892788887024, 0.23146988451480865, 0.07530143857002258]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.963485836982727, 0.03651418536901474, 0.0, 0.0, 0.0, 0.0], [0.43639373779296875, 0.5226370692253113, 0.04096921905875206, 0.0, 0.0, 0.0], [0.36086130142211914, 0.3512967824935913, 0.26551079750061035, 0.02233118936419487, 0.0, 0.0], [0.3942915201187134, 0.021704623475670815, 0.07794354856014252, 0.37168946862220764, 0.1343708485364914, 0.0], [0.6310727000236511, 0.01698393188416958, 0.025941966101527214, 0.08615903556346893, 0.21831940114498138, 0.021522900089621544]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9988250136375427, 0.0011750265257433057, 0.0, 0.0, 0.0, 0.0], [0.9944872260093689, 0.00048264089855365455, 0.00503021664917469, 0.0, 0.0, 0.0], [0.9981209635734558, 2.7051630240748636e-05, 0.00011307386739645153, 0.0017389451386407018, 0.0, 0.0], [0.9982239603996277, 6.836584361735731e-05, 0.00010199925600318238, 6.0283757193246856e-05, 0.0015453121159225702, 0.0], [0.9982888102531433, 1.0552238336458686e-06, 3.2780964829726145e-05, 0.00013039026816841215, 0.0006605856469832361, 0.0008863684488460422]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9936710596084595, 0.006328921765089035, 0.0, 0.0, 0.0, 0.0], [0.9727687239646912, 0.0018561383476480842, 0.02537512220442295, 0.0, 0.0, 0.0], [0.9724301099777222, 0.0019586123526096344, 0.011192457750439644, 0.01441886555403471, 0.0, 0.0], [0.9782042503356934, 0.0009589158580638468, 0.0018706476548686624, 0.00632655480876565, 0.012639649212360382, 0.0], [0.9592596888542175, 0.0024555146228522062, 0.0016124180983752012, 0.005019665230065584, 0.006687097251415253, 0.024965699762105942]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9628998041152954, 0.03710014000535011, 0.0, 0.0, 0.0, 0.0], [0.3680197596549988, 0.6152254343032837, 0.016754796728491783, 0.0, 0.0, 0.0], [0.31735214591026306, 0.6140002608299255, 0.05375148355960846, 0.01489606499671936, 0.0, 0.0], [0.48987358808517456, 0.21071450412273407, 0.04693010076880455, 0.2070041298866272, 0.045477770268917084, 0.0], [0.48774272203445435, 0.17695209383964539, 0.06915208697319031, 0.09849290549755096, 0.12091457843780518, 0.046745575964450836]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9794418811798096, 0.020558098331093788, 0.0, 0.0, 0.0, 0.0], [0.6677899956703186, 0.31032389402389526, 0.0218860674649477, 0.0, 0.0, 0.0], [0.7118753790855408, 0.11108547449111938, 0.14187391102313995, 0.035165149718523026, 0.0, 0.0], [0.45014700293540955, 0.040360286831855774, 0.04045799747109413, 0.38856977224349976, 0.08046494424343109, 0.0], [0.49346214532852173, 0.013696905225515366, 0.008126788772642612, 0.13074517250061035, 0.308614045381546, 0.045354992151260376]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846053123474121, 0.015394614078104496, 0.0, 0.0, 0.0, 0.0], [0.9806739091873169, 0.007713806349784136, 0.01161233615130186, 0.0, 0.0, 0.0], [0.9326632022857666, 0.01957837864756584, 0.024103516712784767, 0.02365492470562458, 0.0, 0.0], [0.9422017931938171, 0.000953896960709244, 0.0010897991014644504, 0.0031933619175106287, 0.052561093121767044, 0.0], [0.9352930784225464, 0.0010279371635988355, 0.004444435704499483, 0.0016371363308280706, 0.010590983554720879, 0.04700643941760063]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9985783100128174, 0.0014216777635738254, 0.0, 0.0, 0.0, 0.0], [0.9893350005149841, 0.0011178924469277263, 0.009547130204737186, 0.0, 0.0, 0.0], [0.9979978203773499, 7.997050124686211e-05, 0.00013218850654084235, 0.0017900384264066815, 0.0, 0.0], [0.9986976385116577, 4.104415711481124e-05, 3.868347448587883e-06, 2.367625893384684e-05, 0.0012337109073996544, 0.0], [0.9971562623977661, 1.8522179743740708e-05, 1.8826630139301415e-06, 2.7900254281121306e-05, 0.0006533460109494627, 0.0021419869735836983]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9768234491348267, 0.02317662164568901, 0.0, 0.0, 0.0, 0.0], [0.9194676876068115, 0.05088196322321892, 0.02965034544467926, 0.0, 0.0, 0.0], [0.8474553823471069, 0.061001770198345184, 0.04372374340891838, 0.04781917855143547, 0.0, 0.0], [0.8011624217033386, 0.041867032647132874, 0.043758004903793335, 0.04189474135637283, 0.07131776213645935, 0.0], [0.8031865954399109, 0.024504970759153366, 0.01732354238629341, 0.047443993389606476, 0.06109940633177757, 0.04644133895635605]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9829428195953369, 0.017057137563824654, 0.0, 0.0, 0.0, 0.0], [0.8863733410835266, 0.09492677450180054, 0.018699826672673225, 0.0, 0.0, 0.0], [0.9231084585189819, 0.03696348890662193, 0.032198354601860046, 0.007729677949100733, 0.0, 0.0], [0.9068527221679688, 0.016046587377786636, 0.014310481958091259, 0.04543795436620712, 0.017352258786559105, 0.0], [0.6555962562561035, 0.05091032758355141, 0.02838490903377533, 0.1256556659936905, 0.10546886920928955, 0.03398396074771881]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9502319693565369, 0.04976801201701164, 0.0, 0.0, 0.0, 0.0], [0.8829866647720337, 0.10009617358446121, 0.016917172819375992, 0.0, 0.0, 0.0], [0.8057460188865662, 0.1446353793144226, 0.030189279466867447, 0.01942942664027214, 0.0, 0.0], [0.8706232905387878, 0.03244056552648544, 0.02695164829492569, 0.04410288855433464, 0.025881504639983177, 0.0], [0.6883639693260193, 0.009681489318609238, 0.01644940674304962, 0.0987112894654274, 0.08971197158098221, 0.09708179533481598]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9792683124542236, 0.020731676369905472, 0.0, 0.0, 0.0, 0.0], [0.9523283243179321, 0.02593386545777321, 0.02173778973519802, 0.0, 0.0, 0.0], [0.9144353866577148, 0.017671233043074608, 0.022358505055308342, 0.04553479701280594, 0.0, 0.0], [0.9448293447494507, 0.006467582657933235, 0.006386060733348131, 0.032630886882543564, 0.00968615710735321, 0.0], [0.9347907304763794, 0.007862498052418232, 0.007788183633238077, 0.02143281139433384, 0.008491137996315956, 0.019634803757071495]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.983370304107666, 0.016629686579108238, 0.0, 0.0, 0.0, 0.0], [0.9631112813949585, 0.009229938499629498, 0.02765873447060585, 0.0, 0.0, 0.0], [0.9706627130508423, 0.004149396438151598, 0.006813123356550932, 0.01837467961013317, 0.0, 0.0], [0.987951934337616, 0.002165883546695113, 0.00034901159233413637, 0.0015838207909837365, 0.00794942770153284, 0.0], [0.9457950592041016, 0.014583553187549114, 0.00036529707722365856, 0.000956955598667264, 0.013621604070067406, 0.024677496403455734]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9878059029579163, 0.012194046750664711, 0.0, 0.0, 0.0, 0.0], [0.8710371851921082, 0.09448141604661942, 0.034481462091207504, 0.0, 0.0, 0.0], [0.6309770345687866, 0.1109040230512619, 0.19230300188064575, 0.06581595540046692, 0.0, 0.0], [0.5360509157180786, 0.04618922993540764, 0.13605229556560516, 0.2645542621612549, 0.01715322583913803, 0.0], [0.8287516236305237, 0.02373279444873333, 0.020080383867025375, 0.07245291024446487, 0.03043138049542904, 0.02455095946788788]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8995686173439026, 0.10043134540319443, 0.0, 0.0, 0.0, 0.0], [0.27034321427345276, 0.6504333019256592, 0.07922343164682388, 0.0, 0.0, 0.0], [0.20541763305664062, 0.5892506837844849, 0.18085815012454987, 0.024473493918776512, 0.0, 0.0], [0.5573878288269043, 0.17741289734840393, 0.08806769549846649, 0.09881812334060669, 0.07831349968910217, 0.0], [0.5922902226448059, 0.08700639754533768, 0.05643284320831299, 0.056858986616134644, 0.12181543558835983, 0.08559618890285492]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9316377639770508, 0.0683622807264328, 0.0, 0.0, 0.0, 0.0], [0.9572945833206177, 0.02624361403286457, 0.01646178774535656, 0.0, 0.0, 0.0], [0.9880543351173401, 0.004273326601833105, 0.0029545894358307123, 0.00471765361726284, 0.0, 0.0], [0.99403977394104, 0.0009413412190042436, 0.0004739806754514575, 0.00011646930943243206, 0.004428436513990164, 0.0], [0.9806035161018372, 2.546903124311939e-05, 0.00016239489195868373, 0.00014764198567718267, 0.0013442462077364326, 0.017716852948069572]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.993178129196167, 0.006821890361607075, 0.0, 0.0, 0.0, 0.0], [0.9756524562835693, 0.01318411435931921, 0.011163423769176006, 0.0, 0.0, 0.0], [0.9418966770172119, 0.004721746779978275, 0.0023818088229745626, 0.050999779254198074, 0.0, 0.0], [0.9905040860176086, 0.0022848115768283606, 6.198474875418469e-05, 0.0005984468152746558, 0.006550653837621212, 0.0], [0.9697661399841309, 0.0008878829539753497, 0.00023466694983653724, 0.0017040737438946962, 0.004128328990191221, 0.023278919979929924]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9716230630874634, 0.028376862406730652, 0.0, 0.0, 0.0, 0.0], [0.9223619699478149, 0.02890726923942566, 0.04873079061508179, 0.0, 0.0, 0.0], [0.8426315188407898, 0.02387217804789543, 0.047481440007686615, 0.08601480722427368, 0.0, 0.0], [0.8521119952201843, 0.02074424922466278, 0.04494624584913254, 0.05765005946159363, 0.024547399953007698, 0.0], [0.8800725936889648, 0.022448506206274033, 0.01823572628200054, 0.019254814833402634, 0.01585424318909645, 0.04413411393761635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9412722587585449, 0.0587276890873909, 0.0, 0.0, 0.0, 0.0], [0.9163140058517456, 0.057591959834098816, 0.02609400823712349, 0.0, 0.0, 0.0], [0.839242160320282, 0.05769055336713791, 0.013829052448272705, 0.0892382562160492, 0.0, 0.0], [0.8987162709236145, 0.013477863743901253, 0.0003456453559920192, 0.003298757830634713, 0.08416145294904709, 0.0], [0.8701689839363098, 0.002700872253626585, 0.0014350019628182054, 0.00566619448363781, 0.088743194937706, 0.03128569945693016]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9656724333763123, 0.034327585250139236, 0.0, 0.0, 0.0, 0.0], [0.9178614020347595, 0.062258072197437286, 0.01988055184483528, 0.0, 0.0, 0.0], [0.8233145475387573, 0.06282392889261246, 0.036704398691654205, 0.07715703547000885, 0.0, 0.0], [0.850174605846405, 0.03816927224397659, 0.0319649763405323, 0.05160144716501236, 0.028089681640267372, 0.0], [0.6572401523590088, 0.058774061501026154, 0.04336016625165939, 0.0901322066783905, 0.08146588504314423, 0.06902756541967392]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9162063002586365, 0.08379373699426651, 0.0, 0.0, 0.0, 0.0], [0.9451773762702942, 0.04099282994866371, 0.013829832896590233, 0.0, 0.0, 0.0], [0.8928353786468506, 0.05368685722351074, 0.01759699359536171, 0.035880763083696365, 0.0, 0.0], [0.8337051868438721, 0.0479961521923542, 0.03351322561502457, 0.04680856689810753, 0.03797685354948044, 0.0], [0.8167192339897156, 0.06337135285139084, 0.013286283239722252, 0.020469767972826958, 0.025292349979281425, 0.06086102873086929]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9525134563446045, 0.047486573457717896, 0.0, 0.0, 0.0, 0.0], [0.30198583006858826, 0.6520950794219971, 0.04591907188296318, 0.0, 0.0, 0.0], [0.28558245301246643, 0.5569531917572021, 0.1444738656282425, 0.012990481220185757, 0.0, 0.0], [0.8438042998313904, 0.032251279801130295, 0.03954283893108368, 0.06848151236772537, 0.01592002436518669, 0.0], [0.6664933562278748, 0.060959313064813614, 0.040643513202667236, 0.06804481148719788, 0.09186359494924545, 0.07199548929929733]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9682655334472656, 0.03173445910215378, 0.0, 0.0, 0.0, 0.0], [0.7385216355323792, 0.22856871783733368, 0.032909613102674484, 0.0, 0.0, 0.0], [0.5946674942970276, 0.23033156991004944, 0.14867623150348663, 0.026324672624468803, 0.0, 0.0], [0.6339268684387207, 0.05813005566596985, 0.09654270857572556, 0.14291921257972717, 0.06848114728927612, 0.0], [0.40375715494155884, 0.08945377916097641, 0.07635065168142319, 0.255871057510376, 0.1433035284280777, 0.03126388415694237]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9869793653488159, 0.013020661659538746, 0.0, 0.0, 0.0, 0.0], [0.8631383180618286, 0.11056677997112274, 0.026294905692338943, 0.0, 0.0, 0.0], [0.9488076567649841, 0.028615081682801247, 0.006535593420267105, 0.016041621565818787, 0.0, 0.0], [0.9672168493270874, 0.006605003960430622, 0.00045171717647463083, 0.004844441078603268, 0.020881807431578636, 0.0], [0.9354621171951294, 0.020478062331676483, 0.0011700298637151718, 0.00705695990473032, 0.01631814055144787, 0.0195146594196558]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9846673011779785, 0.015332723967730999, 0.0, 0.0, 0.0, 0.0], [0.9052741527557373, 0.08373655378818512, 0.010989287868142128, 0.0, 0.0, 0.0], [0.8145945072174072, 0.042837370187044144, 0.10568250715732574, 0.036885637789964676, 0.0, 0.0], [0.23519638180732727, 0.01201847568154335, 0.05280109494924545, 0.6516201496124268, 0.048363905400037766, 0.0], [0.31818586587905884, 0.018632402643561363, 0.03948185592889786, 0.3755546510219574, 0.2078723907470703, 0.0402727872133255]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9811733365058899, 0.018826717510819435, 0.0, 0.0, 0.0, 0.0], [0.8618940711021423, 0.06479166448116302, 0.07331428676843643, 0.0, 0.0, 0.0], [0.766453742980957, 0.07330445200204849, 0.1035352423787117, 0.056706640869379044, 0.0, 0.0], [0.8128494620323181, 0.03215496614575386, 0.05900568142533302, 0.05416534096002579, 0.041824568063020706, 0.0], [0.8687859773635864, 0.026987846940755844, 0.02046993002295494, 0.0162973552942276, 0.032183725386857986, 0.035275135189294815]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.926457941532135, 0.073542021214962, 0.0, 0.0, 0.0, 0.0], [0.8403539657592773, 0.06373762339353561, 0.09590839594602585, 0.0, 0.0, 0.0], [0.7330992221832275, 0.06451141089200974, 0.10380076617002487, 0.09858863800764084, 0.0, 0.0], [0.9143611788749695, 0.008257799781858921, 0.007320398464798927, 0.017966315150260925, 0.05209440737962723, 0.0], [0.8971914052963257, 0.008555522188544273, 0.007019452750682831, 0.014860584400594234, 0.03399764746427536, 0.03837532922625542]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9180347919464111, 0.08196521550416946, 0.0, 0.0, 0.0, 0.0], [0.8328663110733032, 0.12199043482542038, 0.04514329135417938, 0.0, 0.0, 0.0], [0.7994154095649719, 0.08744152635335922, 0.03605787828564644, 0.0770852342247963, 0.0, 0.0], [0.8809852004051208, 0.020749634131789207, 0.020554551854729652, 0.017120789736509323, 0.06058980152010918, 0.0], [0.7453038692474365, 0.04433402419090271, 0.022549238055944443, 0.03315272554755211, 0.03357045724987984, 0.12108980864286423]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9867060780525208, 0.013293893076479435, 0.0, 0.0, 0.0, 0.0], [0.982177734375, 0.012414124794304371, 0.005408101249486208, 0.0, 0.0, 0.0], [0.9630486369132996, 0.015290760435163975, 0.010345693677663803, 0.011314942501485348, 0.0, 0.0], [0.9213568568229675, 0.014132469892501831, 0.017639169469475746, 0.01656768098473549, 0.03030369058251381, 0.0], [0.9373326301574707, 0.00906432792544365, 0.007548368535935879, 0.006576458923518658, 0.011827644892036915, 0.027650468051433563]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9951003193855286, 0.004899636376649141, 0.0, 0.0, 0.0, 0.0], [0.9476009607315063, 0.04140780493617058, 0.01099125761538744, 0.0, 0.0, 0.0], [0.9142176508903503, 0.02352375164628029, 0.03914502263069153, 0.02311357483267784, 0.0, 0.0], [0.9534735679626465, 0.008932963944971561, 0.015272833406925201, 0.007908266969025135, 0.014412309974431992, 0.0], [0.9427104592323303, 0.00823306292295456, 0.004650986287742853, 0.004178097005933523, 0.00546351308003068, 0.034764066338539124]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9543375372886658, 0.04566241055727005, 0.0, 0.0, 0.0, 0.0], [0.9696040749549866, 0.019547631964087486, 0.010848279111087322, 0.0, 0.0, 0.0], [0.9710449576377869, 0.012425404042005539, 0.008068868890404701, 0.008460727520287037, 0.0, 0.0], [0.9726192951202393, 0.0026976584922522306, 0.0004483137163333595, 0.0013814778067171574, 0.022853175178170204, 0.0], [0.9675467610359192, 0.00961342453956604, 0.0032030234578996897, 0.004248819779604673, 0.007442229427397251, 0.007945857010781765]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9887008666992188, 0.011299127712845802, 0.0, 0.0, 0.0, 0.0], [0.9382631182670593, 0.042042478919029236, 0.019694417715072632, 0.0, 0.0, 0.0], [0.8351993560791016, 0.03487858176231384, 0.051344819366931915, 0.0785772055387497, 0.0, 0.0], [0.9042678475379944, 0.010541572235524654, 0.016426734626293182, 0.025921881198883057, 0.04284193366765976, 0.0], [0.8913140296936035, 0.00891267228871584, 0.005010711494833231, 0.008175608702003956, 0.013514735735952854, 0.07307220995426178]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8693908452987671, 0.13060913980007172, 0.0, 0.0, 0.0, 0.0], [0.3507988452911377, 0.6063520908355713, 0.042849019169807434, 0.0, 0.0, 0.0], [0.35475632548332214, 0.3502025008201599, 0.24722392857074738, 0.04781724512577057, 0.0, 0.0], [0.3537069857120514, 0.03527746722102165, 0.09567110240459442, 0.4497954249382019, 0.06554897129535675, 0.0], [0.4132595956325531, 0.0905555933713913, 0.05286560207605362, 0.1746789813041687, 0.1738486886024475, 0.09479160606861115]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9629756212234497, 0.0370243638753891, 0.0, 0.0, 0.0, 0.0], [0.9756426811218262, 0.019658580422401428, 0.004698721691966057, 0.0, 0.0, 0.0], [0.9775736927986145, 0.013286304660141468, 0.002559035550802946, 0.006581062916666269, 0.0, 0.0], [0.9870141744613647, 0.007388271391391754, 0.0009579194593243301, 0.001831823494285345, 0.0028077620081603527, 0.0], [0.9409247040748596, 0.016633691266179085, 0.002297905972227454, 0.005890644155442715, 0.005512925796210766, 0.028740182518959045]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9628270864486694, 0.037172894924879074, 0.0, 0.0, 0.0, 0.0], [0.9582236409187317, 0.024641895666718483, 0.017134448513388634, 0.0, 0.0, 0.0], [0.9351298809051514, 0.015331615693867207, 0.014811009168624878, 0.03472743555903435, 0.0, 0.0], [0.9225173592567444, 0.010528774000704288, 0.01101017277687788, 0.019440023228526115, 0.03650379180908203, 0.0], [0.8420169353485107, 0.043571919202804565, 0.007488253526389599, 0.014961487613618374, 0.023852787911891937, 0.06810857355594635]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9926387071609497, 0.007361318916082382, 0.0, 0.0, 0.0, 0.0], [0.9957393407821655, 0.0033469724003225565, 0.0009136894368566573, 0.0, 0.0, 0.0], [0.9869900345802307, 0.001974780112504959, 0.001524552470073104, 0.009510664269328117, 0.0, 0.0], [0.9933527708053589, 0.001020328258164227, 0.00034337403485551476, 0.0010291122598573565, 0.004254385828971863, 0.0], [0.9749016761779785, 0.0004348014772403985, 0.00043065653881058097, 0.0012364378198981285, 0.0015347728040069342, 0.021461615338921547]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9897475242614746, 0.010252472013235092, 0.0, 0.0, 0.0, 0.0], [0.9790638089179993, 0.016509108245372772, 0.004427104722708464, 0.0, 0.0, 0.0], [0.9521436095237732, 0.029432388022542, 0.008943166583776474, 0.00948089174926281, 0.0, 0.0], [0.939594030380249, 0.02151094563305378, 0.01027856208384037, 0.004555220250040293, 0.02406122162938118, 0.0], [0.9205074906349182, 0.016153674572706223, 0.010818609967827797, 0.016644401475787163, 0.014566432684659958, 0.02130942977964878]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9898502826690674, 0.01014974806457758, 0.0, 0.0, 0.0, 0.0], [0.9820908904075623, 0.006907513365149498, 0.011001549661159515, 0.0, 0.0, 0.0], [0.9684996008872986, 0.008987593464553356, 0.015342592261731625, 0.00717011047527194, 0.0, 0.0], [0.9274123311042786, 0.00948523823171854, 0.022066060453653336, 0.03222879767417908, 0.008807609789073467, 0.0], [0.9006660580635071, 0.021623695269227028, 0.013808242976665497, 0.009843834675848484, 0.008521351031959057, 0.04553688317537308]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9954444169998169, 0.004555598832666874, 0.0, 0.0, 0.0, 0.0], [0.995254397392273, 0.0024602413177490234, 0.002285487949848175, 0.0, 0.0, 0.0], [0.9862446188926697, 0.0015168129466474056, 0.004072288051247597, 0.008166329935193062, 0.0, 0.0], [0.9889963865280151, 0.0012260442599654198, 0.0007996379281394184, 0.0006774240755476058, 0.008300574496388435, 0.0], [0.9865202903747559, 0.0003942710463888943, 0.0009571776608936489, 0.0004954351461492479, 0.0009604924125596881, 0.010672245174646378]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9821295142173767, 0.0178704671561718, 0.0, 0.0, 0.0, 0.0], [0.7489438056945801, 0.22002701461315155, 0.031029196456074715, 0.0, 0.0, 0.0], [0.28547796607017517, 0.21125660836696625, 0.4787161350250244, 0.024549288675189018, 0.0, 0.0], [0.8056640625, 0.026974599808454514, 0.043028153479099274, 0.0699373409152031, 0.0543958880007267, 0.0], [0.330722838640213, 0.022326549515128136, 0.01662709005177021, 0.08019424229860306, 0.4157470166683197, 0.13438232243061066]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9697746634483337, 0.03022538498044014, 0.0, 0.0, 0.0, 0.0], [0.9800565838813782, 0.015018886886537075, 0.004924529232084751, 0.0, 0.0, 0.0], [0.9237861037254333, 0.052764758467674255, 0.006302410736680031, 0.017146745696663857, 0.0, 0.0], [0.9451841711997986, 0.036180540919303894, 0.00198921631090343, 0.003958734683692455, 0.012687314301729202, 0.0], [0.9633325934410095, 0.01866302639245987, 0.003041850635781884, 0.0070709227584302425, 0.005009420681744814, 0.002882068045437336]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.987324595451355, 0.012675454840064049, 0.0, 0.0, 0.0, 0.0], [0.9904569983482361, 0.005541946738958359, 0.00400111498311162, 0.0, 0.0, 0.0], [0.9814971685409546, 0.004653464537113905, 0.003725278889760375, 0.010124054737389088, 0.0, 0.0], [0.9744364619255066, 0.004632262047380209, 0.002379997167736292, 0.006518092937767506, 0.01203305646777153, 0.0], [0.962449848651886, 0.0033743546810001135, 0.0013198552187532187, 0.0017274972051382065, 0.0029446722473949194, 0.028183748945593834]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9807674288749695, 0.019232535734772682, 0.0, 0.0, 0.0, 0.0], [0.9664247035980225, 0.015413912013173103, 0.018161388114094734, 0.0, 0.0, 0.0], [0.9632683396339417, 0.0045381225645542145, 0.002925397362560034, 0.02926814928650856, 0.0, 0.0], [0.9562349319458008, 0.0012223637895658612, 0.0005304092774167657, 0.008671483024954796, 0.033340904861688614, 0.0], [0.9657101035118103, 0.0009808284230530262, 0.001668625045567751, 0.002634829841554165, 0.005866350140422583, 0.023139292374253273]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639716148376465, 0.036028459668159485, 0.0, 0.0, 0.0, 0.0], [0.9562800526618958, 0.033733103424310684, 0.009986814111471176, 0.0, 0.0, 0.0], [0.8539999127388, 0.08073018491268158, 0.03334438428282738, 0.031925465911626816, 0.0, 0.0], [0.9547491073608398, 0.009605025872588158, 0.004146162886172533, 0.002013320801779628, 0.02948642708361149, 0.0], [0.9331134557723999, 0.028699735179543495, 0.005477481987327337, 0.006368075963109732, 0.012613091617822647, 0.013728121295571327]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9392993450164795, 0.06070063263177872, 0.0, 0.0, 0.0, 0.0], [0.929839015007019, 0.06189544126391411, 0.008265510201454163, 0.0, 0.0, 0.0], [0.8471821546554565, 0.09035056084394455, 0.017636140808463097, 0.04483111575245857, 0.0, 0.0], [0.8857702016830444, 0.03918176516890526, 0.007867725566029549, 0.0227658748626709, 0.04441441595554352, 0.0], [0.856328010559082, 0.10088998824357986, 0.006531470455229282, 0.008485930040478706, 0.007368440739810467, 0.02039625681936741]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8353263735771179, 0.16467364132404327, 0.0, 0.0, 0.0, 0.0], [0.6160863041877747, 0.3137645721435547, 0.07014910131692886, 0.0, 0.0, 0.0], [0.3431629240512848, 0.2758498191833496, 0.11966046690940857, 0.26132678985595703, 0.0, 0.0], [0.5908164381980896, 0.050290923565626144, 0.04166606813669205, 0.21994958817958832, 0.09727700054645538, 0.0], [0.8481416702270508, 0.06318087130784988, 0.014733650721609592, 0.05526726692914963, 0.009014973416924477, 0.00966164655983448]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9627320766448975, 0.037267982959747314, 0.0, 0.0, 0.0, 0.0], [0.7757522463798523, 0.17996273934841156, 0.04428492859005928, 0.0, 0.0, 0.0], [0.6317059397697449, 0.24380749464035034, 0.10925635695457458, 0.015230262652039528, 0.0, 0.0], [0.9539911150932312, 0.018182285130023956, 0.011601786129176617, 0.012299060821533203, 0.0039257691241800785, 0.0], [0.4035700857639313, 0.1423756629228592, 0.05661202594637871, 0.19757331907749176, 0.09299200773239136, 0.1068769320845604]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9802619218826294, 0.0197380930185318, 0.0, 0.0, 0.0, 0.0], [0.98739093542099, 0.0078004347160458565, 0.004808679223060608, 0.0, 0.0, 0.0], [0.9283918738365173, 0.008301235735416412, 0.01330567803233862, 0.05000121891498566, 0.0, 0.0], [0.8981053829193115, 0.015591333620250225, 0.01017761416733265, 0.03998704254627228, 0.036138709634542465, 0.0], [0.975350022315979, 0.0003543298807926476, 0.0005866029532626271, 0.0011877480428665876, 0.0010750886285677552, 0.021446382626891136]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9295329451560974, 0.0704670399427414, 0.0, 0.0, 0.0, 0.0], [0.9361504316329956, 0.04116694629192352, 0.0226825550198555, 0.0, 0.0, 0.0], [0.8486821055412292, 0.05802805721759796, 0.024856194853782654, 0.06843377649784088, 0.0, 0.0], [0.8661178946495056, 0.02232474647462368, 0.01036915834993124, 0.02600196562707424, 0.07518626749515533, 0.0], [0.8074419498443604, 0.04438265413045883, 0.018497120589017868, 0.033577870577573776, 0.01856123097240925, 0.07753915339708328]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9680535197257996, 0.03194649890065193, 0.0, 0.0, 0.0, 0.0], [0.9693689942359924, 0.025684893131256104, 0.004946068394929171, 0.0, 0.0, 0.0], [0.9620568156242371, 0.02255241759121418, 0.005471334792673588, 0.009919446893036366, 0.0, 0.0], [0.9727528095245361, 0.010137109085917473, 0.000757327419705689, 0.0028828924987465143, 0.013469826430082321, 0.0], [0.9624636769294739, 0.0031108984258025885, 0.0010007594246417284, 0.0019475906156003475, 0.008266216143965721, 0.023210890591144562]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8542505502700806, 0.14574941992759705, 0.0, 0.0, 0.0, 0.0], [0.9725967645645142, 0.014116342179477215, 0.01328686811029911, 0.0, 0.0, 0.0], [0.9257621169090271, 0.03257268667221069, 0.014612147584557533, 0.027053041383624077, 0.0, 0.0], [0.7923436760902405, 0.02730504609644413, 0.01880672201514244, 0.1385406255722046, 0.02300397865474224, 0.0], [0.6152049899101257, 0.026655390858650208, 0.02935311757028103, 0.05590883269906044, 0.1161130741238594, 0.1567646563053131]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9804654121398926, 0.019534552469849586, 0.0, 0.0, 0.0, 0.0], [0.9882452487945557, 0.007509459275752306, 0.004245327785611153, 0.0, 0.0, 0.0], [0.9584206342697144, 0.010963553562760353, 0.010456085205078125, 0.02015974558889866, 0.0, 0.0], [0.9604811668395996, 0.007182624191045761, 0.0030723451636731625, 0.00689891679212451, 0.022365113720297813, 0.0], [0.966888964176178, 0.003281292272731662, 0.005500538740307093, 0.004234083462506533, 0.005038043484091759, 0.015057160519063473]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9498194456100464, 0.05018052086234093, 0.0, 0.0, 0.0, 0.0], [0.9781363606452942, 0.01643000729382038, 0.005433553829789162, 0.0, 0.0, 0.0], [0.8618696928024292, 0.03609352558851242, 0.0755554735660553, 0.02648121677339077, 0.0, 0.0], [0.544983983039856, 0.015411156229674816, 0.023516563698649406, 0.2574361264705658, 0.1586521416902542, 0.0], [0.9571872353553772, 0.003080392023548484, 0.0014446907443925738, 0.006861583329737186, 0.014818795025348663, 0.01660730689764023]]], [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6156561374664307, 0.3843438923358917, 0.0, 0.0, 0.0, 0.0], [0.367606520652771, 0.42816346883773804, 0.20423001050949097, 0.0, 0.0, 0.0], [0.1647154688835144, 0.4136791527271271, 0.25092384219169617, 0.17068159580230713, 0.0, 0.0], [0.41844630241394043, 0.1524759829044342, 0.10305386781692505, 0.11071502417325974, 0.21530881524085999, 0.0], [0.19686943292617798, 0.2014620453119278, 0.12827253341674805, 0.09203249216079712, 0.09167563915252686, 0.289687842130661]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9027368426322937, 0.09726322442293167, 0.0, 0.0, 0.0, 0.0], [0.9736634492874146, 0.014004282653331757, 0.01233229786157608, 0.0, 0.0, 0.0], [0.8504459261894226, 0.05690564960241318, 0.032060883939266205, 0.06058758124709129, 0.0, 0.0], [0.7661212086677551, 0.03530392050743103, 0.03433046117424965, 0.09675193578004837, 0.06749238818883896, 0.0], [0.8650376796722412, 0.020085245370864868, 0.011498053558170795, 0.018558334559202194, 0.018430272117257118, 0.06639042496681213]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9653082489967346, 0.03469167277216911, 0.0, 0.0, 0.0, 0.0], [0.9816324710845947, 0.014176053926348686, 0.004191512707620859, 0.0, 0.0, 0.0], [0.9275255799293518, 0.04737215116620064, 0.011528274044394493, 0.013573979958891869, 0.0, 0.0], [0.9293117523193359, 0.025833306834101677, 0.0072271269746124744, 0.014300605282187462, 0.02332727052271366, 0.0], [0.8895059823989868, 0.046896327286958694, 0.00471712090075016, 0.0062866006046533585, 0.006090158596634865, 0.04650387167930603]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8938026428222656, 0.10619727522134781, 0.0, 0.0, 0.0, 0.0], [0.8221704959869385, 0.06304488331079483, 0.1147845983505249, 0.0, 0.0, 0.0], [0.5047373175621033, 0.1537574827671051, 0.2277042418718338, 0.11380095779895782, 0.0, 0.0], [0.4082077145576477, 0.09066354483366013, 0.11696869134902954, 0.2455315738916397, 0.1386285126209259, 0.0], [0.7291042804718018, 0.06638873368501663, 0.023112762719392776, 0.031102973967790604, 0.057143229991197586, 0.09314799308776855]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9247532486915588, 0.07524671405553818, 0.0, 0.0, 0.0, 0.0], [0.8957377076148987, 0.06989550590515137, 0.03436676412820816, 0.0, 0.0, 0.0], [0.7924939393997192, 0.0960114374756813, 0.05509119853377342, 0.0564035065472126, 0.0, 0.0], [0.7891507148742676, 0.0788029208779335, 0.038401566445827484, 0.05396970361471176, 0.039674971252679825, 0.0], [0.7807860374450684, 0.07993526756763458, 0.04253169521689415, 0.032342053949832916, 0.017816925421357155, 0.046588074415922165]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9480887651443481, 0.051911257207393646, 0.0, 0.0, 0.0, 0.0], [0.8636947274208069, 0.047562047839164734, 0.08874327689409256, 0.0, 0.0, 0.0], [0.9341371059417725, 0.022224098443984985, 0.02262449823319912, 0.021014342084527016, 0.0, 0.0], [0.9588144421577454, 0.008020879700779915, 0.004490068182349205, 0.005862277466803789, 0.022812334820628166, 0.0], [0.938591718673706, 0.021227749064564705, 0.004872478079050779, 0.010940182954072952, 0.009524590335786343, 0.014843449927866459]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9763734340667725, 0.023626558482646942, 0.0, 0.0, 0.0, 0.0], [0.9884802103042603, 0.0051893810741603374, 0.0063303555361926556, 0.0, 0.0, 0.0], [0.9477092027664185, 0.017985135316848755, 0.0101566007360816, 0.024149004369974136, 0.0, 0.0], [0.9671926498413086, 0.006552808452397585, 0.0033227831590920687, 0.005563320126384497, 0.017368406057357788, 0.0], [0.9584562182426453, 0.007502940017729998, 0.005136321298778057, 0.008071621879935265, 0.005997124593704939, 0.014835815876722336]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8840709328651428, 0.11592899262905121, 0.0, 0.0, 0.0, 0.0], [0.9931254386901855, 0.005070803686976433, 0.0018038037233054638, 0.0, 0.0, 0.0], [0.9534159302711487, 0.023829057812690735, 0.007748991250991821, 0.015006075613200665, 0.0, 0.0], [0.9151287078857422, 0.010873107239603996, 0.013190997764468193, 0.011050429195165634, 0.04975675791501999, 0.0], [0.8769673109054565, 0.03385208174586296, 0.00848648976534605, 0.009969139471650124, 0.034685853868722916, 0.03603913635015488]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00037095055449754, 0.999629020690918, 0.0, 0.0, 0.0, 0.0], [6.524990021716803e-05, 0.37378302216529846, 0.6261518001556396, 0.0, 0.0, 0.0], [4.6060202294029295e-05, 0.21050900220870972, 0.41159701347351074, 0.37784793972969055, 0.0, 0.0], [4.753091707243584e-05, 0.11616972088813782, 0.23264293372631073, 0.39853307604789734, 0.25260666012763977, 0.0], [1.2476431265895371e-06, 0.14819703996181488, 0.15813156962394714, 0.30074343085289, 0.11939020454883575, 0.2735365033149719]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.971555769443512, 0.02844420075416565, 0.0, 0.0, 0.0, 0.0], [0.9529065489768982, 0.0323307067155838, 0.01476274710148573, 0.0, 0.0, 0.0], [0.9343127012252808, 0.023512955754995346, 0.020498046651482582, 0.021676257252693176, 0.0, 0.0], [0.9529678225517273, 0.008551419712603092, 0.0043593295849859715, 0.008064556866884232, 0.026056945323944092, 0.0], [0.9653593897819519, 0.008487647399306297, 0.00349928205832839, 0.002721576252952218, 0.0032828852999955416, 0.016649367287755013]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8630781769752502, 0.13692183792591095, 0.0, 0.0, 0.0, 0.0], [0.7696154713630676, 0.08513331413269043, 0.14525116980075836, 0.0, 0.0, 0.0], [0.713333249092102, 0.10170916467905045, 0.11931303888559341, 0.06564458459615707, 0.0, 0.0], [0.7186220288276672, 0.05444295331835747, 0.013868178240954876, 0.07808024436235428, 0.1349865198135376, 0.0], [0.7990155220031738, 0.0580558180809021, 0.009447004646062851, 0.01777038723230362, 0.021138489246368408, 0.09457285702228546]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9518988728523254, 0.04810105636715889, 0.0, 0.0, 0.0, 0.0], [0.8580652475357056, 0.029445769265294075, 0.11248904466629028, 0.0, 0.0, 0.0], [0.6577733755111694, 0.08513451367616653, 0.12613117694854736, 0.13096091151237488, 0.0, 0.0], [0.8087366819381714, 0.032301660627126694, 0.01841818355023861, 0.06856147199869156, 0.07198206335306168, 0.0], [0.6683299541473389, 0.13281367719173431, 0.021880606189370155, 0.02787739969789982, 0.04923413321375847, 0.09986421465873718]]]], \"left_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"], \"right_text\": [\"No\", \",\", \" I\", \" am\", \" your\", \" father\"]}], \"default_filter\": \"0\", \"display_mode\": \"dark\", \"root_div_id\": \"bertviz-44469e5e4085440eb4a5d60e6255af5b\", \"include_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"include_heads\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11], \"total_heads\": 12} is a template marker that is replaced by actual params.\n        const config = {};\n\n        const MIN_X = 0;\n        const MIN_Y = 0;\n        const DIV_WIDTH = 970;\n        const THUMBNAIL_PADDING = 5;\n        const DETAIL_WIDTH = 300;\n        const DETAIL_ATTENTION_WIDTH = 140;\n        const DETAIL_BOX_WIDTH = 80;\n        const DETAIL_BOX_HEIGHT = 18;\n        const DETAIL_PADDING = 15;\n        const ATTN_PADDING = 0;\n        const DETAIL_HEADING_HEIGHT = 25;\n        const HEADING_TEXT_SIZE = 15;\n        const HEADING_PADDING = 5;\n        const TEXT_SIZE = 13;\n        const TEXT_PADDING = 5;\n        const LAYER_COLORS = d3.schemeCategory10;\n        const PALETTE = {\n            'light': {\n                'text': 'black',\n                'background': 'white',\n                'highlight': '#F5F5F5'\n            },\n            'dark': {\n                'text': '#ccc',\n                'background': 'black',\n                'highlight': '#222'\n            }\n        }\n\n        function render() {\n\n            // Set global state variables\n\n            var attData = config.attention[config.filter];\n            config.leftText = attData.left_text;\n            config.rightText = attData.right_text;\n            config.attn = attData.attn;\n            config.numLayers = config.attn.length;\n            config.numHeads = config.attn[0].length;\n            config.thumbnailBoxHeight = 7 * (12 / config.totalHeads);\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            config.thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n            config.thumbnailWidth = (DIV_WIDTH - axisSize) / config.totalHeads;\n            config.detailHeight = Math.max(config.leftText.length, config.rightText.length) * DETAIL_BOX_HEIGHT + 2 * DETAIL_PADDING + DETAIL_HEADING_HEIGHT;\n            config.divHeight = Math.max(config.numLayers * config.thumbnailHeight + axisSize, config.detailHeight);\n\n            const vis = $(`#${config.rootDivId} #vis`)\n            vis.empty();\n            vis.attr(\"height\", config.divHeight);\n            config.svg = d3.select(`#${config.rootDivId} #vis`)\n                .append('svg')\n                .attr(\"width\", DIV_WIDTH)\n                .attr(\"height\", config.divHeight)\n                .attr(\"fill\", getBackgroundColor());\n\n            renderAxisLabels();\n\n            var i;\n            var j;\n            for (i = 0; i < config.numLayers; i++) {\n                for (j = 0; j < config.numHeads; j++) {\n                    renderThumbnail(i, j);\n                }\n            }\n        }\n\n        function renderAxisLabels() {\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            const tableWidth = config.thumbnailWidth * config.heads.length;\n            config.svg.append(\"text\")\n                .text(\"Heads\")\n                .attr(\"fill\", \"black\")\n                .attr(\"font-weight\", \"bold\")\n                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n                .attr(\"x\", axisSize + tableWidth / 2)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", 0)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n            for (let i = 0; i < config.numHeads; i++) {\n                config.svg.append(\"text\")\n                    .text(config.heads[i])\n                    .attr(\"fill\", \"black\")\n                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n                    .attr(\"x\", axisSize + (i + .5) * config.thumbnailWidth)\n                    .attr(\"text-anchor\", \"middle\")\n                    .attr(\"y\", HEADING_TEXT_SIZE + HEADING_PADDING)\n                    .attr(\"dy\", TEXT_SIZE);\n            }\n            let x = 0;\n            let y = axisSize + config.thumbnailHeight * config.layers.length / 2;\n            console.log(\"x\", x, y)\n            config.svg.append(\"text\")\n                .text(\"Layers\")\n                .attr(\"fill\", \"black\")\n                .attr(\"font-weight\", \"bold\")\n                .attr(\"transform\", \"rotate(270, \" + x  + \", \" + y + \")\")\n                .attr(\"font-size\", HEADING_TEXT_SIZE + \"px\")\n                .attr(\"x\", x)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", y)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n            for (let i = 0; i < config.numLayers; i++) {\n                x = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE; // HACK\n                y = axisSize + (i + .5) * config.thumbnailHeight;\n                config.svg.append(\"text\")\n                    .text(config.layers[i])\n                    .attr(\"fill\", \"black\")\n                    .attr(\"font-size\", TEXT_SIZE + \"px\")\n                    .attr(\"x\", x)\n                    .attr(\"text-anchor\", \"end\")\n                    .attr(\"y\", y)\n                    .attr(\"dy\", TEXT_SIZE / 2);\n            }\n        }\n\n\n        function renderThumbnail(layerIndex, headIndex) {\n            const axisSize = HEADING_TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING\n            const x = headIndex * config.thumbnailWidth + axisSize;\n            const y = layerIndex * config.thumbnailHeight + axisSize;\n            renderThumbnailAttn(x, y, config.attn[layerIndex][headIndex], layerIndex, headIndex);\n        }\n\n        function renderDetail(att, layerIndex, headIndex) {\n            const axisSize = TEXT_SIZE + HEADING_PADDING + TEXT_SIZE + TEXT_PADDING;\n            var xOffset = .8 * config.thumbnailWidth;\n            var maxX = DIV_WIDTH;\n            var maxY = config.divHeight - 3;\n            var leftPos = axisSize + headIndex * config.thumbnailWidth;\n            var x = leftPos + THUMBNAIL_PADDING + xOffset;\n            if (x < MIN_X) {\n                x = MIN_X;\n            } else if (x + DETAIL_WIDTH > maxX) {\n                x = leftPos + THUMBNAIL_PADDING - DETAIL_WIDTH + 8;\n            }\n            var posLeftText = x;\n            var posAttention = posLeftText + DETAIL_BOX_WIDTH;\n            var posRightText = posAttention + DETAIL_ATTENTION_WIDTH;\n            var thumbnailHeight = Math.max(config.leftText.length, config.rightText.length) * config.thumbnailBoxHeight + 2 * THUMBNAIL_PADDING;\n            var yOffset = 20;\n            var y = layerIndex * thumbnailHeight + THUMBNAIL_PADDING + yOffset;\n            if (y < MIN_Y) {\n                y = MIN_Y;\n            } else if (y + config.detailHeight > maxY) {\n                y = maxY - config.detailHeight;\n            }\n            renderDetailFrame(x, y, layerIndex);\n            y = y + DETAIL_PADDING;\n            renderDetailHeading(x, y, layerIndex, headIndex);\n            y = y + DETAIL_HEADING_HEIGHT;\n            renderDetailText(config.leftText, \"leftText\", posLeftText, y , layerIndex);\n            renderDetailAttn(posAttention, y, att, layerIndex, headIndex);\n            renderDetailText(config.rightText, \"rightText\", posRightText, y, layerIndex);\n        }\n\n        function renderDetailHeading(x, y, layerIndex, headIndex) {\n            var fillColor = getTextColor();\n            config.svg.append(\"text\")\n                .classed(\"detail\", true)\n                .text('Layer ' + config.layers[layerIndex] + \", Head \" + config.heads[headIndex])\n                .attr(\"font-size\", TEXT_SIZE + \"px\")\n                .attr(\"font-weight\", \"bold\")\n                .style(\"cursor\", \"default\")\n                .style(\"-webkit-user-select\", \"none\")\n                .attr(\"fill\", fillColor)\n                .attr(\"x\", x + DETAIL_WIDTH / 2)\n                .attr(\"text-anchor\", \"middle\")\n                .attr(\"y\", y)\n                .attr(\"height\", DETAIL_HEADING_HEIGHT)\n                .attr(\"width\", DETAIL_WIDTH)\n                .attr(\"dy\", HEADING_TEXT_SIZE);\n        }\n\n        function renderDetailText(text, id, x, y, layerIndex) {\n            var tokenContainer = config.svg.append(\"svg:g\")\n                .classed(\"detail\", true)\n                .selectAll(\"g\")\n                .data(text)\n                .enter()\n                .append(\"g\");\n\n            var fillColor = getTextColor();\n\n            tokenContainer.append(\"rect\")\n                .classed(\"highlight\", true)\n                .attr(\"fill\", fillColor)\n                .style(\"opacity\", 0.0)\n                .attr(\"height\", DETAIL_BOX_HEIGHT)\n                .attr(\"width\", DETAIL_BOX_WIDTH)\n                .attr(\"x\", x)\n                .attr(\"y\", function (d, i) {\n                    return y + i * DETAIL_BOX_HEIGHT;\n                });\n\n            var textContainer = tokenContainer.append(\"text\")\n                .classed(\"token\", true)\n                .text(function (d) {\n                    return d;\n                })\n                .attr(\"font-size\", TEXT_SIZE + \"px\")\n                .style(\"cursor\", \"default\")\n                .style(\"-webkit-user-select\", \"none\")\n                .attr(\"fill\", fillColor)\n                .attr(\"x\", x)\n                .attr(\"y\", function (d, i) {\n                    return i * DETAIL_BOX_HEIGHT + y;\n                })\n                .attr(\"height\", DETAIL_BOX_HEIGHT)\n                .attr(\"width\", DETAIL_BOX_WIDTH)\n                .attr(\"dy\", TEXT_SIZE);\n\n            if (id == \"leftText\") {\n                textContainer.style(\"text-anchor\", \"end\")\n                    .attr(\"dx\", DETAIL_BOX_WIDTH - 2);\n                tokenContainer.on(\"mouseover\", function (d, index) {\n                    highlightSelection(index);\n                });\n                tokenContainer.on(\"mouseleave\", function () {\n                    unhighlightSelection();\n                });\n            }\n        }\n\n        function highlightSelection(index) {\n            config.svg.select(\"#leftText\")\n                .selectAll(\".highlight\")\n                .style(\"opacity\", function (d, i) {\n                    return i == index ? 1.0 : 0.0;\n                });\n            config.svg.selectAll(\".attn-line-group\")\n                .style(\"opacity\", function (d, i) {\n                    return i == index ? 1.0 : 0.0;\n                });\n        }\n\n        function unhighlightSelection() {\n            config.svg.select(\"#leftText\")\n                .selectAll(\".highlight\")\n                .style(\"opacity\", 0.0);\n            config.svg.selectAll(\".attn-line-group\")\n                .style(\"opacity\", 1);\n        }\n\n        function renderThumbnailAttn(x, y, att, layerIndex, headIndex) {\n\n            var attnContainer = config.svg.append(\"svg:g\");\n\n            var attnBackground = attnContainer.append(\"rect\")\n                .attr(\"id\", 'attn_background_' + layerIndex + \"_\" + headIndex)\n                .classed(\"attn_background\", true)\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.thumbnailHeight)\n                .attr(\"width\", config.thumbnailWidth)\n                .attr(\"stroke-width\", 2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", 0)\n                .attr(\"fill\", getBackgroundColor());\n            var x1 = x + THUMBNAIL_PADDING;\n            var x2 = x1 + config.thumbnailWidth - 14;\n            var y1 = y + THUMBNAIL_PADDING;\n\n            attnContainer.selectAll(\"g\")\n                .data(att)\n                .enter()\n                .append(\"g\") // Add group for each source token\n                .attr(\"source-index\", function (d, i) { // Save index of source token\n                    return i;\n                })\n                .selectAll(\"line\")\n                .data(function (d) { // Loop over all target tokens\n                    return d;\n                })\n                .enter() // When entering\n                .append(\"line\")\n                .attr(\"x1\", x1)\n                .attr(\"y1\", function (d) {\n                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n                    return y1 + (sourceIndex + .5) * config.thumbnailBoxHeight;\n                })\n                .attr(\"x2\", x2)\n                .attr(\"y2\", function (d, targetIndex) {\n                    return y1 + (targetIndex + .5) * config.thumbnailBoxHeight;\n                })\n                .attr(\"stroke-width\", 2.2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", function (d) {\n                    return d;\n                });\n\n            var clickRegion = attnContainer.append(\"rect\")\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.thumbnailHeight)\n                .attr(\"width\", config.thumbnailWidth)\n                .style(\"opacity\", 0);\n\n            clickRegion.on(\"click\", function (d, index) {\n                var attnBackgroundOther = config.svg.selectAll(\".attn_background\");\n                attnBackgroundOther.attr(\"fill\", getBackgroundColor());\n                attnBackgroundOther.attr(\"stroke-opacity\", 0);\n\n                config.svg.selectAll(\".detail\").remove();\n                if (config.detail_layer != layerIndex || config.detail_head != headIndex) {\n                    renderDetail(att, layerIndex, headIndex);\n                    config.detail_layer = layerIndex;\n                    config.detail_head = headIndex;\n                    attnBackground.attr(\"fill\", getHighlightColor());\n                    attnBackground.attr(\"stroke-opacity\", .8);\n                } else {\n                    config.detail_layer = null;\n                    config.detail_head = null;\n                    attnBackground.attr(\"fill\", getBackgroundColor());\n                    attnBackground.attr(\"stroke-opacity\", 0);\n                }\n            });\n\n            clickRegion.on(\"mouseover\", function (d) {\n                d3.select(this).style(\"cursor\", \"pointer\");\n            });\n        }\n\n        function renderDetailFrame(x, y, layerIndex) {\n            var detailFrame = config.svg.append(\"rect\")\n                .classed(\"detail\", true)\n                .attr(\"x\", x)\n                .attr(\"y\", y)\n                .attr(\"height\", config.detailHeight)\n                .attr(\"width\", DETAIL_WIDTH)\n                .style(\"opacity\", 1)\n                .attr(\"stroke-width\", 1.5)\n                .attr(\"stroke-opacity\", 0.7)\n                .attr(\"stroke\", getLayerColor(layerIndex));\n        }\n\n        function renderDetailAttn(x, y, att, layerIndex) {\n            var attnContainer = config.svg.append(\"svg:g\")\n                .classed(\"detail\", true)\n                .attr(\"pointer-events\", \"none\");\n            attnContainer.selectAll(\"g\")\n                .data(att)\n                .enter()\n                .append(\"g\") // Add group for each source token\n                .classed('attn-line-group', true)\n                .attr(\"source-index\", function (d, i) { // Save index of source token\n                    return i;\n                })\n                .selectAll(\"line\")\n                .data(function (d) { // Loop over all target tokens\n                    return d;\n                })\n                .enter()\n                .append(\"line\")\n                .attr(\"x1\", x + ATTN_PADDING)\n                .attr(\"y1\", function (d) {\n                    var sourceIndex = +this.parentNode.getAttribute(\"source-index\");\n                    return y + (sourceIndex + .5) * DETAIL_BOX_HEIGHT;\n                })\n                .attr(\"x2\", x + DETAIL_ATTENTION_WIDTH - ATTN_PADDING)\n                .attr(\"y2\", function (d, targetIndex) {\n                    return y + (targetIndex + .5) * DETAIL_BOX_HEIGHT;\n                })\n                .attr(\"stroke-width\", 2.2)\n                .attr(\"stroke\", getLayerColor(layerIndex))\n                .attr(\"stroke-opacity\", function (d) {\n                    return d;\n                });\n        }\n\n        function getLayerColor(layer) {\n          return LAYER_COLORS[config.layers[layer] % 10];\n        }\n\n        function getTextColor() {\n            return PALETTE[config.mode]['text']\n        }\n\n        function getBackgroundColor() {\n           return PALETTE[config.mode]['background']\n        }\n\n        function getHighlightColor() {\n           return PALETTE[config.mode]['highlight']\n        }\n\n        function initialize() {\n            config.attention = params['attention'];\n            config.filter = params['default_filter'];\n            config.mode = params['display_mode'];\n            config.layers = params['include_layers']\n            config.heads = params['include_heads']\n            config.totalHeads = params['total_heads']\n            config.rootDivId = params['root_div_id'];\n            $(`#${config.rootDivId} #filter`).on('change', function (e) {\n                config.filter = e.currentTarget.value;\n                render();\n            });\n        }\n\n        initialize();\n        render();\n\n    });",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n",
    "\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = 'openai-community/gpt2'\n",
    "input_text = \"No, I am your father\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positional encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFq78-kjbrWp"
   },
   "source": [
    "Last week, Carlo discussed token embedding, which is when words are encoded into a vocabulary. Now, we just discussed attention mechanisms which account for context between words. Another question we should ask is how do we account for the order of words in an input sentence\n",
    "\n",
    "Consider the following two sentences to see why this is important:\n",
    "\n",
    "``The man ate the sandwich.``\n",
    "\n",
    "``The sandwich ate the man.``\n",
    "\n",
    "Clearly, these are two vastly different situations even though they have the same words. The Transformer can \n",
    "\n",
    "Transformers differentiate between these situations by adding a **Positional encoding** vector to each input embedding. These vectors follow a specific pattern that the model learns, which helps it determine the position of each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/positional_encoding.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "Image credit: https://medium.com/@xuer.chen.human/llm-study-notes-positional-encoding-0639a1002ec0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set up positional encoding similarly as token embedding using the ``nn.Embedding`` tool. We use a simple embedding here but there are more complex positional encodings used such as sinusoidal. \n",
    "\n",
    "For an explanation of different positional encodings, refer to this post: https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding(65, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "Embedding                                2,048\n",
       "=================================================================\n",
       "Total params: 2,048\n",
       "Trainable params: 2,048\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 65\n",
    "n_embd = 64\n",
    "\n",
    "token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "print(token_embedding_table)\n",
    "summary(position_embedding_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will notice the positional encoding size is `(block_size, n_embed)` because it encodes for the postion of a token within the sequence of size `block_size`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the position embedding used is simply added to the token embedding to apply positional embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at token embedding alone:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.7221, -0.9629, -2.0578,  1.9740,  0.7434,  1.1139,  0.6926,  0.0296,\n",
      "         0.6405, -1.6464,  0.4935,  0.7485,  0.9238, -0.4940,  0.4814, -0.3859,\n",
      "        -0.3094,  1.1066, -0.2891,  0.1891,  2.0440, -0.7945, -0.4331,  0.3007,\n",
      "         1.4317,  0.2881, -0.4343,  0.4280,  1.2469,  1.4047, -0.3404, -2.2190,\n",
      "         0.4893,  0.0398, -0.2717, -2.2400, -0.0029, -1.4251,  0.7330,  0.3551,\n",
      "         0.1472, -1.1895, -0.8407,  0.3134, -0.6709, -0.8176,  0.6929, -0.6374,\n",
      "         0.3174,  0.4837, -0.0073, -1.5924,  1.8606, -1.2910, -0.1594,  0.3111,\n",
      "        -0.1536, -0.3414, -0.0170, -0.1633,  0.2794,  0.6755,  0.7066, -1.6665],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x = token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And token + positional embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.4326, -1.6287, -0.8684,  3.0704,  0.3646,  1.9826,  0.7582, -0.1918,\n",
      "         1.0491, -2.2562, -0.4931, -0.7808,  1.7206, -1.0297,  2.0798, -1.3427,\n",
      "        -0.7896, -0.1746,  0.0926,  0.0543,  2.3831, -0.6208,  0.3902,  0.1097,\n",
      "         1.0455, -1.4557,  0.3402,  2.6717,  1.8380,  1.2628, -0.4831, -4.6023,\n",
      "         0.6959,  1.0347,  0.5903, -0.7541,  0.4682, -0.3895,  2.1526,  0.6272,\n",
      "        -0.8558, -0.8434,  0.1311, -1.0272, -2.0580,  0.0584,  0.3442, -0.3464,\n",
      "        -0.3444,  2.3134, -1.1142, -1.4629,  3.3503, -2.0594,  1.4105,  0.4558,\n",
      "        -1.3366,  1.9283,  1.5187,  0.3906,  1.1448, -0.8422,  2.2692, -0.7949],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1,3,15,4,7,1,4,9])\n",
    "x= position_embedding_table(x) + token_embedding_table(x)\n",
    "print(x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see a clear offset between these two embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During the training process, these embeddings will be learned to best encode the token and positional embeddings of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iF1HzH9xNJ7S"
   },
   "source": [
    "## Output layers\n",
    "\n",
    "At the end of our Transformer model, we are left with a vector, so how do we turn this into a word?\n",
    "\n",
    "<img src=\"images/transformer-decoder-intro.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "Using a final Linear layer and a Softmax Layer.\n",
    "The Linear layer projects the vector produced by the stack of decoders, into a larger vector called a logits vector.\n",
    "\n",
    "If our model knows 10,000 unique English words learned from its training dataset the logits vector is 10,000 cells wide – each cell corresponds to the score of a unique word.\n",
    "\n",
    "The softmax layer turns those scores into probabilities. The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/transformer_decoder_output_softmax.png\" alt=\"Drawing\" style=\"width: 450px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS6r-z8dN_RV"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XK8q67P03yr4"
   },
   "source": [
    "## Training\n",
    "\n",
    "How does an LLM improve over time?\n",
    "We want to compare the probabilitiy distribution for each token generated by our model to the ground truths. \n",
    "Our model produces a probability distribution for each token. We want to compare these probability distributions to the ground truths. \n",
    "For example, when translating the sentence: “je suis étudiant” into “i am a student” as can be seen in the example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/output_target_probability_distributions.png\" alt=\"Drawing\" style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS6r-z8dN_RV"
   },
   "source": [
    "Image credit: https://jalammar.github.io/illustrated-transformer/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model can calculate the loss between the vector it generates and the ground truth vector seen in this example. A commonly used loss function is cross entropy loss:\n",
    "\n",
    "$CE = -\\sum_{x \\in X} p(x) log q(x)$\n",
    "\n",
    "where p(x) represents the true distribution and q(x) represents the predicted distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9119)\n",
      "tensor([0.4018, 0.2693, 0.3289])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.911800826601303"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from torch.nn import functional as F\n",
    "logits = torch.tensor([0.5, 0.1, 0.3])\n",
    "targets = torch.tensor([1.0, 0.0, 0.0])\n",
    "loss = F.cross_entropy(logits, targets)\n",
    "print(loss)\n",
    "\n",
    "print(F.softmax(logits, dim=-1))\n",
    "-1*math.log(0.4018)-0*math.log(0.3322)-0*math.log(0.2660)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important metric commonly used in LLMs is **perplexity**.\n",
    "\n",
    "Intuitively, perplexity means to be surprised. We measure how much the model is surprised by seeing new data. The lower the perplexity, the better the training is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, perplexity is just the exponent of the negative cross entropy loss:\n",
    "\n",
    "$\\text{perplexity} = exp(\\text{CE})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4891)\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(loss)\n",
    "print(perplexity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we are using cross entropy loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's train a mini-LLM from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "batch_size = 16 # how many independent sequences will we process in parallel?\n",
    "block_size = 32 # what is the maximum context length for predictions?\n",
    "max_iters = 5000\n",
    "eval_interval = 10\n",
    "learning_rate = 1e-3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "eval_iters = 200\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4\n",
    "dropout = 0.0\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data and create train and test datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to be using the tiny Shakespeare dataset. \n",
    "Data is tokenized according to a simple character based tokenizer.\n",
    "Data is split into a train and test set so we have something to test after performing training (9:1 split)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "Vocab size: 65\n"
     ]
    }
   ],
   "source": [
    "with open('dataset/input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('Length of text:', len(text))\n",
    "print(text[:50])\n",
    "# here are all the unique characters that occur in this text\n",
    "chars = sorted(list(set(text))); print(chars)\n",
    "vocab_size = len(chars); print('Vocab size:', vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '$': 3, '&': 4, \"'\": 5, ',': 6, '-': 7, '.': 8, '3': 9, ':': 10, ';': 11, '?': 12, 'A': 13, 'B': 14, 'C': 15, 'D': 16, 'E': 17, 'F': 18, 'G': 19, 'H': 20, 'I': 21, 'J': 22, 'K': 23, 'L': 24, 'M': 25, 'N': 26, 'O': 27, 'P': 28, 'Q': 29, 'R': 30, 'S': 31, 'T': 32, 'U': 33, 'V': 34, 'W': 35, 'X': 36, 'Y': 37, 'Z': 38, 'a': 39, 'b': 40, 'c': 41, 'd': 42, 'e': 43, 'f': 44, 'g': 45, 'h': 46, 'i': 47, 'j': 48, 'k': 49, 'l': 50, 'm': 51, 'n': 52, 'o': 53, 'p': 54, 'q': 55, 'r': 56, 's': 57, 't': 58, 'u': 59, 'v': 60, 'w': 61, 'x': 62, 'y': 63, 'z': 64}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '$', 4: '&', 5: \"'\", 6: ',', 7: '-', 8: '.', 9: '3', 10: ':', 11: ';', 12: '?', 13: 'A', 14: 'B', 15: 'C', 16: 'D', 17: 'E', 18: 'F', 19: 'G', 20: 'H', 21: 'I', 22: 'J', 23: 'K', 24: 'L', 25: 'M', 26: 'N', 27: 'O', 28: 'P', 29: 'Q', 30: 'R', 31: 'S', 32: 'T', 33: 'U', 34: 'V', 35: 'W', 36: 'X', 37: 'Y', 38: 'Z', 39: 'a', 40: 'b', 41: 'c', 42: 'd', 43: 'e', 44: 'f', 45: 'g', 46: 'h', 47: 'i', 48: 'j', 49: 'k', 50: 'l', 51: 'm', 52: 'n', 53: 'o', 54: 'p', 55: 'q', 56: 'r', 57: 's', 58: 't', 59: 'u', 60: 'v', 61: 'w', 62: 'x', 63: 'y', 64: 'z'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# create a mapping from characters to integers\n",
    "stoi = { ch:i for i,ch in enumerate(chars) }; print(stoi)\n",
    "itos = { i:ch for i,ch in enumerate(chars) }; print(itos)\n",
    "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
    "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([18, 47, 56,  ..., 45,  8,  0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([1003854]), torch.Size([111540]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Train and test splits\n",
    "data = torch.tensor(encode(text), dtype=torch.long); print(data)\n",
    "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]\n",
    "train_data.shape, val_data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data loading\n",
    "def get_batch(split):\n",
    "    # generate a small batch of data of inputs x and targets y\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the components of the Decoder block: \n",
    "* MultiHeadAttention\n",
    "* FeedForward Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Head(nn.Module):\n",
    "    \"\"\" one head of self-attention \"\"\"\n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,C) 16,32,16\n",
    "        q = self.query(x) # (B,T,C)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,C)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(n_embd, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FeedFoward(nn.Module):\n",
    "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd), # Projection layer going back into the residual pathway\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine components into the Decoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))    # Communication\n",
    "        x = x + self.ffwd(self.ln2(x))  # Computation\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the full Transformer model \n",
    "This is a combination of the Token embeddings, Positional embeddings, a stack of Transformer blocks and an output block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# super simple language model\n",
    "class LanguageModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be training a larger LLM on distributed resources in session 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework\n",
    "\n",
    "1. In this notebook, we learned the various components of an LLM. \n",
    "    Your homework this week is to take the mini LLM we created from scratch and run your own training loop. Show how the training and validation perplexity change over the steps.\n",
    "      \n",
    "    Hint: this function might be useful for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Train loss: 4.169633388519287, Val loss: 4.177514553070068\n",
      "Iter 10, Train loss: 3.4245123863220215, Val loss: 3.453566312789917\n",
      "Iter 20, Train loss: 3.2626430988311768, Val loss: 3.299898147583008\n",
      "Iter 30, Train loss: 3.1545820236206055, Val loss: 3.2141313552856445\n",
      "Iter 40, Train loss: 3.038402795791626, Val loss: 3.073349952697754\n",
      "Iter 50, Train loss: 2.934239387512207, Val loss: 2.9602413177490234\n",
      "Iter 60, Train loss: 2.853992223739624, Val loss: 2.8537707328796387\n",
      "Iter 70, Train loss: 2.7849130630493164, Val loss: 2.777923583984375\n",
      "Iter 80, Train loss: 2.730893850326538, Val loss: 2.72627592086792\n",
      "Iter 90, Train loss: 2.6884331703186035, Val loss: 2.6969571113586426\n",
      "Iter 100, Train loss: 2.6535654067993164, Val loss: 2.659243106842041\n",
      "Iter 110, Train loss: 2.6276681423187256, Val loss: 2.6255743503570557\n",
      "Iter 120, Train loss: 2.614443063735962, Val loss: 2.601290225982666\n",
      "Iter 130, Train loss: 2.5987021923065186, Val loss: 2.6057674884796143\n",
      "Iter 140, Train loss: 2.589689016342163, Val loss: 2.588552951812744\n",
      "Iter 150, Train loss: 2.5731537342071533, Val loss: 2.572242498397827\n",
      "Iter 160, Train loss: 2.5430383682250977, Val loss: 2.5462827682495117\n",
      "Iter 170, Train loss: 2.5436911582946777, Val loss: 2.5533552169799805\n",
      "Iter 180, Train loss: 2.5330100059509277, Val loss: 2.5366361141204834\n",
      "Iter 190, Train loss: 2.5107977390289307, Val loss: 2.520364761352539\n",
      "Iter 200, Train loss: 2.499431848526001, Val loss: 2.51531720161438\n",
      "Iter 210, Train loss: 2.497070074081421, Val loss: 2.5082032680511475\n",
      "Iter 220, Train loss: 2.494377851486206, Val loss: 2.499199867248535\n",
      "Iter 230, Train loss: 2.487844944000244, Val loss: 2.4879839420318604\n",
      "Iter 240, Train loss: 2.4881865978240967, Val loss: 2.4982874393463135\n",
      "Iter 250, Train loss: 2.464376449584961, Val loss: 2.4789867401123047\n",
      "Iter 260, Train loss: 2.4417152404785156, Val loss: 2.4550490379333496\n",
      "Iter 270, Train loss: 2.4366636276245117, Val loss: 2.4395055770874023\n",
      "Iter 280, Train loss: 2.4538474082946777, Val loss: 2.4441163539886475\n",
      "Iter 290, Train loss: 2.434349298477173, Val loss: 2.440208673477173\n",
      "Iter 300, Train loss: 2.425503730773926, Val loss: 2.434783458709717\n",
      "Iter 310, Train loss: 2.411982774734497, Val loss: 2.4207639694213867\n",
      "Iter 320, Train loss: 2.4214484691619873, Val loss: 2.4227304458618164\n",
      "Iter 330, Train loss: 2.3958523273468018, Val loss: 2.4179325103759766\n",
      "Iter 340, Train loss: 2.3916068077087402, Val loss: 2.402181625366211\n",
      "Iter 350, Train loss: 2.3908307552337646, Val loss: 2.3948235511779785\n",
      "Iter 360, Train loss: 2.3664395809173584, Val loss: 2.3851089477539062\n",
      "Iter 370, Train loss: 2.37605619430542, Val loss: 2.373162031173706\n",
      "Iter 380, Train loss: 2.363464593887329, Val loss: 2.362834930419922\n",
      "Iter 390, Train loss: 2.3485074043273926, Val loss: 2.3524091243743896\n",
      "Iter 400, Train loss: 2.3481132984161377, Val loss: 2.361520767211914\n",
      "Iter 410, Train loss: 2.341049909591675, Val loss: 2.356146812438965\n",
      "Iter 420, Train loss: 2.332533359527588, Val loss: 2.3465416431427\n",
      "Iter 430, Train loss: 2.33443284034729, Val loss: 2.3372583389282227\n",
      "Iter 440, Train loss: 2.32216739654541, Val loss: 2.3218119144439697\n",
      "Iter 450, Train loss: 2.3136446475982666, Val loss: 2.3211164474487305\n",
      "Iter 460, Train loss: 2.315068006515503, Val loss: 2.332759380340576\n",
      "Iter 470, Train loss: 2.3150839805603027, Val loss: 2.3255317211151123\n",
      "Iter 480, Train loss: 2.310006618499756, Val loss: 2.308251142501831\n",
      "Iter 490, Train loss: 2.305316925048828, Val loss: 2.303905725479126\n",
      "Iter 500, Train loss: 2.2868142127990723, Val loss: 2.3036744594573975\n",
      "Iter 510, Train loss: 2.2804152965545654, Val loss: 2.2953176498413086\n",
      "Iter 520, Train loss: 2.2706613540649414, Val loss: 2.2848920822143555\n",
      "Iter 530, Train loss: 2.284742593765259, Val loss: 2.2880942821502686\n",
      "Iter 540, Train loss: 2.2685821056365967, Val loss: 2.280618190765381\n",
      "Iter 550, Train loss: 2.2631092071533203, Val loss: 2.274852752685547\n",
      "Iter 560, Train loss: 2.255128860473633, Val loss: 2.25480580329895\n",
      "Iter 570, Train loss: 2.253988265991211, Val loss: 2.249021053314209\n",
      "Iter 580, Train loss: 2.242530107498169, Val loss: 2.2527410984039307\n",
      "Iter 590, Train loss: 2.240356922149658, Val loss: 2.2440171241760254\n",
      "Iter 600, Train loss: 2.2294199466705322, Val loss: 2.247035503387451\n",
      "Iter 610, Train loss: 2.2234396934509277, Val loss: 2.2397544384002686\n",
      "Iter 620, Train loss: 2.2273387908935547, Val loss: 2.2455410957336426\n",
      "Iter 630, Train loss: 2.221017837524414, Val loss: 2.2374989986419678\n",
      "Iter 640, Train loss: 2.201265335083008, Val loss: 2.2276434898376465\n",
      "Iter 650, Train loss: 2.2134761810302734, Val loss: 2.2308273315429688\n",
      "Iter 660, Train loss: 2.205038070678711, Val loss: 2.2277331352233887\n",
      "Iter 670, Train loss: 2.217026948928833, Val loss: 2.2293615341186523\n",
      "Iter 680, Train loss: 2.1992886066436768, Val loss: 2.219442367553711\n",
      "Iter 690, Train loss: 2.205629825592041, Val loss: 2.227278232574463\n",
      "Iter 700, Train loss: 2.187588691711426, Val loss: 2.211524248123169\n",
      "Iter 710, Train loss: 2.1860713958740234, Val loss: 2.2001190185546875\n",
      "Iter 720, Train loss: 2.1811208724975586, Val loss: 2.200089931488037\n",
      "Iter 730, Train loss: 2.1864004135131836, Val loss: 2.199897050857544\n",
      "Iter 740, Train loss: 2.1743929386138916, Val loss: 2.209346294403076\n",
      "Iter 750, Train loss: 2.169050455093384, Val loss: 2.193387031555176\n",
      "Iter 760, Train loss: 2.1603074073791504, Val loss: 2.193505048751831\n",
      "Iter 770, Train loss: 2.158897638320923, Val loss: 2.1867775917053223\n",
      "Iter 780, Train loss: 2.157554864883423, Val loss: 2.187440872192383\n",
      "Iter 790, Train loss: 2.1618716716766357, Val loss: 2.199328660964966\n",
      "Iter 800, Train loss: 2.1538758277893066, Val loss: 2.1780898571014404\n",
      "Iter 810, Train loss: 2.1469085216522217, Val loss: 2.171938419342041\n",
      "Iter 820, Train loss: 2.145146369934082, Val loss: 2.1693925857543945\n",
      "Iter 830, Train loss: 2.1411561965942383, Val loss: 2.154296636581421\n",
      "Iter 840, Train loss: 2.1308746337890625, Val loss: 2.1672005653381348\n",
      "Iter 850, Train loss: 2.1329121589660645, Val loss: 2.155864953994751\n",
      "Iter 860, Train loss: 2.128237009048462, Val loss: 2.15863037109375\n",
      "Iter 870, Train loss: 2.1202759742736816, Val loss: 2.160519599914551\n",
      "Iter 880, Train loss: 2.1194188594818115, Val loss: 2.1603596210479736\n",
      "Iter 890, Train loss: 2.1126275062561035, Val loss: 2.1262691020965576\n",
      "Iter 900, Train loss: 2.1161704063415527, Val loss: 2.164776086807251\n",
      "Iter 910, Train loss: 2.1251590251922607, Val loss: 2.166365385055542\n",
      "Iter 920, Train loss: 2.1074535846710205, Val loss: 2.1440961360931396\n",
      "Iter 930, Train loss: 2.1094255447387695, Val loss: 2.1422033309936523\n",
      "Iter 940, Train loss: 2.111196279525757, Val loss: 2.1437947750091553\n",
      "Iter 950, Train loss: 2.093161106109619, Val loss: 2.134972333908081\n",
      "Iter 960, Train loss: 2.091527223587036, Val loss: 2.12351131439209\n",
      "Iter 970, Train loss: 2.091519832611084, Val loss: 2.1285858154296875\n",
      "Iter 980, Train loss: 2.087111473083496, Val loss: 2.127648115158081\n",
      "Iter 990, Train loss: 2.0824174880981445, Val loss: 2.122833251953125\n",
      "Iter 1000, Train loss: 2.091339349746704, Val loss: 2.1172738075256348\n",
      "Iter 1010, Train loss: 2.0773019790649414, Val loss: 2.1045708656311035\n",
      "Iter 1020, Train loss: 2.0727415084838867, Val loss: 2.113410711288452\n",
      "Iter 1030, Train loss: 2.071693181991577, Val loss: 2.1056056022644043\n",
      "Iter 1040, Train loss: 2.0770018100738525, Val loss: 2.131290912628174\n",
      "Iter 1050, Train loss: 2.0695672035217285, Val loss: 2.115046501159668\n",
      "Iter 1060, Train loss: 2.0691428184509277, Val loss: 2.117367744445801\n",
      "Iter 1070, Train loss: 2.0884058475494385, Val loss: 2.1291589736938477\n",
      "Iter 1080, Train loss: 2.077691078186035, Val loss: 2.131416082382202\n",
      "Iter 1090, Train loss: 2.0571601390838623, Val loss: 2.09968638420105\n",
      "Iter 1100, Train loss: 2.060215950012207, Val loss: 2.104218006134033\n",
      "Iter 1110, Train loss: 2.0503270626068115, Val loss: 2.0934054851531982\n",
      "Iter 1120, Train loss: 2.046232223510742, Val loss: 2.0956294536590576\n",
      "Iter 1130, Train loss: 2.057757616043091, Val loss: 2.0961344242095947\n",
      "Iter 1140, Train loss: 2.048138380050659, Val loss: 2.094597578048706\n",
      "Iter 1150, Train loss: 2.0464344024658203, Val loss: 2.0850799083709717\n",
      "Iter 1160, Train loss: 2.039862632751465, Val loss: 2.0837180614471436\n",
      "Iter 1170, Train loss: 2.041851282119751, Val loss: 2.0787947177886963\n",
      "Iter 1180, Train loss: 2.0302951335906982, Val loss: 2.0799129009246826\n",
      "Iter 1190, Train loss: 2.027632713317871, Val loss: 2.074864625930786\n",
      "Iter 1200, Train loss: 2.028000593185425, Val loss: 2.0743134021759033\n",
      "Iter 1210, Train loss: 2.0279481410980225, Val loss: 2.0847785472869873\n",
      "Iter 1220, Train loss: 2.0305745601654053, Val loss: 2.0801899433135986\n",
      "Iter 1230, Train loss: 2.040091037750244, Val loss: 2.094235420227051\n",
      "Iter 1240, Train loss: 2.0290610790252686, Val loss: 2.07586669921875\n",
      "Iter 1250, Train loss: 2.013993263244629, Val loss: 2.0686380863189697\n",
      "Iter 1260, Train loss: 2.019193649291992, Val loss: 2.077012062072754\n",
      "Iter 1270, Train loss: 2.0092294216156006, Val loss: 2.05411434173584\n",
      "Iter 1280, Train loss: 2.015700101852417, Val loss: 2.0757956504821777\n",
      "Iter 1290, Train loss: 2.0126535892486572, Val loss: 2.078840732574463\n",
      "Iter 1300, Train loss: 2.0097780227661133, Val loss: 2.0663998126983643\n",
      "Iter 1310, Train loss: 2.015281915664673, Val loss: 2.0611672401428223\n",
      "Iter 1320, Train loss: 1.9948248863220215, Val loss: 2.0546681880950928\n",
      "Iter 1330, Train loss: 2.004894971847534, Val loss: 2.05452823638916\n",
      "Iter 1340, Train loss: 2.009669542312622, Val loss: 2.077315092086792\n",
      "Iter 1350, Train loss: 2.0039780139923096, Val loss: 2.0742454528808594\n",
      "Iter 1360, Train loss: 1.9931824207305908, Val loss: 2.0525527000427246\n",
      "Iter 1370, Train loss: 1.9891241788864136, Val loss: 2.0447134971618652\n",
      "Iter 1380, Train loss: 1.9922823905944824, Val loss: 2.0600156784057617\n",
      "Iter 1390, Train loss: 1.987643837928772, Val loss: 2.0456180572509766\n",
      "Iter 1400, Train loss: 1.990278959274292, Val loss: 2.047508955001831\n",
      "Iter 1410, Train loss: 1.9884300231933594, Val loss: 2.046415328979492\n",
      "Iter 1420, Train loss: 1.9785025119781494, Val loss: 2.050645351409912\n",
      "Iter 1430, Train loss: 1.9799364805221558, Val loss: 2.0355074405670166\n",
      "Iter 1440, Train loss: 1.988346815109253, Val loss: 2.0293045043945312\n",
      "Iter 1450, Train loss: 1.967037558555603, Val loss: 2.0321919918060303\n",
      "Iter 1460, Train loss: 1.9696911573410034, Val loss: 2.0250704288482666\n",
      "Iter 1470, Train loss: 1.9736404418945312, Val loss: 2.0242836475372314\n",
      "Iter 1480, Train loss: 1.9645205736160278, Val loss: 2.0197107791900635\n",
      "Iter 1490, Train loss: 1.959547758102417, Val loss: 2.0428595542907715\n",
      "Iter 1500, Train loss: 1.967010498046875, Val loss: 2.0351033210754395\n",
      "Iter 1510, Train loss: 1.9666796922683716, Val loss: 2.0222654342651367\n",
      "Iter 1520, Train loss: 1.9657602310180664, Val loss: 2.021078109741211\n",
      "Iter 1530, Train loss: 1.9639997482299805, Val loss: 2.0275940895080566\n",
      "Iter 1540, Train loss: 1.958868145942688, Val loss: 2.022732734680176\n",
      "Iter 1550, Train loss: 1.964117169380188, Val loss: 2.0390334129333496\n",
      "Iter 1560, Train loss: 1.940630316734314, Val loss: 2.025449275970459\n",
      "Iter 1570, Train loss: 1.954392671585083, Val loss: 2.014585256576538\n",
      "Iter 1580, Train loss: 1.944047451019287, Val loss: 2.0185658931732178\n",
      "Iter 1590, Train loss: 1.9520094394683838, Val loss: 2.0289716720581055\n",
      "Iter 1600, Train loss: 1.9606226682662964, Val loss: 2.0391931533813477\n",
      "Iter 1610, Train loss: 1.9438165426254272, Val loss: 2.025134801864624\n",
      "Iter 1620, Train loss: 1.9433189630508423, Val loss: 2.0403101444244385\n",
      "Iter 1630, Train loss: 1.9433658123016357, Val loss: 2.020508289337158\n",
      "Iter 1640, Train loss: 1.942024827003479, Val loss: 2.018967866897583\n",
      "Iter 1650, Train loss: 1.9324835538864136, Val loss: 2.021775007247925\n",
      "Iter 1660, Train loss: 1.938613772392273, Val loss: 2.019132375717163\n",
      "Iter 1670, Train loss: 1.9379982948303223, Val loss: 2.0171396732330322\n",
      "Iter 1680, Train loss: 1.9447014331817627, Val loss: 2.0140979290008545\n",
      "Iter 1690, Train loss: 1.9304765462875366, Val loss: 2.0085787773132324\n",
      "Iter 1700, Train loss: 1.9240951538085938, Val loss: 2.0035386085510254\n",
      "Iter 1710, Train loss: 1.9208426475524902, Val loss: 2.00675106048584\n",
      "Iter 1720, Train loss: 1.9347665309906006, Val loss: 2.010103464126587\n",
      "Iter 1730, Train loss: 1.9315680265426636, Val loss: 2.002741575241089\n",
      "Iter 1740, Train loss: 1.923194169998169, Val loss: 2.013329029083252\n",
      "Iter 1750, Train loss: 1.9263849258422852, Val loss: 2.0098342895507812\n",
      "Iter 1760, Train loss: 1.9240927696228027, Val loss: 1.998563528060913\n",
      "Iter 1770, Train loss: 1.9164232015609741, Val loss: 2.0060224533081055\n",
      "Iter 1780, Train loss: 1.9151573181152344, Val loss: 2.0064713954925537\n",
      "Iter 1790, Train loss: 1.906829833984375, Val loss: 2.010126829147339\n",
      "Iter 1800, Train loss: 1.9168907403945923, Val loss: 2.006983757019043\n",
      "Iter 1810, Train loss: 1.9183577299118042, Val loss: 2.0024144649505615\n",
      "Iter 1820, Train loss: 1.9037065505981445, Val loss: 1.997789740562439\n",
      "Iter 1830, Train loss: 1.9012395143508911, Val loss: 2.00954532623291\n",
      "Iter 1840, Train loss: 1.9150307178497314, Val loss: 2.001920461654663\n",
      "Iter 1850, Train loss: 1.8945977687835693, Val loss: 1.9881497621536255\n",
      "Iter 1860, Train loss: 1.8962324857711792, Val loss: 1.9856802225112915\n",
      "Iter 1870, Train loss: 1.9026074409484863, Val loss: 1.9879481792449951\n",
      "Iter 1880, Train loss: 1.8864827156066895, Val loss: 1.988760232925415\n",
      "Iter 1890, Train loss: 1.8997321128845215, Val loss: 1.9898698329925537\n",
      "Iter 1900, Train loss: 1.8986698389053345, Val loss: 1.9829035997390747\n",
      "Iter 1910, Train loss: 1.909153938293457, Val loss: 2.003281593322754\n",
      "Iter 1920, Train loss: 1.8858836889266968, Val loss: 1.9954370260238647\n",
      "Iter 1930, Train loss: 1.885451316833496, Val loss: 1.9784016609191895\n",
      "Iter 1940, Train loss: 1.8759149312973022, Val loss: 1.9750933647155762\n",
      "Iter 1950, Train loss: 1.881729245185852, Val loss: 1.9768754243850708\n",
      "Iter 1960, Train loss: 1.8892207145690918, Val loss: 1.9967617988586426\n",
      "Iter 1970, Train loss: 1.8673770427703857, Val loss: 1.9687469005584717\n",
      "Iter 1980, Train loss: 1.8827922344207764, Val loss: 1.9781715869903564\n",
      "Iter 1990, Train loss: 1.8736838102340698, Val loss: 1.9689199924468994\n",
      "Iter 2000, Train loss: 1.8733124732971191, Val loss: 1.971785306930542\n",
      "Iter 2010, Train loss: 1.8783401250839233, Val loss: 1.9640965461730957\n",
      "Iter 2020, Train loss: 1.8799967765808105, Val loss: 1.965975284576416\n",
      "Iter 2030, Train loss: 1.8728703260421753, Val loss: 1.9818682670593262\n",
      "Iter 2040, Train loss: 1.861541748046875, Val loss: 1.9640239477157593\n",
      "Iter 2050, Train loss: 1.8653546571731567, Val loss: 1.9607748985290527\n",
      "Iter 2060, Train loss: 1.8628332614898682, Val loss: 1.974130392074585\n",
      "Iter 2070, Train loss: 1.8699219226837158, Val loss: 1.9762780666351318\n",
      "Iter 2080, Train loss: 1.8608354330062866, Val loss: 1.9692327976226807\n",
      "Iter 2090, Train loss: 1.8756636381149292, Val loss: 1.9715118408203125\n",
      "Iter 2100, Train loss: 1.8706480264663696, Val loss: 1.965427279472351\n",
      "Iter 2110, Train loss: 1.8741728067398071, Val loss: 1.972060203552246\n",
      "Iter 2120, Train loss: 1.8675919771194458, Val loss: 1.9726028442382812\n",
      "Iter 2130, Train loss: 1.8695356845855713, Val loss: 1.96278977394104\n",
      "Iter 2140, Train loss: 1.862787127494812, Val loss: 1.9621142148971558\n",
      "Iter 2150, Train loss: 1.8631690740585327, Val loss: 1.9658255577087402\n",
      "Iter 2160, Train loss: 1.8560621738433838, Val loss: 1.9732344150543213\n",
      "Iter 2170, Train loss: 1.8581018447875977, Val loss: 1.970800518989563\n",
      "Iter 2180, Train loss: 1.8602149486541748, Val loss: 1.960994839668274\n",
      "Iter 2190, Train loss: 1.8545728921890259, Val loss: 1.961293339729309\n",
      "Iter 2200, Train loss: 1.860966682434082, Val loss: 1.970552921295166\n",
      "Iter 2210, Train loss: 1.8506722450256348, Val loss: 1.9716016054153442\n",
      "Iter 2220, Train loss: 1.8634179830551147, Val loss: 1.960749864578247\n",
      "Iter 2230, Train loss: 1.8439733982086182, Val loss: 1.9670042991638184\n",
      "Iter 2240, Train loss: 1.8527321815490723, Val loss: 1.9548194408416748\n",
      "Iter 2250, Train loss: 1.8405828475952148, Val loss: 1.958322286605835\n",
      "Iter 2260, Train loss: 1.8480228185653687, Val loss: 1.9460464715957642\n",
      "Iter 2270, Train loss: 1.8412060737609863, Val loss: 1.9521970748901367\n",
      "Iter 2280, Train loss: 1.8531479835510254, Val loss: 1.9604867696762085\n",
      "Iter 2290, Train loss: 1.8583945035934448, Val loss: 1.9629151821136475\n",
      "Iter 2300, Train loss: 1.8381314277648926, Val loss: 1.9410464763641357\n",
      "Iter 2310, Train loss: 1.8538209199905396, Val loss: 1.953708529472351\n",
      "Iter 2320, Train loss: 1.8263633251190186, Val loss: 1.941523790359497\n",
      "Iter 2330, Train loss: 1.8258697986602783, Val loss: 1.9411996603012085\n",
      "Iter 2340, Train loss: 1.8364659547805786, Val loss: 1.947571873664856\n",
      "Iter 2350, Train loss: 1.8324981927871704, Val loss: 1.9567153453826904\n",
      "Iter 2360, Train loss: 1.8376975059509277, Val loss: 1.9543142318725586\n",
      "Iter 2370, Train loss: 1.8369680643081665, Val loss: 1.95027756690979\n",
      "Iter 2380, Train loss: 1.8317996263504028, Val loss: 1.9564779996871948\n",
      "Iter 2390, Train loss: 1.8309739828109741, Val loss: 1.9449092149734497\n",
      "Iter 2400, Train loss: 1.8251498937606812, Val loss: 1.96028470993042\n",
      "Iter 2410, Train loss: 1.8309756517410278, Val loss: 1.9457571506500244\n",
      "Iter 2420, Train loss: 1.833849310874939, Val loss: 1.9653064012527466\n",
      "Iter 2430, Train loss: 1.8211880922317505, Val loss: 1.949412226676941\n",
      "Iter 2440, Train loss: 1.8215575218200684, Val loss: 1.9322915077209473\n",
      "Iter 2450, Train loss: 1.8125399351119995, Val loss: 1.9419865608215332\n",
      "Iter 2460, Train loss: 1.815192699432373, Val loss: 1.9502314329147339\n",
      "Iter 2470, Train loss: 1.8249131441116333, Val loss: 1.943326473236084\n",
      "Iter 2480, Train loss: 1.8222023248672485, Val loss: 1.9405467510223389\n",
      "Iter 2490, Train loss: 1.8178249597549438, Val loss: 1.9414900541305542\n",
      "Iter 2500, Train loss: 1.8169959783554077, Val loss: 1.9382246732711792\n",
      "Iter 2510, Train loss: 1.818127155303955, Val loss: 1.9448095560073853\n",
      "Iter 2520, Train loss: 1.8051788806915283, Val loss: 1.9290012121200562\n",
      "Iter 2530, Train loss: 1.809839129447937, Val loss: 1.9329584836959839\n",
      "Iter 2540, Train loss: 1.8231091499328613, Val loss: 1.9481889009475708\n",
      "Iter 2550, Train loss: 1.8031991720199585, Val loss: 1.9449752569198608\n",
      "Iter 2560, Train loss: 1.808785080909729, Val loss: 1.9464366436004639\n",
      "Iter 2570, Train loss: 1.8124215602874756, Val loss: 1.9454994201660156\n",
      "Iter 2580, Train loss: 1.8212594985961914, Val loss: 1.942287564277649\n",
      "Iter 2590, Train loss: 1.807003140449524, Val loss: 1.9383195638656616\n",
      "Iter 2600, Train loss: 1.8068757057189941, Val loss: 1.9451017379760742\n",
      "Iter 2610, Train loss: 1.8096706867218018, Val loss: 1.9377622604370117\n",
      "Iter 2620, Train loss: 1.7955520153045654, Val loss: 1.927728295326233\n",
      "Iter 2630, Train loss: 1.8002012968063354, Val loss: 1.9325469732284546\n",
      "Iter 2640, Train loss: 1.8174606561660767, Val loss: 1.9414886236190796\n",
      "Iter 2650, Train loss: 1.8067089319229126, Val loss: 1.9303712844848633\n",
      "Iter 2660, Train loss: 1.802588939666748, Val loss: 1.9386885166168213\n",
      "Iter 2670, Train loss: 1.7833153009414673, Val loss: 1.9222697019577026\n",
      "Iter 2680, Train loss: 1.799871563911438, Val loss: 1.9329053163528442\n",
      "Iter 2690, Train loss: 1.8023699522018433, Val loss: 1.9302242994308472\n",
      "Iter 2700, Train loss: 1.8031563758850098, Val loss: 1.9305766820907593\n",
      "Iter 2710, Train loss: 1.7967950105667114, Val loss: 1.9351905584335327\n",
      "Iter 2720, Train loss: 1.8102443218231201, Val loss: 1.9350868463516235\n",
      "Iter 2730, Train loss: 1.78802490234375, Val loss: 1.9197468757629395\n",
      "Iter 2740, Train loss: 1.793362021446228, Val loss: 1.9114937782287598\n",
      "Iter 2750, Train loss: 1.804070234298706, Val loss: 1.917573094367981\n",
      "Iter 2760, Train loss: 1.7924270629882812, Val loss: 1.9391560554504395\n",
      "Iter 2770, Train loss: 1.7901767492294312, Val loss: 1.9059607982635498\n",
      "Iter 2780, Train loss: 1.7955337762832642, Val loss: 1.9097785949707031\n",
      "Iter 2790, Train loss: 1.7820930480957031, Val loss: 1.8975719213485718\n",
      "Iter 2800, Train loss: 1.7874178886413574, Val loss: 1.9065132141113281\n",
      "Iter 2810, Train loss: 1.7900664806365967, Val loss: 1.906449556350708\n",
      "Iter 2820, Train loss: 1.7822598218917847, Val loss: 1.908459186553955\n",
      "Iter 2830, Train loss: 1.7828336954116821, Val loss: 1.9048173427581787\n",
      "Iter 2840, Train loss: 1.784996747970581, Val loss: 1.9243465662002563\n",
      "Iter 2850, Train loss: 1.7792264223098755, Val loss: 1.9171441793441772\n",
      "Iter 2860, Train loss: 1.7771776914596558, Val loss: 1.908734917640686\n",
      "Iter 2870, Train loss: 1.7781836986541748, Val loss: 1.9124218225479126\n",
      "Iter 2880, Train loss: 1.7705916166305542, Val loss: 1.9032193422317505\n",
      "Iter 2890, Train loss: 1.7851680517196655, Val loss: 1.9168179035186768\n",
      "Iter 2900, Train loss: 1.7763615846633911, Val loss: 1.9160783290863037\n",
      "Iter 2910, Train loss: 1.7797062397003174, Val loss: 1.9144110679626465\n",
      "Iter 2920, Train loss: 1.7895724773406982, Val loss: 1.9316030740737915\n",
      "Iter 2930, Train loss: 1.7697834968566895, Val loss: 1.9214242696762085\n",
      "Iter 2940, Train loss: 1.7656828165054321, Val loss: 1.9159274101257324\n",
      "Iter 2950, Train loss: 1.766959547996521, Val loss: 1.9119689464569092\n",
      "Iter 2960, Train loss: 1.7773566246032715, Val loss: 1.9082194566726685\n",
      "Iter 2970, Train loss: 1.770279049873352, Val loss: 1.8988969326019287\n",
      "Iter 2980, Train loss: 1.781161904335022, Val loss: 1.9070852994918823\n",
      "Iter 2990, Train loss: 1.772263765335083, Val loss: 1.9040603637695312\n",
      "Iter 3000, Train loss: 1.7639166116714478, Val loss: 1.9052571058273315\n",
      "Iter 3010, Train loss: 1.7620333433151245, Val loss: 1.905247449874878\n",
      "Iter 3020, Train loss: 1.7676117420196533, Val loss: 1.9124752283096313\n",
      "Iter 3030, Train loss: 1.7726202011108398, Val loss: 1.9072494506835938\n",
      "Iter 3040, Train loss: 1.7742561101913452, Val loss: 1.9114435911178589\n",
      "Iter 3050, Train loss: 1.7654595375061035, Val loss: 1.8959773778915405\n",
      "Iter 3060, Train loss: 1.767541766166687, Val loss: 1.9032059907913208\n",
      "Iter 3070, Train loss: 1.7700145244598389, Val loss: 1.893383502960205\n",
      "Iter 3080, Train loss: 1.752793312072754, Val loss: 1.8979417085647583\n",
      "Iter 3090, Train loss: 1.7509933710098267, Val loss: 1.8857847452163696\n",
      "Iter 3100, Train loss: 1.757156252861023, Val loss: 1.8830676078796387\n",
      "Iter 3110, Train loss: 1.762812614440918, Val loss: 1.8723840713500977\n",
      "Iter 3120, Train loss: 1.7570229768753052, Val loss: 1.8828541040420532\n",
      "Iter 3130, Train loss: 1.7658151388168335, Val loss: 1.8774504661560059\n",
      "Iter 3140, Train loss: 1.7548346519470215, Val loss: 1.8872497081756592\n",
      "Iter 3150, Train loss: 1.7610762119293213, Val loss: 1.8939518928527832\n",
      "Iter 3160, Train loss: 1.7539314031600952, Val loss: 1.8844681978225708\n",
      "Iter 3170, Train loss: 1.7578784227371216, Val loss: 1.8904626369476318\n",
      "Iter 3180, Train loss: 1.7601351737976074, Val loss: 1.888837456703186\n",
      "Iter 3190, Train loss: 1.7464358806610107, Val loss: 1.8864768743515015\n",
      "Iter 3200, Train loss: 1.7486838102340698, Val loss: 1.882258415222168\n",
      "Iter 3210, Train loss: 1.74235200881958, Val loss: 1.8869714736938477\n",
      "Iter 3220, Train loss: 1.7596731185913086, Val loss: 1.892886757850647\n",
      "Iter 3230, Train loss: 1.7633253335952759, Val loss: 1.8911902904510498\n",
      "Iter 3240, Train loss: 1.756522536277771, Val loss: 1.8855122327804565\n",
      "Iter 3250, Train loss: 1.7530783414840698, Val loss: 1.8821208477020264\n",
      "Iter 3260, Train loss: 1.7567204236984253, Val loss: 1.889244794845581\n",
      "Iter 3270, Train loss: 1.7490140199661255, Val loss: 1.8985600471496582\n",
      "Iter 3280, Train loss: 1.7485907077789307, Val loss: 1.8768913745880127\n",
      "Iter 3290, Train loss: 1.7348427772521973, Val loss: 1.8789887428283691\n",
      "Iter 3300, Train loss: 1.745581030845642, Val loss: 1.890425682067871\n",
      "Iter 3310, Train loss: 1.7484254837036133, Val loss: 1.8801754713058472\n",
      "Iter 3320, Train loss: 1.7562217712402344, Val loss: 1.8859951496124268\n",
      "Iter 3330, Train loss: 1.7531336545944214, Val loss: 1.8847423791885376\n",
      "Iter 3340, Train loss: 1.7391531467437744, Val loss: 1.8707456588745117\n",
      "Iter 3350, Train loss: 1.742254614830017, Val loss: 1.8798414468765259\n",
      "Iter 3360, Train loss: 1.7458993196487427, Val loss: 1.8779672384262085\n",
      "Iter 3370, Train loss: 1.7534440755844116, Val loss: 1.8694418668746948\n",
      "Iter 3380, Train loss: 1.747849702835083, Val loss: 1.8717554807662964\n",
      "Iter 3390, Train loss: 1.7360671758651733, Val loss: 1.8715221881866455\n",
      "Iter 3400, Train loss: 1.7395150661468506, Val loss: 1.8808633089065552\n",
      "Iter 3410, Train loss: 1.738062858581543, Val loss: 1.8754876852035522\n",
      "Iter 3420, Train loss: 1.729310154914856, Val loss: 1.8744802474975586\n",
      "Iter 3430, Train loss: 1.7327489852905273, Val loss: 1.8644037246704102\n",
      "Iter 3440, Train loss: 1.7381805181503296, Val loss: 1.886085033416748\n",
      "Iter 3450, Train loss: 1.7325352430343628, Val loss: 1.8624571561813354\n",
      "Iter 3460, Train loss: 1.7388436794281006, Val loss: 1.8724136352539062\n",
      "Iter 3470, Train loss: 1.7387571334838867, Val loss: 1.8870846033096313\n",
      "Iter 3480, Train loss: 1.734337329864502, Val loss: 1.8784676790237427\n",
      "Iter 3490, Train loss: 1.7376627922058105, Val loss: 1.8776670694351196\n",
      "Iter 3500, Train loss: 1.730262041091919, Val loss: 1.8704442977905273\n",
      "Iter 3510, Train loss: 1.7390265464782715, Val loss: 1.8723922967910767\n",
      "Iter 3520, Train loss: 1.7385222911834717, Val loss: 1.8664031028747559\n",
      "Iter 3530, Train loss: 1.7305127382278442, Val loss: 1.8581740856170654\n",
      "Iter 3540, Train loss: 1.7267329692840576, Val loss: 1.8580338954925537\n",
      "Iter 3550, Train loss: 1.7278921604156494, Val loss: 1.8577125072479248\n",
      "Iter 3560, Train loss: 1.7269343137741089, Val loss: 1.874769687652588\n",
      "Iter 3570, Train loss: 1.7255151271820068, Val loss: 1.8615174293518066\n",
      "Iter 3580, Train loss: 1.7273505926132202, Val loss: 1.8688011169433594\n",
      "Iter 3590, Train loss: 1.7101502418518066, Val loss: 1.8677140474319458\n",
      "Iter 3600, Train loss: 1.7253799438476562, Val loss: 1.8650846481323242\n",
      "Iter 3610, Train loss: 1.7341346740722656, Val loss: 1.8865824937820435\n",
      "Iter 3620, Train loss: 1.7241979837417603, Val loss: 1.87501859664917\n",
      "Iter 3630, Train loss: 1.7235870361328125, Val loss: 1.8706876039505005\n",
      "Iter 3640, Train loss: 1.725702166557312, Val loss: 1.8657270669937134\n",
      "Iter 3650, Train loss: 1.721560001373291, Val loss: 1.8648681640625\n",
      "Iter 3660, Train loss: 1.7213116884231567, Val loss: 1.880092740058899\n",
      "Iter 3670, Train loss: 1.7248506546020508, Val loss: 1.8694603443145752\n",
      "Iter 3680, Train loss: 1.7185935974121094, Val loss: 1.8666961193084717\n",
      "Iter 3690, Train loss: 1.7299795150756836, Val loss: 1.8712977170944214\n",
      "Iter 3700, Train loss: 1.7364020347595215, Val loss: 1.8667792081832886\n",
      "Iter 3710, Train loss: 1.7086185216903687, Val loss: 1.870521903038025\n",
      "Iter 3720, Train loss: 1.7252223491668701, Val loss: 1.8677384853363037\n",
      "Iter 3730, Train loss: 1.709418535232544, Val loss: 1.864190697669983\n",
      "Iter 3740, Train loss: 1.71306574344635, Val loss: 1.862898349761963\n",
      "Iter 3750, Train loss: 1.7211897373199463, Val loss: 1.874427318572998\n",
      "Iter 3760, Train loss: 1.7040798664093018, Val loss: 1.864535927772522\n",
      "Iter 3770, Train loss: 1.7190756797790527, Val loss: 1.8731051683425903\n",
      "Iter 3780, Train loss: 1.7106492519378662, Val loss: 1.8678526878356934\n",
      "Iter 3790, Train loss: 1.7068945169448853, Val loss: 1.8662687540054321\n",
      "Iter 3800, Train loss: 1.7171080112457275, Val loss: 1.8606027364730835\n",
      "Iter 3810, Train loss: 1.7203822135925293, Val loss: 1.8571193218231201\n",
      "Iter 3820, Train loss: 1.7180659770965576, Val loss: 1.8726190328598022\n",
      "Iter 3830, Train loss: 1.715036153793335, Val loss: 1.874996304512024\n",
      "Iter 3840, Train loss: 1.69960355758667, Val loss: 1.8537238836288452\n",
      "Iter 3850, Train loss: 1.7098444700241089, Val loss: 1.8506375551223755\n",
      "Iter 3860, Train loss: 1.7097550630569458, Val loss: 1.855308175086975\n",
      "Iter 3870, Train loss: 1.7162362337112427, Val loss: 1.8531635999679565\n",
      "Iter 3880, Train loss: 1.710390567779541, Val loss: 1.8681026697158813\n",
      "Iter 3890, Train loss: 1.7062811851501465, Val loss: 1.8553258180618286\n",
      "Iter 3900, Train loss: 1.7111258506774902, Val loss: 1.8521337509155273\n",
      "Iter 3910, Train loss: 1.707158088684082, Val loss: 1.8629302978515625\n",
      "Iter 3920, Train loss: 1.7084548473358154, Val loss: 1.8526214361190796\n",
      "Iter 3930, Train loss: 1.7047125101089478, Val loss: 1.8549127578735352\n",
      "Iter 3940, Train loss: 1.703240990638733, Val loss: 1.870966911315918\n",
      "Iter 3950, Train loss: 1.7024853229522705, Val loss: 1.8556534051895142\n",
      "Iter 3960, Train loss: 1.712540864944458, Val loss: 1.8404749631881714\n",
      "Iter 3970, Train loss: 1.7101188898086548, Val loss: 1.8562593460083008\n",
      "Iter 3980, Train loss: 1.6980072259902954, Val loss: 1.8424464464187622\n",
      "Iter 3990, Train loss: 1.700819492340088, Val loss: 1.8430250883102417\n",
      "Iter 4000, Train loss: 1.704901099205017, Val loss: 1.853416919708252\n",
      "Iter 4010, Train loss: 1.7113853693008423, Val loss: 1.8549684286117554\n",
      "Iter 4020, Train loss: 1.7111647129058838, Val loss: 1.867398977279663\n",
      "Iter 4030, Train loss: 1.7053539752960205, Val loss: 1.8420014381408691\n",
      "Iter 4040, Train loss: 1.7045013904571533, Val loss: 1.8526618480682373\n",
      "Iter 4050, Train loss: 1.7005342245101929, Val loss: 1.8446696996688843\n",
      "Iter 4060, Train loss: 1.7002055644989014, Val loss: 1.8544225692749023\n",
      "Iter 4070, Train loss: 1.6993210315704346, Val loss: 1.8561103343963623\n",
      "Iter 4080, Train loss: 1.6991205215454102, Val loss: 1.8579261302947998\n",
      "Iter 4090, Train loss: 1.7028257846832275, Val loss: 1.8522841930389404\n",
      "Iter 4100, Train loss: 1.6803131103515625, Val loss: 1.8411438465118408\n",
      "Iter 4110, Train loss: 1.7008932828903198, Val loss: 1.8497614860534668\n",
      "Iter 4120, Train loss: 1.6945245265960693, Val loss: 1.8440316915512085\n",
      "Iter 4130, Train loss: 1.6879521608352661, Val loss: 1.8615007400512695\n",
      "Iter 4140, Train loss: 1.691908359527588, Val loss: 1.846132755279541\n",
      "Iter 4150, Train loss: 1.6920527219772339, Val loss: 1.8463783264160156\n",
      "Iter 4160, Train loss: 1.6952617168426514, Val loss: 1.8533238172531128\n",
      "Iter 4170, Train loss: 1.6911407709121704, Val loss: 1.854751467704773\n",
      "Iter 4180, Train loss: 1.7027029991149902, Val loss: 1.8556146621704102\n",
      "Iter 4190, Train loss: 1.7020089626312256, Val loss: 1.8442426919937134\n",
      "Iter 4200, Train loss: 1.699791431427002, Val loss: 1.8592193126678467\n",
      "Iter 4210, Train loss: 1.6913402080535889, Val loss: 1.8458231687545776\n",
      "Iter 4220, Train loss: 1.685766339302063, Val loss: 1.8332792520523071\n",
      "Iter 4230, Train loss: 1.6972392797470093, Val loss: 1.84625244140625\n",
      "Iter 4240, Train loss: 1.681270718574524, Val loss: 1.8341418504714966\n",
      "Iter 4250, Train loss: 1.6918407678604126, Val loss: 1.8373565673828125\n",
      "Iter 4260, Train loss: 1.6961649656295776, Val loss: 1.836714506149292\n",
      "Iter 4270, Train loss: 1.694023847579956, Val loss: 1.8355282545089722\n",
      "Iter 4280, Train loss: 1.6900840997695923, Val loss: 1.8316973447799683\n",
      "Iter 4290, Train loss: 1.6913938522338867, Val loss: 1.8401374816894531\n",
      "Iter 4300, Train loss: 1.6881616115570068, Val loss: 1.84385347366333\n",
      "Iter 4310, Train loss: 1.691336989402771, Val loss: 1.8352291584014893\n",
      "Iter 4320, Train loss: 1.6889123916625977, Val loss: 1.832277774810791\n",
      "Iter 4330, Train loss: 1.6897921562194824, Val loss: 1.8411287069320679\n",
      "Iter 4340, Train loss: 1.6872488260269165, Val loss: 1.829002022743225\n",
      "Iter 4350, Train loss: 1.6890300512313843, Val loss: 1.8406404256820679\n",
      "Iter 4360, Train loss: 1.683457851409912, Val loss: 1.8361314535140991\n",
      "Iter 4370, Train loss: 1.6811022758483887, Val loss: 1.8326976299285889\n",
      "Iter 4380, Train loss: 1.6790306568145752, Val loss: 1.8239600658416748\n",
      "Iter 4390, Train loss: 1.688501000404358, Val loss: 1.8354650735855103\n",
      "Iter 4400, Train loss: 1.695199728012085, Val loss: 1.844035029411316\n",
      "Iter 4410, Train loss: 1.6951942443847656, Val loss: 1.8417166471481323\n",
      "Iter 4420, Train loss: 1.6898574829101562, Val loss: 1.853850245475769\n",
      "Iter 4430, Train loss: 1.6916760206222534, Val loss: 1.8391097784042358\n",
      "Iter 4440, Train loss: 1.6915271282196045, Val loss: 1.8445427417755127\n",
      "Iter 4450, Train loss: 1.6919611692428589, Val loss: 1.8409948348999023\n",
      "Iter 4460, Train loss: 1.6972472667694092, Val loss: 1.8455864191055298\n",
      "Iter 4470, Train loss: 1.674918532371521, Val loss: 1.8242268562316895\n",
      "Iter 4480, Train loss: 1.6789426803588867, Val loss: 1.8219212293624878\n",
      "Iter 4490, Train loss: 1.6702418327331543, Val loss: 1.8391402959823608\n",
      "Iter 4500, Train loss: 1.6837295293807983, Val loss: 1.8254817724227905\n",
      "Iter 4510, Train loss: 1.6804240942001343, Val loss: 1.830033302307129\n",
      "Iter 4520, Train loss: 1.6852242946624756, Val loss: 1.821718454360962\n",
      "Iter 4530, Train loss: 1.6794841289520264, Val loss: 1.824463963508606\n",
      "Iter 4540, Train loss: 1.6755801439285278, Val loss: 1.8306159973144531\n",
      "Iter 4550, Train loss: 1.6817289590835571, Val loss: 1.8298439979553223\n",
      "Iter 4560, Train loss: 1.6850335597991943, Val loss: 1.8455804586410522\n",
      "Iter 4570, Train loss: 1.681512713432312, Val loss: 1.8342801332473755\n",
      "Iter 4580, Train loss: 1.67573881149292, Val loss: 1.8274112939834595\n",
      "Iter 4590, Train loss: 1.670052170753479, Val loss: 1.8195117712020874\n",
      "Iter 4600, Train loss: 1.6777606010437012, Val loss: 1.8353008031845093\n",
      "Iter 4610, Train loss: 1.6872024536132812, Val loss: 1.83082914352417\n",
      "Iter 4620, Train loss: 1.672024130821228, Val loss: 1.8416528701782227\n",
      "Iter 4630, Train loss: 1.6873176097869873, Val loss: 1.8351402282714844\n",
      "Iter 4640, Train loss: 1.6769031286239624, Val loss: 1.8272284269332886\n",
      "Iter 4650, Train loss: 1.6722866296768188, Val loss: 1.8275201320648193\n",
      "Iter 4660, Train loss: 1.6704583168029785, Val loss: 1.8320269584655762\n",
      "Iter 4670, Train loss: 1.6802383661270142, Val loss: 1.8477352857589722\n",
      "Iter 4680, Train loss: 1.680776834487915, Val loss: 1.8271466493606567\n",
      "Iter 4690, Train loss: 1.6863012313842773, Val loss: 1.8243286609649658\n",
      "Iter 4700, Train loss: 1.6757004261016846, Val loss: 1.8396847248077393\n",
      "Iter 4710, Train loss: 1.6699453592300415, Val loss: 1.8320670127868652\n",
      "Iter 4720, Train loss: 1.6680772304534912, Val loss: 1.846281886100769\n",
      "Iter 4730, Train loss: 1.6714692115783691, Val loss: 1.8314132690429688\n",
      "Iter 4740, Train loss: 1.6769894361495972, Val loss: 1.8158211708068848\n",
      "Iter 4750, Train loss: 1.6593290567398071, Val loss: 1.8165267705917358\n",
      "Iter 4760, Train loss: 1.66856050491333, Val loss: 1.839287281036377\n",
      "Iter 4770, Train loss: 1.6655051708221436, Val loss: 1.8320181369781494\n",
      "Iter 4780, Train loss: 1.659965991973877, Val loss: 1.8122423887252808\n",
      "Iter 4790, Train loss: 1.6667230129241943, Val loss: 1.827836275100708\n",
      "Iter 4800, Train loss: 1.6713000535964966, Val loss: 1.8125628232955933\n",
      "Iter 4810, Train loss: 1.65972101688385, Val loss: 1.8164669275283813\n",
      "Iter 4820, Train loss: 1.6697998046875, Val loss: 1.8281006813049316\n",
      "Iter 4830, Train loss: 1.6761565208435059, Val loss: 1.8213162422180176\n",
      "Iter 4840, Train loss: 1.6649258136749268, Val loss: 1.8324191570281982\n",
      "Iter 4850, Train loss: 1.664771556854248, Val loss: 1.8311450481414795\n",
      "Iter 4860, Train loss: 1.6571861505508423, Val loss: 1.825355052947998\n",
      "Iter 4870, Train loss: 1.674768090248108, Val loss: 1.812433123588562\n",
      "Iter 4880, Train loss: 1.6620874404907227, Val loss: 1.8089396953582764\n",
      "Iter 4890, Train loss: 1.6592953205108643, Val loss: 1.81174898147583\n",
      "Iter 4900, Train loss: 1.6635468006134033, Val loss: 1.8155608177185059\n",
      "Iter 4910, Train loss: 1.6622616052627563, Val loss: 1.820339560508728\n",
      "Iter 4920, Train loss: 1.6554621458053589, Val loss: 1.8137227296829224\n",
      "Iter 4930, Train loss: 1.6738427877426147, Val loss: 1.8307623863220215\n",
      "Iter 4940, Train loss: 1.6661808490753174, Val loss: 1.8149996995925903\n",
      "Iter 4950, Train loss: 1.664526104927063, Val loss: 1.8254727125167847\n",
      "Iter 4960, Train loss: 1.6715387105941772, Val loss: 1.8287353515625\n",
      "Iter 4970, Train loss: 1.6596866846084595, Val loss: 1.8275834321975708\n",
      "Iter 4980, Train loss: 1.6586848497390747, Val loss: 1.8234885931015015\n",
      "Iter 4990, Train loss: 1.6591784954071045, Val loss: 1.8106091022491455\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_lm = LanguageModel().to(device)\n",
    "model = simple_lm\n",
    "optimizer = torch.optim.Adam(simple_lm.parameters(), lr=learning_rate)\n",
    "simple_lm.train()\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for i in range(max_iters):\n",
    "    X, Y = get_batch('train')\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = simple_lm(X, Y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_losses.append(loss.item())\n",
    "    if i % eval_interval == 0:\n",
    "        losses = estimate_loss()\n",
    "        val_losses.append(losses['val'])\n",
    "        print(f'Iter {i}, Train loss: {losses[\"train\"]}, Val loss: {losses[\"val\"]}')\n",
    "        if i > 0 and losses['val'] > max(val_losses[:-1]):\n",
    "            break\n",
    "\n",
    "# plot the training and validation losses\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.plot(train_losses, label='train')\n",
    "plt.plot(eval_interval * (1 + np.arange(len(val_losses))), val_losses, label='val')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0, Train loss: 1.6572396755218506, Val loss: 1.8228111267089844\n",
      "Iter 10, Train loss: 1.6647391319274902, Val loss: 1.821241021156311\n",
      "Iter 20, Train loss: 1.6654843091964722, Val loss: 1.8345156908035278\n",
      "Iter 30, Train loss: 1.660069227218628, Val loss: 1.8177745342254639\n",
      "Iter 40, Train loss: 1.6691648960113525, Val loss: 1.8267877101898193\n",
      "Iter 50, Train loss: 1.6595860719680786, Val loss: 1.8176461458206177\n",
      "Iter 60, Train loss: 1.6477197408676147, Val loss: 1.8213813304901123\n",
      "Iter 70, Train loss: 1.6578116416931152, Val loss: 1.8219643831253052\n",
      "Iter 80, Train loss: 1.6682156324386597, Val loss: 1.8280127048492432\n",
      "Iter 90, Train loss: 1.6673979759216309, Val loss: 1.8304139375686646\n",
      "Iter 100, Train loss: 1.6588283777236938, Val loss: 1.8182175159454346\n",
      "Iter 110, Train loss: 1.6662116050720215, Val loss: 1.844873309135437\n",
      "Iter 120, Train loss: 1.6506741046905518, Val loss: 1.801588535308838\n",
      "Iter 130, Train loss: 1.66621994972229, Val loss: 1.8209905624389648\n",
      "Iter 140, Train loss: 1.6586552858352661, Val loss: 1.8224362134933472\n",
      "Iter 150, Train loss: 1.663231611251831, Val loss: 1.8132067918777466\n",
      "Iter 160, Train loss: 1.6573201417922974, Val loss: 1.8206536769866943\n",
      "Iter 170, Train loss: 1.6587024927139282, Val loss: 1.825989842414856\n",
      "Iter 180, Train loss: 1.6757465600967407, Val loss: 1.8169927597045898\n",
      "Iter 190, Train loss: 1.6543906927108765, Val loss: 1.834322452545166\n",
      "Iter 200, Train loss: 1.6632565259933472, Val loss: 1.8190054893493652\n",
      "Iter 210, Train loss: 1.6562962532043457, Val loss: 1.825844168663025\n",
      "Iter 220, Train loss: 1.6548492908477783, Val loss: 1.820277452468872\n",
      "Iter 230, Train loss: 1.6479793787002563, Val loss: 1.8176859617233276\n",
      "Iter 240, Train loss: 1.6512467861175537, Val loss: 1.8170443773269653\n",
      "Iter 250, Train loss: 1.6594775915145874, Val loss: 1.8180413246154785\n",
      "Iter 260, Train loss: 1.6553066968917847, Val loss: 1.8123112916946411\n",
      "Iter 270, Train loss: 1.6657803058624268, Val loss: 1.815758228302002\n",
      "Iter 280, Train loss: 1.6618999242782593, Val loss: 1.813170313835144\n",
      "Iter 290, Train loss: 1.6631008386611938, Val loss: 1.8275532722473145\n",
      "Iter 300, Train loss: 1.6628280878067017, Val loss: 1.812933087348938\n",
      "Iter 310, Train loss: 1.6633762121200562, Val loss: 1.8285516500473022\n",
      "Iter 320, Train loss: 1.6610490083694458, Val loss: 1.818812370300293\n",
      "Iter 330, Train loss: 1.6608821153640747, Val loss: 1.8273652791976929\n",
      "Iter 340, Train loss: 1.6579402685165405, Val loss: 1.8166015148162842\n",
      "Iter 350, Train loss: 1.6656075716018677, Val loss: 1.814031720161438\n",
      "Iter 360, Train loss: 1.6556302309036255, Val loss: 1.8060342073440552\n",
      "Iter 370, Train loss: 1.6567034721374512, Val loss: 1.8161959648132324\n",
      "Iter 380, Train loss: 1.6613765954971313, Val loss: 1.8211510181427002\n",
      "Iter 390, Train loss: 1.6560888290405273, Val loss: 1.8236024379730225\n",
      "Iter 400, Train loss: 1.6738194227218628, Val loss: 1.8242722749710083\n",
      "Iter 410, Train loss: 1.6543294191360474, Val loss: 1.8177118301391602\n",
      "Iter 420, Train loss: 1.6660822629928589, Val loss: 1.8133889436721802\n",
      "Iter 430, Train loss: 1.6617854833602905, Val loss: 1.8229151964187622\n",
      "Iter 440, Train loss: 1.6611076593399048, Val loss: 1.8132461309432983\n",
      "Iter 450, Train loss: 1.6554064750671387, Val loss: 1.8147594928741455\n",
      "Iter 460, Train loss: 1.6602367162704468, Val loss: 1.8181451559066772\n",
      "Iter 470, Train loss: 1.6580368280410767, Val loss: 1.8074641227722168\n",
      "Iter 480, Train loss: 1.6633727550506592, Val loss: 1.8248977661132812\n",
      "Iter 490, Train loss: 1.6676729917526245, Val loss: 1.8186843395233154\n",
      "Iter 500, Train loss: 1.6499556303024292, Val loss: 1.8138625621795654\n",
      "Iter 510, Train loss: 1.6599828004837036, Val loss: 1.811795711517334\n",
      "Iter 520, Train loss: 1.6627414226531982, Val loss: 1.8185981512069702\n",
      "Iter 530, Train loss: 1.6694695949554443, Val loss: 1.818164348602295\n",
      "Iter 540, Train loss: 1.6602345705032349, Val loss: 1.8149975538253784\n",
      "Iter 550, Train loss: 1.6629942655563354, Val loss: 1.8268005847930908\n",
      "Iter 560, Train loss: 1.6662362813949585, Val loss: 1.8153467178344727\n",
      "Iter 570, Train loss: 1.6580053567886353, Val loss: 1.8085339069366455\n",
      "Iter 580, Train loss: 1.6653245687484741, Val loss: 1.8251490592956543\n",
      "Iter 590, Train loss: 1.6655099391937256, Val loss: 1.8162928819656372\n",
      "Iter 600, Train loss: 1.6664336919784546, Val loss: 1.8144030570983887\n",
      "Iter 610, Train loss: 1.6670547723770142, Val loss: 1.8202497959136963\n",
      "Iter 620, Train loss: 1.6534500122070312, Val loss: 1.8267766237258911\n",
      "Iter 630, Train loss: 1.665398359298706, Val loss: 1.8265252113342285\n",
      "Iter 640, Train loss: 1.653114676475525, Val loss: 1.809419870376587\n",
      "Iter 650, Train loss: 1.6666836738586426, Val loss: 1.8211185932159424\n",
      "Iter 660, Train loss: 1.6608816385269165, Val loss: 1.8176161050796509\n",
      "Iter 670, Train loss: 1.660913109779358, Val loss: 1.822966456413269\n",
      "Iter 680, Train loss: 1.6601605415344238, Val loss: 1.8166723251342773\n",
      "Iter 690, Train loss: 1.6586384773254395, Val loss: 1.8204278945922852\n",
      "Iter 700, Train loss: 1.664243221282959, Val loss: 1.8327724933624268\n",
      "Iter 710, Train loss: 1.658840298652649, Val loss: 1.8195242881774902\n",
      "Iter 720, Train loss: 1.657583475112915, Val loss: 1.820806860923767\n",
      "Iter 730, Train loss: 1.6468621492385864, Val loss: 1.8273265361785889\n",
      "Iter 740, Train loss: 1.663699746131897, Val loss: 1.825980544090271\n",
      "Iter 750, Train loss: 1.6680514812469482, Val loss: 1.8168206214904785\n",
      "Iter 760, Train loss: 1.6630349159240723, Val loss: 1.8195507526397705\n",
      "Iter 770, Train loss: 1.6617188453674316, Val loss: 1.8234018087387085\n",
      "Iter 780, Train loss: 1.6634756326675415, Val loss: 1.826566457748413\n",
      "Iter 790, Train loss: 1.6589491367340088, Val loss: 1.8226957321166992\n",
      "Iter 800, Train loss: 1.6614092588424683, Val loss: 1.8089492321014404\n",
      "Iter 810, Train loss: 1.664071798324585, Val loss: 1.8148078918457031\n",
      "Iter 820, Train loss: 1.6534887552261353, Val loss: 1.8090133666992188\n",
      "Iter 830, Train loss: 1.6510571241378784, Val loss: 1.8210989236831665\n",
      "Iter 840, Train loss: 1.6533409357070923, Val loss: 1.824256420135498\n",
      "Iter 850, Train loss: 1.6728811264038086, Val loss: 1.818297266960144\n",
      "Iter 860, Train loss: 1.6610151529312134, Val loss: 1.8294422626495361\n",
      "Iter 870, Train loss: 1.6600247621536255, Val loss: 1.822880506515503\n",
      "Iter 880, Train loss: 1.6688369512557983, Val loss: 1.8305561542510986\n",
      "Iter 890, Train loss: 1.655145525932312, Val loss: 1.8298041820526123\n",
      "Iter 900, Train loss: 1.6679073572158813, Val loss: 1.829349398612976\n",
      "Iter 910, Train loss: 1.661196231842041, Val loss: 1.826785683631897\n",
      "Iter 920, Train loss: 1.6600158214569092, Val loss: 1.8106951713562012\n",
      "Iter 930, Train loss: 1.663490891456604, Val loss: 1.8182098865509033\n",
      "Iter 940, Train loss: 1.6646536588668823, Val loss: 1.821242094039917\n",
      "Iter 950, Train loss: 1.655307412147522, Val loss: 1.8186907768249512\n",
      "Iter 960, Train loss: 1.6635066270828247, Val loss: 1.8152052164077759\n",
      "Iter 970, Train loss: 1.6607693433761597, Val loss: 1.8151975870132446\n",
      "Iter 980, Train loss: 1.657679557800293, Val loss: 1.8125513792037964\n",
      "Iter 990, Train loss: 1.6615298986434937, Val loss: 1.8133224248886108\n",
      "Iter 1000, Train loss: 1.6665711402893066, Val loss: 1.8269038200378418\n",
      "Iter 1010, Train loss: 1.6659878492355347, Val loss: 1.8214913606643677\n",
      "Iter 1020, Train loss: 1.6573760509490967, Val loss: 1.8256689310073853\n",
      "Iter 1030, Train loss: 1.6593040227890015, Val loss: 1.8087997436523438\n",
      "Iter 1040, Train loss: 1.6619926691055298, Val loss: 1.8202134370803833\n",
      "Iter 1050, Train loss: 1.657090425491333, Val loss: 1.8249839544296265\n",
      "Iter 1060, Train loss: 1.6668392419815063, Val loss: 1.8178093433380127\n",
      "Iter 1070, Train loss: 1.6672173738479614, Val loss: 1.8208483457565308\n",
      "Iter 1080, Train loss: 1.6628319025039673, Val loss: 1.8128571510314941\n",
      "Iter 1090, Train loss: 1.663752794265747, Val loss: 1.8235158920288086\n",
      "Iter 1100, Train loss: 1.6576007604599, Val loss: 1.8167624473571777\n",
      "Iter 1110, Train loss: 1.6698449850082397, Val loss: 1.8280587196350098\n",
      "Iter 1120, Train loss: 1.656967043876648, Val loss: 1.8227219581604004\n",
      "Iter 1130, Train loss: 1.6562767028808594, Val loss: 1.8108923435211182\n",
      "Iter 1140, Train loss: 1.6636813879013062, Val loss: 1.8125039339065552\n",
      "Iter 1150, Train loss: 1.6624903678894043, Val loss: 1.8230494260787964\n",
      "Iter 1160, Train loss: 1.6617463827133179, Val loss: 1.823886513710022\n",
      "Iter 1170, Train loss: 1.6663011312484741, Val loss: 1.8247243165969849\n",
      "Iter 1180, Train loss: 1.660453200340271, Val loss: 1.825636386871338\n",
      "Iter 1190, Train loss: 1.6640901565551758, Val loss: 1.8239080905914307\n",
      "Iter 1200, Train loss: 1.6648271083831787, Val loss: 1.8226507902145386\n",
      "Iter 1210, Train loss: 1.6713790893554688, Val loss: 1.824471116065979\n",
      "Iter 1220, Train loss: 1.663922905921936, Val loss: 1.8177838325500488\n",
      "Iter 1230, Train loss: 1.6664400100708008, Val loss: 1.8178820610046387\n",
      "Iter 1240, Train loss: 1.6650166511535645, Val loss: 1.8187366724014282\n",
      "Iter 1250, Train loss: 1.6660337448120117, Val loss: 1.822621464729309\n",
      "Iter 1260, Train loss: 1.658715009689331, Val loss: 1.8284709453582764\n",
      "Iter 1270, Train loss: 1.6658357381820679, Val loss: 1.8264055252075195\n",
      "Iter 1280, Train loss: 1.6581355333328247, Val loss: 1.8195271492004395\n",
      "Iter 1290, Train loss: 1.6508272886276245, Val loss: 1.817521572113037\n",
      "Iter 1300, Train loss: 1.6555802822113037, Val loss: 1.8163501024246216\n",
      "Iter 1310, Train loss: 1.6675647497177124, Val loss: 1.8127365112304688\n",
      "Iter 1320, Train loss: 1.6564096212387085, Val loss: 1.818281888961792\n",
      "Iter 1330, Train loss: 1.6574256420135498, Val loss: 1.8260504007339478\n",
      "Iter 1340, Train loss: 1.667818307876587, Val loss: 1.81105375289917\n",
      "Iter 1350, Train loss: 1.6678112745285034, Val loss: 1.8290584087371826\n",
      "Iter 1360, Train loss: 1.6603055000305176, Val loss: 1.8210699558258057\n",
      "Iter 1370, Train loss: 1.659852147102356, Val loss: 1.8221467733383179\n",
      "Iter 1380, Train loss: 1.6607427597045898, Val loss: 1.8238208293914795\n",
      "Iter 1390, Train loss: 1.6511749029159546, Val loss: 1.8188166618347168\n",
      "Iter 1400, Train loss: 1.6512718200683594, Val loss: 1.8139777183532715\n",
      "Iter 1410, Train loss: 1.6631801128387451, Val loss: 1.8144687414169312\n",
      "Iter 1420, Train loss: 1.6623115539550781, Val loss: 1.8305959701538086\n",
      "Iter 1430, Train loss: 1.6583725214004517, Val loss: 1.8271191120147705\n",
      "Iter 1440, Train loss: 1.654253363609314, Val loss: 1.8228994607925415\n",
      "Iter 1450, Train loss: 1.6578001976013184, Val loss: 1.8170368671417236\n",
      "Iter 1460, Train loss: 1.6738804578781128, Val loss: 1.819923996925354\n",
      "Iter 1470, Train loss: 1.6530311107635498, Val loss: 1.827032208442688\n",
      "Iter 1480, Train loss: 1.6638896465301514, Val loss: 1.8269351720809937\n",
      "Iter 1490, Train loss: 1.6686536073684692, Val loss: 1.809639573097229\n",
      "Iter 1500, Train loss: 1.6628117561340332, Val loss: 1.8287369012832642\n",
      "Iter 1510, Train loss: 1.666978359222412, Val loss: 1.8330825567245483\n",
      "Iter 1520, Train loss: 1.6638741493225098, Val loss: 1.813120722770691\n",
      "Iter 1530, Train loss: 1.6642556190490723, Val loss: 1.823460578918457\n",
      "Iter 1540, Train loss: 1.6592926979064941, Val loss: 1.8288859128952026\n",
      "Iter 1550, Train loss: 1.6683954000473022, Val loss: 1.8193327188491821\n",
      "Iter 1560, Train loss: 1.661906123161316, Val loss: 1.8177098035812378\n",
      "Iter 1570, Train loss: 1.6568478345870972, Val loss: 1.8238714933395386\n",
      "Iter 1580, Train loss: 1.6642299890518188, Val loss: 1.8067047595977783\n",
      "Iter 1590, Train loss: 1.6621249914169312, Val loss: 1.822656512260437\n",
      "Iter 1600, Train loss: 1.662490725517273, Val loss: 1.8157556056976318\n",
      "Iter 1610, Train loss: 1.6564983129501343, Val loss: 1.816573143005371\n",
      "Iter 1620, Train loss: 1.6699715852737427, Val loss: 1.8152992725372314\n",
      "Iter 1630, Train loss: 1.6642181873321533, Val loss: 1.817020058631897\n",
      "Iter 1640, Train loss: 1.6599948406219482, Val loss: 1.816825270652771\n",
      "Iter 1650, Train loss: 1.6738379001617432, Val loss: 1.8086515665054321\n",
      "Iter 1660, Train loss: 1.6631017923355103, Val loss: 1.8140475749969482\n",
      "Iter 1670, Train loss: 1.665717363357544, Val loss: 1.825646996498108\n",
      "Iter 1680, Train loss: 1.658055067062378, Val loss: 1.8214994668960571\n",
      "Iter 1690, Train loss: 1.659448266029358, Val loss: 1.8233875036239624\n",
      "Iter 1700, Train loss: 1.6555651426315308, Val loss: 1.8297079801559448\n",
      "Iter 1710, Train loss: 1.6611628532409668, Val loss: 1.810776710510254\n",
      "Iter 1720, Train loss: 1.6645315885543823, Val loss: 1.8161908388137817\n",
      "Iter 1730, Train loss: 1.663693904876709, Val loss: 1.8353691101074219\n",
      "Iter 1740, Train loss: 1.6617817878723145, Val loss: 1.8115370273590088\n",
      "Iter 1750, Train loss: 1.6569751501083374, Val loss: 1.815868854522705\n",
      "Iter 1760, Train loss: 1.6548793315887451, Val loss: 1.8142558336257935\n",
      "Iter 1770, Train loss: 1.6587716341018677, Val loss: 1.8183212280273438\n",
      "Iter 1780, Train loss: 1.6570541858673096, Val loss: 1.8218607902526855\n",
      "Iter 1790, Train loss: 1.6643075942993164, Val loss: 1.822635531425476\n",
      "Iter 1800, Train loss: 1.6589996814727783, Val loss: 1.8228410482406616\n",
      "Iter 1810, Train loss: 1.667016863822937, Val loss: 1.8225184679031372\n",
      "Iter 1820, Train loss: 1.6583764553070068, Val loss: 1.806624412536621\n",
      "Iter 1830, Train loss: 1.6582146883010864, Val loss: 1.8265856504440308\n",
      "Iter 1840, Train loss: 1.6481504440307617, Val loss: 1.8154667615890503\n",
      "Iter 1850, Train loss: 1.655833125114441, Val loss: 1.824476957321167\n",
      "Iter 1860, Train loss: 1.6689914464950562, Val loss: 1.8162885904312134\n",
      "Iter 1870, Train loss: 1.6449341773986816, Val loss: 1.8211867809295654\n",
      "Iter 1880, Train loss: 1.653578519821167, Val loss: 1.8136014938354492\n",
      "Iter 1890, Train loss: 1.657737374305725, Val loss: 1.8228111267089844\n",
      "Iter 1900, Train loss: 1.6615045070648193, Val loss: 1.8144763708114624\n",
      "Iter 1910, Train loss: 1.6694021224975586, Val loss: 1.8157886266708374\n",
      "Iter 1920, Train loss: 1.6662272214889526, Val loss: 1.8192570209503174\n",
      "Iter 1930, Train loss: 1.6605758666992188, Val loss: 1.816411018371582\n",
      "Iter 1940, Train loss: 1.6561775207519531, Val loss: 1.827938437461853\n",
      "Iter 1950, Train loss: 1.661271333694458, Val loss: 1.8239378929138184\n",
      "Iter 1960, Train loss: 1.6581835746765137, Val loss: 1.818342685699463\n",
      "Iter 1970, Train loss: 1.6486387252807617, Val loss: 1.813442587852478\n",
      "Iter 1980, Train loss: 1.6605087518692017, Val loss: 1.810575008392334\n",
      "Iter 1990, Train loss: 1.6548422574996948, Val loss: 1.8236024379730225\n",
      "Iter 2000, Train loss: 1.662543773651123, Val loss: 1.8209991455078125\n",
      "Iter 2010, Train loss: 1.653987169265747, Val loss: 1.8279390335083008\n",
      "Iter 2020, Train loss: 1.6621395349502563, Val loss: 1.830037236213684\n",
      "Iter 2030, Train loss: 1.6603602170944214, Val loss: 1.8176286220550537\n",
      "Iter 2040, Train loss: 1.659593939781189, Val loss: 1.8200640678405762\n",
      "Iter 2050, Train loss: 1.6662429571151733, Val loss: 1.822292685508728\n",
      "Iter 2060, Train loss: 1.660144329071045, Val loss: 1.8228298425674438\n",
      "Iter 2070, Train loss: 1.658851981163025, Val loss: 1.8272572755813599\n",
      "Iter 2080, Train loss: 1.6547733545303345, Val loss: 1.8164846897125244\n",
      "Iter 2090, Train loss: 1.6618293523788452, Val loss: 1.822356104850769\n",
      "Iter 2100, Train loss: 1.6600844860076904, Val loss: 1.8163331747055054\n",
      "Iter 2110, Train loss: 1.6587915420532227, Val loss: 1.8102238178253174\n",
      "Iter 2120, Train loss: 1.6670594215393066, Val loss: 1.809617519378662\n",
      "Iter 2130, Train loss: 1.6566362380981445, Val loss: 1.822149634361267\n",
      "Iter 2140, Train loss: 1.6684019565582275, Val loss: 1.821018934249878\n",
      "Iter 2150, Train loss: 1.6627531051635742, Val loss: 1.8124606609344482\n",
      "Iter 2160, Train loss: 1.6640630960464478, Val loss: 1.8102947473526\n",
      "Iter 2170, Train loss: 1.6613378524780273, Val loss: 1.8108493089675903\n",
      "Iter 2180, Train loss: 1.6628413200378418, Val loss: 1.8144334554672241\n",
      "Iter 2190, Train loss: 1.6640719175338745, Val loss: 1.8215793371200562\n",
      "Iter 2200, Train loss: 1.6639851331710815, Val loss: 1.8105034828186035\n",
      "Iter 2210, Train loss: 1.6579461097717285, Val loss: 1.8249849081039429\n",
      "Iter 2220, Train loss: 1.6522990465164185, Val loss: 1.813570499420166\n",
      "Iter 2230, Train loss: 1.6603813171386719, Val loss: 1.8251032829284668\n",
      "Iter 2240, Train loss: 1.6634433269500732, Val loss: 1.8143502473831177\n",
      "Iter 2250, Train loss: 1.6658859252929688, Val loss: 1.8258414268493652\n",
      "Iter 2260, Train loss: 1.6650084257125854, Val loss: 1.8133549690246582\n",
      "Iter 2270, Train loss: 1.6564321517944336, Val loss: 1.81806218624115\n",
      "Iter 2280, Train loss: 1.6593530178070068, Val loss: 1.8220785856246948\n",
      "Iter 2290, Train loss: 1.655758023262024, Val loss: 1.8144943714141846\n",
      "Iter 2300, Train loss: 1.657559871673584, Val loss: 1.8035686016082764\n",
      "Iter 2310, Train loss: 1.66886568069458, Val loss: 1.8090227842330933\n",
      "Iter 2320, Train loss: 1.6578521728515625, Val loss: 1.8209936618804932\n",
      "Iter 2330, Train loss: 1.6566071510314941, Val loss: 1.8229588270187378\n",
      "Iter 2340, Train loss: 1.657862901687622, Val loss: 1.8180471658706665\n",
      "Iter 2350, Train loss: 1.6648821830749512, Val loss: 1.8145928382873535\n",
      "Iter 2360, Train loss: 1.6665457487106323, Val loss: 1.8162868022918701\n",
      "Iter 2370, Train loss: 1.6689759492874146, Val loss: 1.821678638458252\n",
      "Iter 2380, Train loss: 1.6649659872055054, Val loss: 1.8308022022247314\n",
      "Iter 2390, Train loss: 1.6578723192214966, Val loss: 1.8157974481582642\n",
      "Iter 2400, Train loss: 1.6617345809936523, Val loss: 1.8149447441101074\n",
      "Iter 2410, Train loss: 1.662634015083313, Val loss: 1.825058102607727\n",
      "Iter 2420, Train loss: 1.6582472324371338, Val loss: 1.8268853425979614\n",
      "Iter 2430, Train loss: 1.6677190065383911, Val loss: 1.8233987092971802\n",
      "Iter 2440, Train loss: 1.661852240562439, Val loss: 1.8113080263137817\n",
      "Iter 2450, Train loss: 1.6582269668579102, Val loss: 1.8225030899047852\n",
      "Iter 2460, Train loss: 1.658382773399353, Val loss: 1.826238751411438\n",
      "Iter 2470, Train loss: 1.6666176319122314, Val loss: 1.8216030597686768\n",
      "Iter 2480, Train loss: 1.6662832498550415, Val loss: 1.8039356470108032\n",
      "Iter 2490, Train loss: 1.6533608436584473, Val loss: 1.8245543241500854\n",
      "Iter 2500, Train loss: 1.6547274589538574, Val loss: 1.8147732019424438\n",
      "Iter 2510, Train loss: 1.6616883277893066, Val loss: 1.8145081996917725\n",
      "Iter 2520, Train loss: 1.6493808031082153, Val loss: 1.8141708374023438\n",
      "Iter 2530, Train loss: 1.6630384922027588, Val loss: 1.8205764293670654\n",
      "Iter 2540, Train loss: 1.6588183641433716, Val loss: 1.8200088739395142\n",
      "Iter 2550, Train loss: 1.6574339866638184, Val loss: 1.8301353454589844\n",
      "Iter 2560, Train loss: 1.6574504375457764, Val loss: 1.8204073905944824\n",
      "Iter 2570, Train loss: 1.6589512825012207, Val loss: 1.8283436298370361\n",
      "Iter 2580, Train loss: 1.6568881273269653, Val loss: 1.8057600259780884\n",
      "Iter 2590, Train loss: 1.6555163860321045, Val loss: 1.8203721046447754\n",
      "Iter 2600, Train loss: 1.6562201976776123, Val loss: 1.8187446594238281\n",
      "Iter 2610, Train loss: 1.656693458557129, Val loss: 1.816483736038208\n",
      "Iter 2620, Train loss: 1.6575684547424316, Val loss: 1.8300737142562866\n",
      "Iter 2630, Train loss: 1.6579376459121704, Val loss: 1.812942624092102\n",
      "Iter 2640, Train loss: 1.665008544921875, Val loss: 1.828485131263733\n",
      "Iter 2650, Train loss: 1.6595747470855713, Val loss: 1.8268977403640747\n",
      "Iter 2660, Train loss: 1.6632189750671387, Val loss: 1.817764401435852\n",
      "Iter 2670, Train loss: 1.6692973375320435, Val loss: 1.825039029121399\n",
      "Iter 2680, Train loss: 1.647621512413025, Val loss: 1.8283133506774902\n",
      "Iter 2690, Train loss: 1.6561369895935059, Val loss: 1.8239468336105347\n",
      "Iter 2700, Train loss: 1.6655402183532715, Val loss: 1.8132681846618652\n",
      "Iter 2710, Train loss: 1.6608027219772339, Val loss: 1.8237273693084717\n",
      "Iter 2720, Train loss: 1.6583611965179443, Val loss: 1.809836745262146\n",
      "Iter 2730, Train loss: 1.6639492511749268, Val loss: 1.813954472541809\n",
      "Iter 2740, Train loss: 1.6606448888778687, Val loss: 1.8209176063537598\n",
      "Iter 2750, Train loss: 1.6576173305511475, Val loss: 1.8141915798187256\n",
      "Iter 2760, Train loss: 1.6639703512191772, Val loss: 1.8132423162460327\n",
      "Iter 2770, Train loss: 1.6626160144805908, Val loss: 1.8188613653182983\n",
      "Iter 2780, Train loss: 1.6477842330932617, Val loss: 1.8180958032608032\n",
      "Iter 2790, Train loss: 1.6597228050231934, Val loss: 1.8240069150924683\n",
      "Iter 2800, Train loss: 1.662519931793213, Val loss: 1.8207576274871826\n",
      "Iter 2810, Train loss: 1.6729633808135986, Val loss: 1.8193068504333496\n",
      "Iter 2820, Train loss: 1.6565330028533936, Val loss: 1.8172188997268677\n",
      "Iter 2830, Train loss: 1.6736016273498535, Val loss: 1.823312520980835\n",
      "Iter 2840, Train loss: 1.658724069595337, Val loss: 1.8269751071929932\n",
      "Iter 2850, Train loss: 1.6557904481887817, Val loss: 1.8281875848770142\n",
      "Iter 2860, Train loss: 1.6519297361373901, Val loss: 1.8286759853363037\n",
      "Iter 2870, Train loss: 1.671004056930542, Val loss: 1.8218357563018799\n",
      "Iter 2880, Train loss: 1.6678401231765747, Val loss: 1.8138923645019531\n",
      "Iter 2890, Train loss: 1.6641730070114136, Val loss: 1.8204584121704102\n",
      "Iter 2900, Train loss: 1.6732486486434937, Val loss: 1.8213926553726196\n",
      "Iter 2910, Train loss: 1.6638368368148804, Val loss: 1.8233959674835205\n",
      "Iter 2920, Train loss: 1.6523452997207642, Val loss: 1.8180545568466187\n",
      "Iter 2930, Train loss: 1.6566675901412964, Val loss: 1.8032793998718262\n",
      "Iter 2940, Train loss: 1.6554135084152222, Val loss: 1.8239854574203491\n",
      "Iter 2950, Train loss: 1.6573355197906494, Val loss: 1.8318883180618286\n",
      "Iter 2960, Train loss: 1.6623220443725586, Val loss: 1.821388840675354\n",
      "Iter 2970, Train loss: 1.6672089099884033, Val loss: 1.8194488286972046\n",
      "Iter 2980, Train loss: 1.6527019739151, Val loss: 1.8261785507202148\n",
      "Iter 2990, Train loss: 1.6648908853530884, Val loss: 1.8133147954940796\n",
      "Iter 3000, Train loss: 1.6648101806640625, Val loss: 1.8267406225204468\n",
      "Iter 3010, Train loss: 1.659234881401062, Val loss: 1.8303353786468506\n",
      "Iter 3020, Train loss: 1.66347074508667, Val loss: 1.8154383897781372\n",
      "Iter 3030, Train loss: 1.661342740058899, Val loss: 1.8245408535003662\n",
      "Iter 3040, Train loss: 1.658186912536621, Val loss: 1.8173530101776123\n",
      "Iter 3050, Train loss: 1.6682144403457642, Val loss: 1.8108559846878052\n",
      "Iter 3060, Train loss: 1.6606065034866333, Val loss: 1.8191369771957397\n",
      "Iter 3070, Train loss: 1.6554439067840576, Val loss: 1.8223543167114258\n",
      "Iter 3080, Train loss: 1.6618332862854004, Val loss: 1.817633867263794\n",
      "Iter 3090, Train loss: 1.669856071472168, Val loss: 1.8162744045257568\n",
      "Iter 3100, Train loss: 1.6626826524734497, Val loss: 1.8274331092834473\n",
      "Iter 3110, Train loss: 1.666892647743225, Val loss: 1.8296937942504883\n",
      "Iter 3120, Train loss: 1.6614187955856323, Val loss: 1.8233600854873657\n",
      "Iter 3130, Train loss: 1.6578209400177002, Val loss: 1.8348675966262817\n",
      "Iter 3140, Train loss: 1.6675024032592773, Val loss: 1.8121331930160522\n",
      "Iter 3150, Train loss: 1.6580719947814941, Val loss: 1.817194938659668\n",
      "Iter 3160, Train loss: 1.672834873199463, Val loss: 1.8125908374786377\n",
      "Iter 3170, Train loss: 1.6592721939086914, Val loss: 1.8182967901229858\n",
      "Iter 3180, Train loss: 1.663209080696106, Val loss: 1.8231853246688843\n",
      "Iter 3190, Train loss: 1.6495099067687988, Val loss: 1.8161635398864746\n",
      "Iter 3200, Train loss: 1.6667746305465698, Val loss: 1.8221862316131592\n",
      "Iter 3210, Train loss: 1.6715774536132812, Val loss: 1.8201515674591064\n",
      "Iter 3220, Train loss: 1.663055419921875, Val loss: 1.8170896768569946\n",
      "Iter 3230, Train loss: 1.6519755125045776, Val loss: 1.821608304977417\n",
      "Iter 3240, Train loss: 1.6633213758468628, Val loss: 1.823048710823059\n",
      "Iter 3250, Train loss: 1.6619950532913208, Val loss: 1.8256527185440063\n",
      "Iter 3260, Train loss: 1.6669217348098755, Val loss: 1.8285990953445435\n",
      "Iter 3270, Train loss: 1.655264139175415, Val loss: 1.816333293914795\n",
      "Iter 3280, Train loss: 1.6605833768844604, Val loss: 1.8314863443374634\n",
      "Iter 3290, Train loss: 1.665258526802063, Val loss: 1.8192602396011353\n",
      "Iter 3300, Train loss: 1.6597729921340942, Val loss: 1.811781644821167\n",
      "Iter 3310, Train loss: 1.658534049987793, Val loss: 1.8167001008987427\n",
      "Iter 3320, Train loss: 1.6576591730117798, Val loss: 1.815879464149475\n",
      "Iter 3330, Train loss: 1.6577762365341187, Val loss: 1.8117724657058716\n",
      "Iter 3340, Train loss: 1.6579818725585938, Val loss: 1.8265498876571655\n",
      "Iter 3350, Train loss: 1.6682980060577393, Val loss: 1.8168444633483887\n",
      "Iter 3360, Train loss: 1.6587063074111938, Val loss: 1.816154956817627\n",
      "Iter 3370, Train loss: 1.6706342697143555, Val loss: 1.8140095472335815\n",
      "Iter 3380, Train loss: 1.6601639986038208, Val loss: 1.8317211866378784\n",
      "Iter 3390, Train loss: 1.6554874181747437, Val loss: 1.8221591711044312\n",
      "Iter 3400, Train loss: 1.6634706258773804, Val loss: 1.8177039623260498\n",
      "Iter 3410, Train loss: 1.6592861413955688, Val loss: 1.8251347541809082\n",
      "Iter 3420, Train loss: 1.6695634126663208, Val loss: 1.8170777559280396\n",
      "Iter 3430, Train loss: 1.6669105291366577, Val loss: 1.8214187622070312\n",
      "Iter 3440, Train loss: 1.6550610065460205, Val loss: 1.8178943395614624\n",
      "Iter 3450, Train loss: 1.6539371013641357, Val loss: 1.8168493509292603\n",
      "Iter 3460, Train loss: 1.6683672666549683, Val loss: 1.8322055339813232\n",
      "Iter 3470, Train loss: 1.6517747640609741, Val loss: 1.8134713172912598\n",
      "Iter 3480, Train loss: 1.6581612825393677, Val loss: 1.8108035326004028\n",
      "Iter 3490, Train loss: 1.6556329727172852, Val loss: 1.8149570226669312\n",
      "Iter 3500, Train loss: 1.6692266464233398, Val loss: 1.827531099319458\n",
      "Iter 3510, Train loss: 1.672595500946045, Val loss: 1.8202341794967651\n",
      "Iter 3520, Train loss: 1.667508840560913, Val loss: 1.809046745300293\n",
      "Iter 3530, Train loss: 1.6651545763015747, Val loss: 1.8251829147338867\n",
      "Iter 3540, Train loss: 1.6631947755813599, Val loss: 1.8182510137557983\n",
      "Iter 3550, Train loss: 1.6610461473464966, Val loss: 1.8146791458129883\n",
      "Iter 3560, Train loss: 1.6631531715393066, Val loss: 1.8284332752227783\n",
      "Iter 3570, Train loss: 1.6693165302276611, Val loss: 1.8225139379501343\n",
      "Iter 3580, Train loss: 1.6626389026641846, Val loss: 1.8279417753219604\n",
      "Iter 3590, Train loss: 1.6568553447723389, Val loss: 1.8094346523284912\n",
      "Iter 3600, Train loss: 1.6526108980178833, Val loss: 1.8332557678222656\n",
      "Iter 3610, Train loss: 1.6594053506851196, Val loss: 1.8229708671569824\n",
      "Iter 3620, Train loss: 1.667626976966858, Val loss: 1.8184478282928467\n",
      "Iter 3630, Train loss: 1.662904977798462, Val loss: 1.8247684240341187\n",
      "Iter 3640, Train loss: 1.6514129638671875, Val loss: 1.822512149810791\n",
      "Iter 3650, Train loss: 1.6618516445159912, Val loss: 1.8226022720336914\n",
      "Iter 3660, Train loss: 1.648114800453186, Val loss: 1.8174368143081665\n",
      "Iter 3670, Train loss: 1.6605340242385864, Val loss: 1.8249229192733765\n",
      "Iter 3680, Train loss: 1.6610525846481323, Val loss: 1.8175766468048096\n",
      "Iter 3690, Train loss: 1.6652270555496216, Val loss: 1.8182590007781982\n",
      "Iter 3700, Train loss: 1.661241054534912, Val loss: 1.8192843198776245\n",
      "Iter 3710, Train loss: 1.6674913167953491, Val loss: 1.823338270187378\n",
      "Iter 3720, Train loss: 1.655408501625061, Val loss: 1.820735216140747\n",
      "Iter 3730, Train loss: 1.6555752754211426, Val loss: 1.820000410079956\n",
      "Iter 3740, Train loss: 1.6685198545455933, Val loss: 1.827512502670288\n",
      "Iter 3750, Train loss: 1.6603764295578003, Val loss: 1.8141379356384277\n",
      "Iter 3760, Train loss: 1.663069486618042, Val loss: 1.8198939561843872\n",
      "Iter 3770, Train loss: 1.6657894849777222, Val loss: 1.8202508687973022\n",
      "Iter 3780, Train loss: 1.6696391105651855, Val loss: 1.8219659328460693\n",
      "Iter 3790, Train loss: 1.6615533828735352, Val loss: 1.8156707286834717\n",
      "Iter 3800, Train loss: 1.6578155755996704, Val loss: 1.8194749355316162\n",
      "Iter 3810, Train loss: 1.659908413887024, Val loss: 1.8253098726272583\n",
      "Iter 3820, Train loss: 1.6582884788513184, Val loss: 1.817080020904541\n",
      "Iter 3830, Train loss: 1.6506670713424683, Val loss: 1.8162031173706055\n",
      "Iter 3840, Train loss: 1.6572755575180054, Val loss: 1.8259435892105103\n",
      "Iter 3850, Train loss: 1.6526511907577515, Val loss: 1.8184236288070679\n",
      "Iter 3860, Train loss: 1.6707146167755127, Val loss: 1.824354887008667\n",
      "Iter 3870, Train loss: 1.6732535362243652, Val loss: 1.8031240701675415\n",
      "Iter 3880, Train loss: 1.6627999544143677, Val loss: 1.8079659938812256\n",
      "Iter 3890, Train loss: 1.668066143989563, Val loss: 1.8252692222595215\n",
      "Iter 3900, Train loss: 1.663831353187561, Val loss: 1.819487452507019\n",
      "Iter 3910, Train loss: 1.6617958545684814, Val loss: 1.822679877281189\n",
      "Iter 3920, Train loss: 1.6626516580581665, Val loss: 1.8176831007003784\n",
      "Iter 3930, Train loss: 1.6630675792694092, Val loss: 1.8254516124725342\n",
      "Iter 3940, Train loss: 1.669702410697937, Val loss: 1.8258557319641113\n",
      "Iter 3950, Train loss: 1.66373872756958, Val loss: 1.8179001808166504\n",
      "Iter 3960, Train loss: 1.6580314636230469, Val loss: 1.8132617473602295\n",
      "Iter 3970, Train loss: 1.6454826593399048, Val loss: 1.8189486265182495\n",
      "Iter 3980, Train loss: 1.661229133605957, Val loss: 1.8230160474777222\n",
      "Iter 3990, Train loss: 1.6555064916610718, Val loss: 1.8111698627471924\n",
      "Iter 4000, Train loss: 1.6575143337249756, Val loss: 1.8183952569961548\n",
      "Iter 4010, Train loss: 1.655587911605835, Val loss: 1.8187205791473389\n",
      "Iter 4020, Train loss: 1.6553306579589844, Val loss: 1.8236204385757446\n",
      "Iter 4030, Train loss: 1.660693645477295, Val loss: 1.8306912183761597\n",
      "Iter 4040, Train loss: 1.658884882926941, Val loss: 1.8249294757843018\n",
      "Iter 4050, Train loss: 1.6620261669158936, Val loss: 1.8119336366653442\n",
      "Iter 4060, Train loss: 1.6467045545578003, Val loss: 1.8261044025421143\n",
      "Iter 4070, Train loss: 1.6689380407333374, Val loss: 1.8170862197875977\n",
      "Iter 4080, Train loss: 1.6591322422027588, Val loss: 1.8138314485549927\n",
      "Iter 4090, Train loss: 1.663381814956665, Val loss: 1.8227747678756714\n",
      "Iter 4100, Train loss: 1.6602985858917236, Val loss: 1.8256417512893677\n",
      "Iter 4110, Train loss: 1.6552362442016602, Val loss: 1.8285033702850342\n",
      "Iter 4120, Train loss: 1.6579362154006958, Val loss: 1.8253880739212036\n",
      "Iter 4130, Train loss: 1.6666795015335083, Val loss: 1.8222407102584839\n",
      "Iter 4140, Train loss: 1.6549127101898193, Val loss: 1.8108642101287842\n",
      "Iter 4150, Train loss: 1.6609241962432861, Val loss: 1.8197439908981323\n",
      "Iter 4160, Train loss: 1.6676814556121826, Val loss: 1.8183599710464478\n",
      "Iter 4170, Train loss: 1.6637076139450073, Val loss: 1.8231909275054932\n",
      "Iter 4180, Train loss: 1.6568607091903687, Val loss: 1.8224289417266846\n",
      "Iter 4190, Train loss: 1.6619515419006348, Val loss: 1.8116850852966309\n",
      "Iter 4200, Train loss: 1.6563488245010376, Val loss: 1.8185878992080688\n",
      "Iter 4210, Train loss: 1.6653250455856323, Val loss: 1.8220088481903076\n",
      "Iter 4220, Train loss: 1.6495826244354248, Val loss: 1.8257418870925903\n",
      "Iter 4230, Train loss: 1.6535637378692627, Val loss: 1.8204565048217773\n",
      "Iter 4240, Train loss: 1.6590580940246582, Val loss: 1.819257378578186\n",
      "Iter 4250, Train loss: 1.6550487279891968, Val loss: 1.823065161705017\n",
      "Iter 4260, Train loss: 1.659993290901184, Val loss: 1.823878526687622\n",
      "Iter 4270, Train loss: 1.6642321348190308, Val loss: 1.8142129182815552\n",
      "Iter 4280, Train loss: 1.6716023683547974, Val loss: 1.8189666271209717\n",
      "Iter 4290, Train loss: 1.6507024765014648, Val loss: 1.8140411376953125\n",
      "Iter 4300, Train loss: 1.655301809310913, Val loss: 1.8187614679336548\n",
      "Iter 4310, Train loss: 1.6627724170684814, Val loss: 1.8228013515472412\n",
      "Iter 4320, Train loss: 1.6746835708618164, Val loss: 1.8257474899291992\n",
      "Iter 4330, Train loss: 1.6612625122070312, Val loss: 1.8238574266433716\n",
      "Iter 4340, Train loss: 1.657027006149292, Val loss: 1.813369870185852\n",
      "Iter 4350, Train loss: 1.6642332077026367, Val loss: 1.816506028175354\n",
      "Iter 4360, Train loss: 1.660362720489502, Val loss: 1.803639531135559\n",
      "Iter 4370, Train loss: 1.662896752357483, Val loss: 1.8128652572631836\n",
      "Iter 4380, Train loss: 1.6615500450134277, Val loss: 1.8149374723434448\n",
      "Iter 4390, Train loss: 1.6586432456970215, Val loss: 1.8022949695587158\n",
      "Iter 4400, Train loss: 1.6682850122451782, Val loss: 1.8110040426254272\n",
      "Iter 4410, Train loss: 1.66847825050354, Val loss: 1.8242149353027344\n",
      "Iter 4420, Train loss: 1.664138913154602, Val loss: 1.826475977897644\n",
      "Iter 4430, Train loss: 1.6637252569198608, Val loss: 1.8165740966796875\n",
      "Iter 4440, Train loss: 1.658071756362915, Val loss: 1.8218106031417847\n",
      "Iter 4450, Train loss: 1.6645489931106567, Val loss: 1.8276361227035522\n",
      "Iter 4460, Train loss: 1.663617730140686, Val loss: 1.817213773727417\n",
      "Iter 4470, Train loss: 1.6697620153427124, Val loss: 1.8219619989395142\n",
      "Iter 4480, Train loss: 1.6542495489120483, Val loss: 1.818091630935669\n",
      "Iter 4490, Train loss: 1.6646877527236938, Val loss: 1.8093018531799316\n",
      "Iter 4500, Train loss: 1.661160945892334, Val loss: 1.8276857137680054\n",
      "Iter 4510, Train loss: 1.6662217378616333, Val loss: 1.8248848915100098\n",
      "Iter 4520, Train loss: 1.6564476490020752, Val loss: 1.8224986791610718\n",
      "Iter 4530, Train loss: 1.6688035726547241, Val loss: 1.8297436237335205\n",
      "Iter 4540, Train loss: 1.6667845249176025, Val loss: 1.8199900388717651\n",
      "Iter 4550, Train loss: 1.6581451892852783, Val loss: 1.8312591314315796\n",
      "Iter 4560, Train loss: 1.657901406288147, Val loss: 1.8184491395950317\n",
      "Iter 4570, Train loss: 1.6622315645217896, Val loss: 1.8220715522766113\n",
      "Iter 4580, Train loss: 1.6536283493041992, Val loss: 1.824399471282959\n",
      "Iter 4590, Train loss: 1.6674875020980835, Val loss: 1.8266466856002808\n",
      "Iter 4600, Train loss: 1.6532635688781738, Val loss: 1.8207358121871948\n",
      "Iter 4610, Train loss: 1.6616215705871582, Val loss: 1.8121052980422974\n",
      "Iter 4620, Train loss: 1.672682285308838, Val loss: 1.8230241537094116\n",
      "Iter 4630, Train loss: 1.6664897203445435, Val loss: 1.830653190612793\n",
      "Iter 4640, Train loss: 1.6551028490066528, Val loss: 1.8120886087417603\n",
      "Iter 4650, Train loss: 1.6564518213272095, Val loss: 1.8213845491409302\n",
      "Iter 4660, Train loss: 1.6670408248901367, Val loss: 1.8166894912719727\n",
      "Iter 4670, Train loss: 1.6659574508666992, Val loss: 1.8275456428527832\n",
      "Iter 4680, Train loss: 1.65769624710083, Val loss: 1.8168781995773315\n",
      "Iter 4690, Train loss: 1.6557399034500122, Val loss: 1.8201549053192139\n",
      "Iter 4700, Train loss: 1.669858694076538, Val loss: 1.8243409395217896\n",
      "Iter 4710, Train loss: 1.6579716205596924, Val loss: 1.8093960285186768\n",
      "Iter 4720, Train loss: 1.6671475172042847, Val loss: 1.8145170211791992\n",
      "Iter 4730, Train loss: 1.6640428304672241, Val loss: 1.8166640996932983\n",
      "Iter 4740, Train loss: 1.657626986503601, Val loss: 1.8222308158874512\n",
      "Iter 4750, Train loss: 1.6636946201324463, Val loss: 1.8222534656524658\n",
      "Iter 4760, Train loss: 1.6674094200134277, Val loss: 1.8280583620071411\n",
      "Iter 4770, Train loss: 1.65509831905365, Val loss: 1.819401502609253\n",
      "Iter 4780, Train loss: 1.663560152053833, Val loss: 1.829979658126831\n",
      "Iter 4790, Train loss: 1.6564264297485352, Val loss: 1.8332078456878662\n",
      "Iter 4800, Train loss: 1.6526919603347778, Val loss: 1.815216064453125\n",
      "Iter 4810, Train loss: 1.6655325889587402, Val loss: 1.815309762954712\n",
      "Iter 4820, Train loss: 1.6628865003585815, Val loss: 1.8373441696166992\n",
      "Iter 4830, Train loss: 1.6518136262893677, Val loss: 1.816896915435791\n",
      "Iter 4840, Train loss: 1.6498597860336304, Val loss: 1.8289663791656494\n",
      "Iter 4850, Train loss: 1.6529569625854492, Val loss: 1.8090460300445557\n",
      "Iter 4860, Train loss: 1.6630302667617798, Val loss: 1.8222039937973022\n",
      "Iter 4870, Train loss: 1.6647151708602905, Val loss: 1.8140891790390015\n",
      "Iter 4880, Train loss: 1.6582818031311035, Val loss: 1.8165019750595093\n",
      "Iter 4890, Train loss: 1.654042363166809, Val loss: 1.8235490322113037\n",
      "Iter 4900, Train loss: 1.6680853366851807, Val loss: 1.826621413230896\n",
      "Iter 4910, Train loss: 1.6613308191299438, Val loss: 1.812278389930725\n",
      "Iter 4920, Train loss: 1.6559855937957764, Val loss: 1.8122563362121582\n",
      "Iter 4930, Train loss: 1.6682612895965576, Val loss: 1.8114069700241089\n",
      "Iter 4940, Train loss: 1.6613839864730835, Val loss: 1.8209619522094727\n",
      "Iter 4950, Train loss: 1.6581177711486816, Val loss: 1.8223191499710083\n",
      "Iter 4960, Train loss: 1.653659701347351, Val loss: 1.8310420513153076\n",
      "Iter 4970, Train loss: 1.6624482870101929, Val loss: 1.8202406167984009\n",
      "Iter 4980, Train loss: 1.6665388345718384, Val loss: 1.8168565034866333\n",
      "Iter 4990, Train loss: 1.6597929000854492, Val loss: 1.8223158121109009\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_9600/908208716.py:27: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_losses = [torch.exp(torch.tensor(l)).item() for l in train_losses]\n",
      "/tmp/ipykernel_9600/908208716.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_losses = [torch.exp(torch.tensor(l)).item() for l in val_losses]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_lm = LanguageModel().to(device)\n",
    "optimizer = torch.optim.Adam(simple_lm.parameters(), lr=learning_rate)\n",
    "\n",
    "# training loop function\n",
    "def train_loop(model, max_iters, eval_interval):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    for i in range(max_iters):\n",
    "        X, Y = get_batch('train')\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(X, Y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % eval_interval == 0:\n",
    "            losses = estimate_loss()\n",
    "            train_losses.append(losses['train'])\n",
    "            val_losses.append(losses['val'])\n",
    "            print(f'Iter {i}, Train loss: {losses[\"train\"]}, Val loss: {losses[\"val\"]}')\n",
    "    return train_losses, val_losses\n",
    "\n",
    "# plot losses and perplexity function as subplots\n",
    "def plot_losses_perplexity(train_losses, val_losses):\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    train_perp = [torch.exp(torch.tensor(l)).item() for l in train_losses]\n",
    "    val_perp = [torch.exp(torch.tensor(l)).item() for l in val_losses]\n",
    "    \n",
    "    \n",
    "    fig, axis = plt.subplot(2, 1)\n",
    "    axis[0].plot(eval_interval * (1 + np.arange(len(train_losses))), train_losses, label='train')\n",
    "    axis[0].plot(eval_interval * (1 + np.arange(len(val_losses))), val_losses, label='val')\n",
    "    axis[0].set_xlabel('Iteration')\n",
    "    axis[0].set_ylabel('Loss')\n",
    "\n",
    "    axis[1].plot(eval_interval * (1 + np.arange(len(train_perp))), train_perp, label='train')\n",
    "    axis[1].plot(eval_interval * (1 + np.arange(len(val_perp))), val_perp, label='val')\n",
    "    axis[1].set_xlabel('Iteration')\n",
    "    axis[1].set_ylabel('Perplexity')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "train_losses, val_losses = train_loop(simple_lm, max_iters, eval_interval)\n",
    "\n",
    "plot_losses_perplexity(train_losses, val_losses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Run the same training loop but modify one of the hyperparameters from this list: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "n_embd = 64\n",
    "n_head = 4 ## so head_size = 16\n",
    "n_layer = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this at least 4 times with a different value and plot each perplexity over training step. Write a sentence on how the perplexity changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_embd = 128\n",
    "n_head = 4 ## so head_size = 32\n",
    "n_layer = 4\n",
    "\n",
    "# train and plot (with the new hyperparameters, run it 4 times)\n",
    "for _ in range(4):\n",
    "    simple_lm_nembd128 = LanguageModel().to(device)\n",
    "    optimizer = torch.optim.Adam(simple_lm_nembd128.parameters(), lr=learning_rate)\n",
    "    train_losses, val_losses = train_loop(simple_lm, max_iters, eval_interval)\n",
    "    plot_losses(train_losses, val_losses, perplexity=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus 1: output some generated text from each model you trained. Did the output make more sense with some hyperparameters than others? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text from the trained models\n",
    "def generate_text(model, text, max_new_tokens):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        idx = torch.tensor(encode(text), dtype=torch.long, device=device).unsqueeze(0)\n",
    "        idx = model.generate(idx, max_new_tokens)\n",
    "    return decode(idx[0].cpu().numpy())\n",
    "\n",
    "generate_text(simple_lm, 'The', 100)\n",
    "generate_text(simple_lm_nembd128, 'The', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bonus 2: We saw a cool visualization of attention mechanisms with BertViz. Take a more complicated model than GPT2 such as \"meta-llama/Llama-2-7b-chat-hf\" and see how the attention mechanisms are different "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, utils, AutoModelForCausalLM\n",
    "\n",
    "from bertviz import model_view\n",
    "utils.logging.set_verbosity_error()  # Suppress standard warnings\n",
    "\n",
    "model_name = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "input_text = \"No, I am your father\"  \n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, output_attentions=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "inputs = tokenizer.encode(input_text, return_tensors='pt')  # Tokenize input text\n",
    "outputs = model(inputs)  # Run model\n",
    "attention = outputs[-1]  # Retrieve attention from model outputs\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[0])  # Convert input ids to token strings\n",
    "model_view(attention, tokens)  # Display model view"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDXLTusqxXHf"
   },
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some recommendations for further reading and additional code for review.\n",
    "\n",
    "* \"The Illustrated Transformer\" by Jay Alammar\n",
    "* \"Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)\"\n",
    "* \"The Illustrated GPT-2 (Visualizing Transformer Language Models)\"\n",
    "* \"A gentle introduction to positional encoding\"\n",
    "* \"LLM Tutorial Workshop (Argonne National Laboratory)\"\n",
    "* \"LLM Tutorial Workshop Part 2 (Argonne National Laboratory)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
